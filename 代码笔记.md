类的用法

1.

以下这个例子用于说明类中self的作用，可以看出，构造了一个狗的类，构造函数中包含了狗的名字、年龄，有两种方法，狗叫、庆祝狗的生日。

```python
class Dog:
    def __init__(self, name, age):
        self.name = name
        self.age = age

    def bark(self):
        print(f"{self.name} says Woof!")

    def celebrate_birthday(self):
        self.age += 1
        print(f"{self.name} is now {self.age} years old!")

# Creating instances of the Dog class
dog1 = Dog("Buddy", 3)
dog2 = Dog("Max", 2)

# Accessing attributes and calling methods using self
print(f"{dog1.name} is {dog1.age} years old.")
dog1.bark()

print(f"{dog2.name} is {dog2.age} years old.")
dog2.bark()
```

以下是

```Python
LinearModel(
  (w1): Linear(in_features=32, out_features=1024, bias=True)
  (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear_stages): ModuleList(
    (0): Linear(
      (relu): ReLU(inplace=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (w1): Linear(in_features=1024, out_features=1024, bias=True)
      (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (w2): Linear(in_features=1024, out_features=1024, bias=True)
      (batch_norm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): Linear(
      (relu): ReLU(inplace=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (w1): Linear(in_features=1024, out_features=1024, bias=True)
      (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (w2): Linear(in_features=1024, out_features=1024, bias=True)
      (batch_norm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (w2): Linear(in_features=1024, out_features=48, bias=True)
  (relu): ReLU(inplace=True)
  (dropout): Dropout(p=0.5, inplace=False)
)
```



```
LinearModel(
  (w1): Linear(in_features=32, out_features=1024, bias=True)
  (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear_stages): ModuleList(
    (0): Linear(
      (relu): ReLU(inplace=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (w1): Linear(in_features=1024, out_features=1024, bias=True)
      (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (w2): Linear(in_features=1024, out_features=1024, bias=True)
      (batch_norm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): Linear(
      (relu): ReLU(inplace=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (w1): Linear(in_features=1024, out_features=1024, bias=True)
      (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (w2): Linear(in_features=1024, out_features=1024, bias=True)
      (batch_norm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (w2): Linear(in_features=1024, out_features=48, bias=True)
  (relu): ReLU(inplace=True)
  (dropout): Dropout(p=0.5, inplace=False)
)
```





实验

```
from mvn.utils.img import * 
image = Image.open(img_0_path)
image.size
Out[9]: (1000, 1002)
import numpy as np
image = np.array(image)
image.shape
Out[12]: (1002, 1000, 3)
```

# 24.4.3 console实验

（主要是进行triangulation实验）

```python
#加载二维的主干网络，并加载到权重里面
for k, v in state_dict.items():
    new_state_dict[k.replace('module.backbone.', '')] = v 
heatmaps, features, _, vol_confidence = model(images_test)
images_test = images_test.view(1, 4, *images_test.shape[1:])
keypoints_3d_gt = shot['keypoints'][:self.num_keypoints]
keypoints_3d_gt = shot['keypoints'][:num_keypoints]
base_point = keypoints_3d_gt[6, :3]
Out[160]: array([-91.679, 154.404, 907.261], dtype=float32)
keypoints_3d_gt.shape
Out[161]: (17, 3)

```

关节点可视化

```python
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

# 假设 keypoints_3d 是一个 17x3 的数组，代表3D关节坐标
# 这里我们创建一个示例数据
keypoints_3d = np.random.rand(17, 3)

# 创建一个 3D 图形
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# 提取 x, y, z 坐标
x = keypoints_3d[:, 0]
y = keypoints_3d[:, 1]
z = keypoints_3d[:, 2]

# 在3D图中绘制关节点
ax.scatter(x, y, z)

# 为了更好地可视化，我们可以连接这些点
# 假设我们有一个连接关节的顺序列表
# 这里我们简单地按顺序连接它们
for i in range(len(keypoints_3d) - 1):
    ax.plot([x[i], x[i+1]], [y[i], y[i+1]], [z[i], z[i+1]], 'gray')

# 设置坐标轴标签
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

# 显示图形
plt.show()

```

```
ga_mask_gt = calc_ga_mask(keypoints_3d_gt, coord_volumes_aux)
```

```bash
Failed to evaluate. Reason:  'list' object has no attribute 'shape'
/mnt/e/test/ContextPose-PyTorch-release-master/mvn/models/loss.py:98: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)
  keypoints_3d_pred = torch.tensor(keypoints_3d_pred, dtype=torch.float)
Traceback (most recent call last):
  File "train.py", line 694, in <module>
    main(args)
  File "train.py", line 654, in main
    n_iters_total_val = one_epoch_full(model, criterion, opt_dict, config, val_dataloader, device, epoch, n_iters_total=n_iters_total_val, is_train=False, mean_and_std=mean_and_std, limb_length=limb_length, master=master, experiment_dir=experiment_dir, writer=writer, whole_val_dataloader=whole_val_dataloader, dist_size=dist_size)
  File "train.py", line 472, in one_epoch_full
    metric_dict['limb_length_error'] = [LimbLengthError()(results['keypoints_3d'], results['keypoints_gt'])]
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/e/test/ContextPose-PyTorch-release-master/mvn/models/loss.py", line 98, in forward
    keypoints_3d_pred = torch.tensor(keypoints_3d_pred, dtype=torch.float)
ValueError: expected sequence of length 8 at dim 1 (got 7)
```



# 命令行的argparse



![image-20240404104454198](D:\Work_APP\Typora\assets\image-20240404104454198.png)

**argument

**1，如何读取命令行中传递进来的参数#t**

​	**sys.argv获取命令行中的所有参数**
​    **format: script的路径,其他参数...#**

**2，专门处理命令行的ibrary: argparse**
**-添加optional argument，有 add_argument("--a" , type=int, help=“xxx")**
**-默认是可选的，意味着可以不用填写**

**-添加positional argument，有 add_argument("a" , type=int,help="xxx")**
**-默认是不可选，必须填写，否则报错**

**-添加flags，标记，开关那种形式**，action参数
**比如说，添加一个参数，是否需要打印信息，--verbose表示打印详细信息**



1.第一种麻烦

![image-20240404103946047](D:\Work_APP\Typora\assets\image-20240404103946047.png)

2.专门处理命令行的

![image-20240404104122582](D:\Work_APP\Typora\assets\image-20240404104122582.png)

![image-20240404104244169](D:\Work_APP\Typora\assets\image-20240404104244169.png)

![image-20240404104210761](D:\Work_APP\Typora\assets\image-20240404104210761.png)

解析命令行
![image-20240404104311436](D:\Work_APP\Typora\assets\image-20240404104311436.png)

告诉别人这个程序是怎么用的，如基线模型的

```bash
(base) PS E:\test\ContextPose-PyTorch-release-master> python train.py --help
usage: train.py [-h] --config CONFIG [--eval] [--eval_dataset EVAL_DATASET]
                [--local_rank LOCAL_RANK] [--seed SEED] [--sync_bn]
                [--logdir LOGDIR] [--azureroot AZUREROOT]

optional arguments:
  -h, --help            show this help message and exit
  --config CONFIG       Path, where config file is stored
  --eval                If set, then only evaluation will be done
  --eval_dataset EVAL_DATASET
                        Dataset split on which evaluate. Can be 'train' and
                        'val'
  --local_rank LOCAL_RANK
                        Local rank of the process on the node
  --seed SEED           Random seed for reproducibility
  --sync_bn             If set, then utilize pytorch convert_syncbn_model
  --logdir LOGDIR       Path, where logs will be stored
  --azureroot AZUREROOT
                        Root path, where codes are stored
(base) PS E:\test\ContextPose-PyTorch-release-master>


```


区别 三种optional，下面是例子

![image-20240404105330059](D:\Work_APP\Typora\assets\image-20240404105330059.png![image-20240404110034983](D:\Work_APP\Typora\assets\image-20240404110034983.png)

![image-20240404105338769](D:\Work_APP\Typora\assets\image-20240404105338769.png)

![image-20240404105358195](D:\Work_APP\Typora\assets\image-20240404105358195.png)



# 命令行argparse Debug

```python
import subprocess

train_file = "../data/cave/cave_train"
eval_file = "../data/cave/cave_val"
output_folder = "outputs"

# 构建命令行命令
command = ["python", "train.py",
           "--train-file", train_file,
           "--eval-file", eval_file,
           "--outputs-dir", output_folder,
           "--model", "PDcon_SSF"
           ]

# 执行命令
# 新建一个py文件，通过调用subprocess库，然后通过运行这个文件，进行命令行调用，从而达到代码断点调试的效果
subprocess.call(command)
# 参考连接
https://blog.csdn.net/qq_49729636/article/details/134616691?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-134616691-blog-133907915.235^v43^pc_blog_bottom_relevance_base7&spm=1001.2101.3001.4242.1&utm_relevant_index=1
```

我的实现方法，根据chatgpt在train.py中加入

```Python
class Args:
    def __init__(self):
        self.config = 'E:\\test\ContextPose-PyTorch-release-master\experiments\human36m\\eval\human36m_vol_softmax_single.yaml'
        self.eval = True
        self.eval_dataset = 'val'
        self.local_rank = 1   #节点排名
        self.seed  = 42
        self.sync_bn =  False
        self.logdir = "E:\\test\ContextPose-PyTorch-release-master\logs"
        self.azureroot = ""
if __name__ == '__main__':
    #这边是测试用的呀
    args = Args()
    # 加载并应用这些配置到程序中
    update_config(args.config)
    update_dir(args.azureroot, args.logdir)
    print("args: {}".format(args))
    main(args)
 
```

我的这种存在缺陷，一定要设置self.eval = True，不然无法进行验证，会直接按照训练的.yaml文件进行，训练的是3750，验证的是1087

# 2024.4.5 设法解决评估错误

```
evaluating....
Failed to evaluate. Reason:  'list' object has no attribute 'shape'
/mnt/e/test/ContextPose-PyTorch-release-master/mvn/models/loss.py:98: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)
  keypoints_3d_pred = torch.tensor(np.arrary(keypoints_3d_pred), dtype=torch.float)
Traceback (most recent call last):
  File "train.py", line 718, in <module>
    main(args)
  File "train.py", line 696, in main
    one_epoch_full(model, criterion, opt_dict, config, dataloader, device, 0, n_iters_total=0, is_train=False, mean_and_std=mean_and_std, limb_length=limb_length, master=master, experiment_dir=experiment_dir, writer=writer, whole_val_dataloader=whole_val_dataloader, dist_size=dist_size)
  File "train.py", line 480, in one_epoch_full
    metric_dict['limb_length_error'] = [LimbLengthError()(results['keypoints_3d'], results['keypoints_gt'])]
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/e/test/ContextPose-PyTorch-release-master/mvn/models/loss.py", line 98, in forward
    keypoints_3d_pred = torch.tensor(np.arrary(keypoints_3d_pred), dtype=torch.float)
ValueError: expected sequence of length 8 at dim 1 (got 7)
```

无论是验证还是训练过程都会在最后评估的时候报这个错

下面补充一个知识点。嘿嘿

```
train、val、test有啥区别
1.train和val都是为了训练模型参数，test是在参数完全确定后做测试，是衡量你的模型性能。
2.train是训练集，val是训练过程中的测试集，是为了让你在边训练边看到训练的结果，及时判断学习状态。test就是训练模型结束后，用于评价模型结果的测试集。

```

为了方便进行调试，这边我先尝试以验证为例，减少训练过程的参数量，加快训练易于debug

1.减少批处理大小，

2.使用子集进行测试
目前都不太行

```
引知乎大佬的说法
取多少数据取决于dataset类怎么写，dataset有三个必要的方法，__init__、__len__和__getitem__
len 是指包含数据列表的长度，一般是包括了数据集路径的列表长度。一般写一个单独的功能函数，返回一个包含有数据集和label元组的列表，具体使用os.walk还有os.listdir函数
将训练集的路径放到列表中，然后从这个列表中取训练集进行训练。那么你想多少数据集放到这个列表中不都是自己所了算吗？你可以添加100张1000张
我们使用__getitem__方法从这个列表中加载具体数据进行处理。至于shuffle与否无所谓，这个是DataLoader实现的，Dataloader里面的RandomSampler类将你的列表长度n，使用torch.randperm()函数返回一个乱序的列表，你的数据就从这个乱序的列表索引。最终，训练取到什么数据，取决你的列表有哪些内容。
可以直接写一个自己的sampler，实例化一个RandomSampler，将里面的replacement：bo ol=False，num_samples:Optional[int]=None这两个参数改掉，让BatchSampler直接调用自己的sampler，这可以
```

暂时不行

先等着直接调吧，

对human36m.py中的evaluate进行修改，还是评估出错，loss.py里面的报错，报错类型如下

```
User
Failed to evaluate. Reason:  can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
  keypoints_3d_predicted = torch.tensor(np.array(keypoints_3d_predicted), dtype=torch.float)
E:\test\ContextPose-PyTorch-release-master\mvn\models\loss.py:98: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  keypoints_3d_pred = torch.tensor(np.array(keypoints_3d_pred), dtype=torch.float)
Traceback (most recent call last):
  File "E:\test\ContextPose-PyTorch-release-master\train.py", line 727, in <module>
    main(args)
  File "E:\test\ContextPose-PyTorch-release-master\train.py", line 698, in main
    one_epoch_full(model, criterion, opt_dict, config, dataloader, device, 0, n_iters_total=0, is_train=False, mean_and_std=mean_and_std, limb_length=limb_length, master=master, experiment_dir=experiment_dir, writer=writer, whole_val_dataloader=whole_val_dataloader, dist_size=dist_size)
  File "E:\test\ContextPose-PyTorch-release-master\train.py", line 482, in one_epoch_full
    metric_dict['limb_length_error'] = [LimbLengthError()(results['keypoints_3d'], results['keypoints_gt'])]
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "E:\test\ContextPose-PyTorch-release-master\mvn\models\loss.py", line 98, in forward
    keypoints_3d_pred = torch.tensor(np.array(keypoints_3d_pred), dtype=torch.float)
TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
```

总结错误：损失函数前向传播应该是没啥问题，主要问题可能出现在，keypoints_3d_pred，这玩意应该少预测东东了。



# 2024.4.6设法解决评估错误（二）

现在的思路是在想办法在数据集上抽样，加快训练，利于debug。

dataset解读：主要分为三个类

```
class Human36MKeypointDataset(Dataset):
class Human36MMultiViewDataset(Dataset):
class Human36MSingleViewDataset(Human36MMultiViewDataset):
```

三个类的作用解读：
一、从名字看是对数据集的关键点进行处理的类，用作多视角任务
	1.剔除数据中的损坏帧的标签
	2.将训练和测试的受试者变成mask的形式（T F）
    3.返回len是len(self.labels['table'])
		getitem ['keypoints_3d'] 

二、多视角处理的，返回的是一些图片和相机参数和标签

三、单视角的，主要用于验证集的处理（当然验证集也有用到上面的）



# 2024.4.7日继续改评估错误（三）

我想这就没进入评估呀。。。。感觉是评估函数的问题，送不进去,

```python
D:\Work_APP\Anconda\envs\motionbert\python.exe E:\test\ContextPose-PyTorch-release-master\train.py 
args: <__main__.Args object at 0x0000023ECB358688>
Number of available GPUs: 1
Loading pretrained weights from: data/pretrained/human36m/backbone_weights.pth
Successfully loaded pretrained weights for backbone
Experiment name: eval_human36m_vol_softmax_single_VolumetricTriangulationNet@07.04.2024-171637
Successfully loaded weights for vol model from ./logs/eval_human36m_vol_softmax_single.pth
Loading data...
Loading limb length mean & std...
data/human36m/extra/mean_and_std_limb_length.h5
  0%|          | 0/1087 [00:00<?, ?it/s]D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\TensorShape.cpp:2228.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
<class 'torch.Tensor'>
  0%|          | 2/1087 [00:08<1:05:59,  3.65s/it]<class 'torch.Tensor'>
<class 'torch.Tensor'>...
100%|█████████▉| 1082/1087 [15:32<00:04,  1.11it/s]<class 'torch.Tensor'>
<class 'torch.Tensor'>
100%|█████████▉| 1084/1087 [15:34<00:02,  1.11it/s]<class 'torch.Tensor'>
100%|█████████▉| 1085/1087 [15:35<00:01,  1.10it/s]<class 'torch.Tensor'>
100%|█████████▉| 1086/1087 [15:36<00:00,  1.09it/s]<class 'torch.Tensor'>
<class 'torch.Tensor'>
100%|██████████| 1087/1087 [15:36<00:00,  1.13it/s]Traceback (most recent call last):
  File "E:\test\ContextPose-PyTorch-release-master\train.py", line 732, in <module>
    main(args)
  File "E:\test\ContextPose-PyTorch-release-master\train.py", line 703, in main
    one_epoch_full(model, criterion, opt_dict, config, dataloader, device, 0, n_iters_total=0, is_train=False, mean_and_std=mean_and_std, limb_length=limb_length, master=master, experiment_dir=experiment_dir, writer=writer, whole_val_dataloader=whole_val_dataloader, dist_size=dist_size)
evaluating....
Failed to evaluate. Reason:  'list' object has no attribute 'shape'
<class 'list'>
  File "E:\test\ContextPose-PyTorch-release-master\train.py", line 487, in one_epoch_full
    metric_dict['limb_length_error'] = [LimbLengthError()(results['keypoints_3d'], results['keypoints_gt'])]
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "E:\test\ContextPose-PyTorch-release-master\mvn\models\loss.py", line 105, in forward
    limb_pred = keypoints_3d_pred[:, joint0] - keypoints_3d_pred[:, joint1]
TypeError: list indices must be integers or slices, not tuple
100%|██████████| 1087/1087 [15:37<00:00,  1.16it/s]

Process finished with exit code 1

```

找到错误

# 2024.4.8 重新尝试learnable-triangulation的复现

上面的contextpose在评估环节出错的原因是gt和预测值维数不匹配，目前预想的解决方案是看预测值到底有几个维，看看是不是少了？

试基线模型的原因是看看是不是由于加入了这个的原因

```
 limb_pred = keypoints_3d_pred[:, joint0] - keypoints_3d_pred[:, joint1]
```

1.number_worker报错

原因是在windows环境下要先将numworker置0否则会报以下错误

```
Traceback (most recent call last):
Traceback (most recent call last):
  File "E:\test\learnable-triangulation-pytorch-master\train.py", line 508, in <module>
  File "<string>", line 1, in <module>
    main(args)
  File "E:\test\learnable-triangulation-pytorch-master\train.py", line 477, in main
    one_epoch(model, criterion, opt, config, val_dataloader, device, 0, n_iters_total=0, is_train=False, master=master, experiment_dir=experiment_dir, writer=writer)
  File "E:\test\learnable-triangulation-pytorch-master\train.py", line 173, in one_epoch
    iterator = enumerate(dataloader)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\utils\data\dataloader.py", line 368, in __iter__
  File "D:\Work_APP\Anconda\envs\motionbert\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\multiprocessing\spawn.py", line 115, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input
    return self._get_iterator()
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\utils\data\dataloader.py", line 314, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\utils\data\dataloader.py", line 927, in __init__
    w.start()
  File "D:\Work_APP\Anconda\envs\motionbert\lib\multiprocessing\process.py", line 112, in start
    self._popen = self._Popen(self)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\multiprocessing\popen_spawn_win32.py", line 89, in __init__
    reduction.dump(process_obj, to_child)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'make_collate_fn.<locals>.collate_fn
```

重新用learnable-triangulation的Dataset读取了一下发现验证数据集val_dataset只有2181个。

| 训练集    |        |
| --------- | ------ |
| S1        | 91128  |
| S5        | 161852 |
| S6        | 103560 |
| S7        | 17500  |
| S8        | 105184 |
| 验证集    |        |
| S9        | 5021   |
| S11       | 3683   |
| 验证集Sum | 8695   |

疑问来了，
1.为啥同样的数据处理下的同样数据量的，contextpose的Dataset读取val会有辣么多8000+个？？

2.Human36MSingleViewDataset这是作者编写的用于和骨长以及全局注意力机制关联的

以下是运行后的预测值和真值的结果，很奇怪的是，results里面的数据是啥类型，出来既然是个列表？

![image-20240408201544433](D:\Work_APP\Typora\assets\image-20240408201544433.png)

![image-20240408201508615](D:\Work_APP\Typora\assets\image-20240408201508615.png)

它只有1087个很奇怪呀。。

为了程序能够运行，我在loss.py里面做了一些操作，通过前面1087个的均值代替第一个1088个，补充第1088位置，在这个基础上就可以整除17了。loss的这个能在训练过程中forward的前向传播，但是在最后评估的时候还会计算这个误差，但是返回的过程中是个list

```Python
class LimbLengthError(nn.Module):
	def forward(self, keypoints_3d_pred, keypoints_3d_gt):
        if isinstance(keypoints_3d_pred, (list, np.ndarray)):
            keypoints_3d_pred = np.array(keypoints_3d_pred)
            keypoints_3d_pred = np.append(keypoints_3d_pred, mean_pred).reshape(-1,17)
        if isinstance(keypoints_3d_gt, (list, np.ndarray)):
            keypoints_3d_gt = np.array(keypoints_3d_gt)
            keypoints_3d_gt = np.append(keypoints_3d_pred, mean_gt).reshape(-1,17)
        #...
        for (joint0, joint1) in self.CONNECTIVITY_DICT:
			limb_pred = keypoints_3d_pred[:, joint0] - keypoints_3d_pred[:, joint1]

```

结果报错

```Python
File "E:\test\ContextPose-PyTorch-release-master\mvn\models\loss.py", line 118, in forward
    limb_pred = torch.from_numpy(limb_pred)
TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
```

参考，CSND连接，准备查看是不是因为torch数据中的长度不一？

```
https://blog.csdn.net/weixin_42419002/article/details/100095302
```

找到的结果可能是，没有满足以下条件
[torch](https://so.csdn.net/so/search?q=torch&spm=1001.2101.3001.7020) 中的数据长度必须是相同的 ------> X中的数据长度必须是相同的 ------> np.shape(X)==( , ) 即必须是二维的。

## 总结trian.py

为了进一步了解总体程序，下面对train.py进行一个系统的梳理。
一、数据集的加载函数

```python
# 开始进入命令行参数
def parse_args():
	# 参数：配置文件，验证，验证数据集，进程在节点的排名，随机种子。。。
    
# 设置human3.6m的数据加载
def setup_human36m_dataloaders(config, is_train, distributed_train, rank = None, world_size = None):
    # **** 参数，cofig，is_train-训练与否，distributed_train-分布式训练，rank-..，world_size-..
    # train_dataset（训练数据集）
    # class Human36MMultiViewDataset(Dataset) → 训练数据集加载 总数 159181
    #  with_damaged_actions（√），scale_bbox边界框缩放倍数， erase擦除人体关节，undistort_images图片畸变，data_format 字符串
    # train_sampler（训练采样器）用于多个GPU的分布式训练
    # train_dataloader (数据集加载器)
	# 训练数据集，批次大小，翻转，采样器，数据加载器（将不同样本的数据批量组合成一个批次），进程数，随机种子，pin_memory（CPU→GPU数据传递）
    #val_dataset（验证数据集）
    #同上面的train_dataset，区别在于retain_every_n_frames_in_test有跳过帧，rank：暂时不知道干啥的，秩，world_size: 世界坐标大小？
    # ****输出参数，train_dataloader, 
    #             val_dataloader,
    #             train_sampler,
    # *********   val_dataset.dist_size，分布式计算环境中使用数据集的分布大小
 def setup_dataloaders(config, is_train=True, distributed_train=False, rank = None, world_size=None):
    '''
    	CMU数据集和human3.6m数据集选择，如果是human36m，
    	如果没有配置rank和world_size则在调用setup_dataloaders()的时候返回的是整个验证集合
    '''
    # 一种是设置了秩，世界大小的→ config, is_train, distributed_train, rank, world_size，区别在于返回的是整个验证还是部分验证
    train_dataloader, val_dataloader, train_sampler, dist_size = setup_human36m_dataloaders(config, is_train, distributed_train, rank, world_size)
	_, whole_val_dataloader, _, _ = setup_human36m_dataloaders(config, is_train, distributed_train)
    # 输出  train_dataloader, val_dataloader, train_sampler, whole_val_dataloader, dist_size
```

二、实验和完整训练周期配置

```python
def setup_experiment(config, model_name, is_train=True):
	# 前缀是训练还是验证
    # 实验标题
    """
    	config.title = "human36m_vol_softmax_single"
    	要么就是 配置标题+ '_' + 模型名称（type（model））
        要么就是模型名称
    """
    #实验标题为前缀和实验标题组合
    experiment_title = prefix + experiment_title
    # 实验名称为：实验主题+现在的时间
    # 创建checkpoint文件
    # 把config.yaml复制进去
    # 把实验的整个过程写进tb文件夹（用来记录tensorboard的事件文件）
    # 增加文本框，把配置文件变成字符串，并且标题为config
    # #返回的是实验的目录， tensorboard的对象
    return experiment_dir, writer
```



```python
# 一整个周期，n_iters_total→迭代次数，caption→说明文字，master？，whole_val_dataloader→整个数据集
def one_epoch_full(model, criterion, opt_dict, config, dataloader, device, epoch, n_iters_total=0, is_train=True, lr=None, mean_and_std=None, limb_length = None, caption='', master=False, experiment_dir=None, writer=None, whole_val_dataloader=None, dist_size=None):
    



```



# 2024.4.9解决评估错误（四）

发现问题所在了，首先在训练的epoch中，算肢体长度是提个batch_size的量送入的

![image-20240409090227334](D:\Work_APP\Typora\assets\image-20240409090227334.png)

但是在进行评估的时候它是整个放进去一起评估，就有问题了，总的放进去相当于是（1087,8,17,3），对这个一起求误差进行评估，因此对程序的loss.py进行以下修改：

```Python
error_total = []
error = 0
if self.is_trian:
    for (joint0, joint1) in self.CONNECTIVITY_DICT:

        limb_pred = keypoints_3d_pred[:, joint0] - keypoints_3d_pred[:, joint1]
        limb_gt = keypoints_3d_gt[:, joint0] - keypoints_3d_gt[:, joint1]
        if isinstance(limb_pred, np.ndarray):
            np.save('limb_pred.npy', limb_pred)
            limb_pred = torch.from_numpy(limb_pred)
            limb_gt = torch.from_numpy(limb_gt)
        limb_length_pred = torch.norm(limb_pred, dim = 1)
        limb_length_gt = torch.norm(limb_gt, dim = 1)
     error += torch.abs(limb_length_pred - limb_length_gt).mean().cpu()
	return float(error) / len(self.CONNECTIVITY_DICT)
        else:
            for Seq_single in range(self.seq_count):
                keypoints_3d_pred_s = keypoints_3d_pred[Seq_single]
                keypoints_3d_gt_s = keypoints_3d_gt[Seq_single]
                for (joint0, joint1) in self.CONNECTIVITY_DICT:
                    limb_pred = keypoints_3d_pred_s[:, joint0] - keypoints_3d_pred_s[:, joint1]
                    limb_gt = keypoints_3d_gt_s[:, joint0] - keypoints_3d_gt_s[:, joint1]
                    if isinstance(limb_pred, np.ndarray):
                        limb_pred = torch.from_numpy(limb_pred)
                        limb_gt = torch.from_numpy(limb_gt)
                    limb_length_pred = torch.norm(limb_pred, dim=1)
                    limb_length_gt = torch.norm(limb_gt, dim=1)
                    error += torch.abs(limb_length_pred - limb_length_gt).mean().cpu()
                    error = float(error) / len(self.CONNECTIVITY_DICT)
                error_total.append(error)
            return error_total
```

但貌似还是评估失败了，捕获异常，原因检测到错误，出现了一些bug

```Python
try:
    if dist_size is None:
    print('evaluating....')
    scalar_metric, full_metric = dataloader.dataset.evaluate(results['keypoints_gt'], results['keypoints_3d'], 		results['proj_matricies_batch'], config)
    else:
        scalar_metric, full_metric = whole_val_dataloader.dataset.evaluate(results['keypoints_gt'],   results['keypoints_3d'], results['proj_matricies_batch'], config)
except Exception as e:
     print("Failed to evaluate. Reason: ", e)
     scalar_metric, full_metric = 0.0, {}
```

## 异常捕获

1.什么是异常:
异常就是程序运行的过程中出现了错误
2.bug是什么意思:
bug就是指异常的意思，因为历史因为小虫子导致计算机失灵的案例，所以延续至今，bug就代表软件出现错误。

①整个程序因为一个BUG停止运行

②对BUG进行提醒,整个程序继续运行

```
try:
可能发生错误的代码
except:
如果出现异常执行的代码
```

```
try:
	f = open('linux.txt" , 'r')
except:
	f =open('linux.txt" , 'w')
```

捕获所有异常

![image-20240409145458487](D:\Work_APP\Typora\assets\image-20240409145458487.png)

捕获指定异常

![image-20240409145513878](D:\Work_APP\Typora\assets\image-20240409145513878.png)

多个异常的捕获。

捕获所有异常

![image-20240409145745189](D:\Work_APP\Typora\assets\image-20240409145745189.png)

Exception是顶级异常，其他异常都是小弟

![image-20240409145908943](D:\Work_APP\Typora\assets\image-20240409145908943.png)

finally有没有异常都要干活

![image-20240409145958082](D:\Work_APP\Typora\assets\image-20240409145958082.png)

补充一个小知识点，对函数的形参和返回值进行类型注解，方便ctrl+p查看

<img src="D:\Work_APP\Typora\assets\image-20240409151102749.png" alt="image-20240409151102749" style="zoom:50%;" />

# 2024.4.10上传云服务器准备训练模型

首先学习怎么上传数据集：

参考资料：

怎么租用示例，怎么通过xftp上传数据集，怎么用pycharm或者vscode连接本地

https://blog.csdn.net/lwd19981223/article/details/127085811/
https://www.bilibili.com/video/BV1HF411S7HT/?spm_id_from=333.337.search-card.all.click&vd_source=4f71440915213043171eeabe5747514d

遇到的第一个问题是不能解压上传的数据集文件：

```bash
# 使用unzip processed 去解压文件，但是报错
Archive:  processed.zip
  End-of-central-directory signature not found.  Either this file is not
  a zipfile, or it constitutes one disk of a multi-part archive.  In the
  latter case the central directory and zipfile comment will be found on
  the last disk(s) of this archive.
note:  processed.zip may be a plain executable, not an archive
unzip:  cannot find zipfile directory in one of processed.zip or
        processed.zip.zip, and cannot find processed.zip.ZIP, period
# 尝试
sudo apt-get update
sudo apt-get install p7zip-full

7z x processed.zip
# 似乎通过上面的命令能解压，目前还在试
7z e archive.zip -ooutput_dir
7z e processed.zip ../autodl-fs/ContextPose-PyTorch-release/data/human36m/processed
unzip processed.zip -d  ../autodl-fs/ContextPose-PyTorch-release/data/huma
n36m/processed
unzip -d processed.zip ../autodl-tmp/ContextPose-PyTorch-release/data/human36m
```

## 模型评估

怎么按照论文中的规则输出结果？

对出现异常的地方复写，源代码定位到

```python
try:
	if dist_size is None:
	print('evaluating....')
	scalar_metric, full_metric = dataloader.dataset.evaluate
 except Exception as e:
        # 出现异常了补救方式
	print("Failed to evaluate. Reason: ", e)
	scalar_metric, full_metric = 0.0, {}
```



由于这个异常导致最后出现的评估结果就是一个{}，因此决定重写并强行执行evaluate试试看。



这边解释一下一些概念

验证集：模型训练过程中单独留出的样本集，可以用于调整模型的超参数和用于对模型的能力进行初步评估。

测试集：用来评估最终模型的泛化能力。但不能作为调参、选择特征等算法相关的选择依据。

现在看看评估也不是那么难呀，哈哈哈哈，就是把最后result中的结果一起放进去, 会调用就行

```python
#评估每个动作的
def evaluate_using_per_pose_error(per_pose_error, split_by_subject):
    def evaluate_by_actions(per_pose_error, mask=None):
        if mask is None:
            # 创建出，和per_pose_error同形状的布尔类型的矩阵
            mask = np.ones_like(per_pose_error, dtype=bool)

            action_scores = {
                'Average': {'total_loss': per_pose_error[mask].sum(), 'frame_count': np.count_nonzero(mask)}
            }

            for action_idx in range(len(labels['action_names'])):
                action_mask = (labels['table']['action_idx'] == action_idx) & mask
                action_per_pose_error = per_pose_error[action_mask]
                action_scores[labels['action_names'][action_idx]] = {
                    'total_loss': action_per_pose_error.sum(), 'frame_count': len(action_per_pose_error)
                }
                # 没有试验的动作名称
                action_names_without_trials = \
                [name[:-2] for name in labels['action_names'] if name.endswith('-1')]

                for action_name_without_trial in action_names_without_trials:
                    combined_score = {'total_loss': 0.0, 'frame_count': 0}

                    for trial in 1, 2:
                        action_name = '%s-%d' % (action_name_without_trial, trial)
                        combined_score['total_loss'] += action_scores[action_name]['total_loss']
                        combined_score['frame_count'] += action_scores[action_name]['frame_count']
                        del action_scores[action_name]

                        action_scores[action_name_without_trial] = combined_score

                 for k, v in action_scores.items():
                 	action_scores[k] = float('nan') if v['frame_count'] == 0 else (v['total_loss'] / v['frame_count'])

                 	return action_scores

                        subject_scores = {
                            'Average': evaluate_by_actions(self, per_pose_error)
                        }

                        for subject_idx in range(len(labels['subject_names'])):
                            subject_mask = labels['table']['subject_idx'] == subject_idx
                            subject_scores[labels['subject_names'][subject_idx]] = \
                            evaluate_by_actions(per_pose_error, subject_mask)

	return subject_scores
# 这个函数以上面的为基础，最后会返回平均误差
 def evaluate(self, keypoints_gt, keypoints_3d_predicted, proj_matricies_batch=None, config=None,  split_by_subject=False, transfer_cmu_to_human36m=False, transfer_human36m_to_human36m=False):
return result['per_pose_error_relative']['Average']['Average'], result
```



```Python
from mvn.datasets.human36m import *
import pickle
model_weight_path = "E:\\test\ContextPose-PyTorch-release-master\logs\eval_human36m_vol_softmax_single_VolumetricTriangulationNet@09.04.2024-111354\checkpoints\\0000\\results.pkl"
with open(model_weight_path, 'rb') as file:
    data = pickle.load(file)
root_path = "E:\\test\ContextPose-PyTorch-release-master\data\human36m\processed"
labels_path = "E:\\test\ContextPose-PyTorch-release-master\data\human36m\extra\human36m-multiview-labels-GTbboxes.npy"
pred_path = "E:\\test\ContextPose-PyTorch-release-master\data\pretrained\human36m\human36m_alg_10-04-2019\checkpoints\\0060\\results\\train.pkl"
image_shape = [384, 384]
scale_bbox = 1.0
dataset_kind = "human36m"
data_format = ""
val_dataset = Human36MSingleViewDataset(
    root=root_path,
    pred_results_path=pred_path,
    train=False,
    test=True,
    image_shape=image_shape,
    labels_path=labels_path,
    with_damaged_actions=True,
    retain_every_n_frames_in_test=1,
    scale_bbox=1.0,
    kind=dataset_kind,
    undistort_images=True,# true
    ignore_cameras=[],
    crop=True,
    erase=False,
    rank=None,
    world_size=None,
    data_format="",
    subset_size=None
)
keypoints_3d_pred = np.array(data['keypoints_3d'], dtype=object)
keypoints_3d_gt = np.array(data['keypoints_gt'], dtype=object)
cmu_joints = [10, 11, 15, 14, 1, 4]
human36m_joints = [10, 11, 15, 14, 1, 4]
metric_dict = defaultdict(list)
root_index = 0
print(metric_dict)

# 用验证集的标签
labels = val_dataset.labels
actions_scores = []
keypoints_gt = keypoints_3d_gt[:, human36m_joints]
keypoints_3d_predicted = keypoints_3d_pred[:, cmu_joints]

keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:root_index + 1]
per_pose_error = np.sqrt(((keypoints_gt - keypoints_3d_predicted) ** 2).sum(2)).mean(1)
keypoints_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:, root_index:root_index + 1]
per_pose_error_relative = np.sqrt(((keypoints_gt_relative - keypoints_predicted_relative) ** 2).sum(2)).mean(1)

result = {
    'per_pose_error': evaluate_using_per_pose_error(per_pose_error, split_by_subject),
    'per_pose_error_relative': evaluate_using_per_pose_error(per_pose_error_relative, split_by_subject)
}
```



![image-20240410225446362](D:\Work_APP\Typora\assets\image-20240410225446362.png)

# 2024.4.11把评估结果整理出来和原文对比一下看看

先在云服务器上训练一下，准备将2个GPU训练的结果拿出来进行评估。

先完成上一次没有做完的，对3d_pose_baseline_pytorch这个项目的训练结果进行评估

## 评估3d_pose_baseline_pytorch

```python
start_epoch = 0
err_best = 1000
glob_step = 0
lr_now = 1.0e-3
model = LinearModel()
model = model.cuda()
print(">>> total params: {:.2f}M".format(sum(p.numel() for p in model.parameters()) / 1000000.0))

criterion = nn.MSELoss(size_average=True).cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)

load_path = "E:\\test\\3d_pose_baseline_pytorch-master\checkpoint\example\ckpt_best.pth.tar"
#模型加载
print(">>> loading ckpt from '{}'".format(load_path))
ckpt = torch.load(load_path)
start_epoch = ckpt['epoch']
err_best = ckpt['err']
glob_step = ckpt['step']
lr_now = ckpt['lr']
model.load_state_dict(ckpt['state_dict'])
optimizer.load_state_dict(ckpt['optimizer'])
# 创建日志
logger = log.Logger(os.path.join(opt.ckpt, 'log.txt'), resume=True)
# list of action(s)
actions = misc.define_actions(opt.action)
num_actions = len(actions)
print(">>> actions to use (total: {}):".format(num_actions))
pprint(actions, indent=4)
print(">>>")

# data loading
print(">>> loading data")
# load statistics data
stat_3d = torch.load(os.path.join(opt.data_dir, 'stat_3d.pth.tar'))

# 每个动作进行测试
print(">>> ckpt loaded (epoch: {} | err: {})".format(start_epoch, err_best))
err_set = []
for action in actions:
    print(">>> TEST on _{}_".format(action))
    test_loader = DataLoader(
        dataset=Human36M(actions=action, data_path=data_dir, use_hg=True, is_train=False),
        batch_size=64,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )
    _, err_test=test(test_loader, model, criterion, stat_3d, procrustes=True)
    err_set.append(err_test)
for action in actions:
	print ("{}".format(action), end='\t')
print ("\n")
for err in err_set:
            print ("{:.4f}".format(err), end='\t')
print (">>>\nERRORS: {}".format(np.array(err_set).mean()))
sys.exit()
```

![image-20240411231818109](D:\Work_APP\Typora\assets\image-20240411231818109.png)

![image-20240411233027171](D:\Work_APP\Typora\assets\image-20240411233027171.png)

最后的平均误差为42.96645265691339

# 2024.4.12在云服务器上运行

这玩意是真烧钱。。。，昨天跑了发现花了差不多7个小时没跑完，我吐了，今天准备充200+接着上一次的跑
接着跑的地方出错了，我寻思应该是配置文件出差了，报错内容是：

```
Experiment name: human36m_vol_softmax_single_VolumetricTriangulationNet@12.04.2024-08:52:27
Successfully loaded pretrained weights for backbone
Failed loading weights for vol model as no checkpoint found at 
Failed loading weights for vol model as no checkpoint found at 
Failed loading optimizer parameters for vol model as no optimizer found at 
Loading data...
Failed loading optimizer parameters for vol model as no optimizer found at 
Loading data...
Loading limb length mean & std...
data/human36m/extra/mean_and_std_limb_length.h5
Loading limb length mean & std...
data/human36m/extra/mean_and_std_limb_length.h5
```

![image-20240412091537027](D:\Work_APP\Typora\assets\image-20240412091537027.png)



![image-20240412091552109](D:\Work_APP\Typora\assets\image-20240412091552109.png)

综合应该是配置文件中没有写接着训练的checkpoint的地址。改进之后在跑了，等待跑通继续看源码。看看有没有早停策略。

<img src="D:\Work_APP\Typora\assets\image-20240412170024727.png" alt="image-20240412170024727" style="zoom:150%;" />

（根据赵宣的回答：）迭代3750次表示一个epoch，159171条数据要跑3750个batch，8695条验证数据要跑544个batch。跑完3750才跑验证。

# 2024.4.15 学习一下一个完整的语言模型的搭建过程和分布式训练

今天早上撰写周报。

下午看分布式训练的视频，文本分类影评是消极的还是积极的。

vocab_size表示单词的数量，embedding_dim（取决于），每一个token用一个向量表示这个向量的长度就是这个

## 一个完整的项目（对影评中的消极和积极评论进行一个分类）

### 一、复杂模型架构

1.第一层，conv（通道数,输出的通道数,卷积核大小，步长是7（大约是1/2的卷积核，相邻划窗有交点经过一次卷积之后序列长度会缩短）

在forward过程中，batch_size乘以这个batch中最大句子的长度，会做填充。

2.有序传递会门卷积，当把一个三维张量传递给一维卷积，[batchszie, 通道数，信号长度]

<img src="D:\Work_APP\Typora\assets\image-20240415151847244.png" alt="image-20240415151847244" style="zoom:50%;" />

<img src="D:\Work_APP\Typora\assets\image-20240415154242980.png" alt="image-20240415154242980" style="zoom:50%;" />

### 二、简单模型

![image-20240415154407811](D:\Work_APP\Typora\assets\image-20240415154407811.png)

### 三、构建单词表（构建dataset）



#### 补充知识点Dateset的类型：

1）map-style datasets类型（推荐）

```
实现了__getitem__()和__len__方法，它代表数据的所以到真正数据局之间映射。
读取数据并非直接把所有数据读取出来，而是读取的数据的隐隐或者键值这种类型是使用最多的类型，采用这种访问数据的方式可以大大节约训练时需要的内存数量，提高模型训练效率。
```

2）Iterable-style-datasets类型

```
实现了__iter__()方法，与上述类型不同之处子域，他将真实数据全部载入，然后在整个数据集上进行迭代，这种读取数据的方式比较适合处理流数据。
```

3）自定义类

```
上面我们提到，Dataset作为一个抽象类，需要定义其子类来实例化。所以我们需要自己定义其子类或者使用已经定义好的子类

必须要继承已经内置的抽象类dataset 必须要重写其中的__init__()方法、__getitem__()方法和__len__()方法 其中__getitem__()方法实现通过给定的索引遍历数据样本，__len__()方法实现返回数据的条数
```



1.通过IMDB这个函数，得到一个dataset对象，在pytorch中dataset有两种类型，可迭代型(**坏处，迭代完之后就会变成空的**)和map style

2.实例化分词器

3.yield_tokens, 对dataset的对象、分词器（tokenizer）进行一个遍历得到一个元组，最后能把一句话变成一个列表(nice to meet you)→（[next], [to],[meet],[you]），返回一个生成器

4.build_vocab_from_iterator，出现次数低于20次的用特殊字符替代

5.特殊字符设置为0

6.根据vocab的长度就可以设置刚开始那个VOCAB_SIZE=15000这个常量

![image-20240415154908973](D:\Work_APP\Typora\assets\image-20240415154908973.png)





collate_fn的作用，对dataset的minbatch做一个后处理，或者数据比较复杂的话，对n""DataLoader所牛成的mini-batch进行后处理

1.把token列表转化成每个token在词典中的索引

2.将minbatch中的所有句子填充到相同长度构成张量

3.对标签（positive，native）转化成0-1张量。

![image-20240415155159858](D:\Work_APP\Typora\assets\image-20240415155159858.png)

collect_fn，这边的labels和comments不是一个样本的，而是batch_size个的，遍历的时候是一个

循环里面，1.首先对影评进行分词，将字符串转化成一个个字符所对应的列表

2.将分词后的tokens传入到vocab中就能得到某个token所对应的索引（字符列表→索引列表），变长（句子长度）的索引列表

3.找到minbatch中最大句子长度

4.117行对索引按照最大长度进行填充，使用![image-20240415160850605](D:\Work_APP\Typora\assets\image-20240415160850605.png)进行填充（unknow）。

![image-20240415160058148](D:\Work_APP\Typora\assets\image-20240415160058148.png)

为啥要int64，因为在后面做loss的时候，需要将target转化成onehot向量，**在转化成该向量的时候它接受的是一个长整形数据**，token_index作为word embedding的输入可以用int32

![image-20240415160228911](D:\Work_APP\Typora\assets\image-20240415160228911.png)

四、训练

![image-20240415164638094](D:\Work_APP\Typora\assets\image-20240415164638094.png)

参数：log_step_interval多少步打印一次日志，save_step_interval多少步保存一次参数，eval_step_interva多少步做一个验证集的运行，看看验证集的loss是多少这样可以即使的看到模型有没有过拟合，模型的收敛速度。save_path保存路径，resume导入一个训练好的模型.

1.使用resume导入训练好的模型

（1）模型的权重等参数的state_dict

(2)优化器的状态参数

（3）全局变量，epoch和step

2.正式训练部分，从start_epcoh进行训练

（1）对epoch进行循环

观测的话用指数移动平均的loss（ema_loss）进行观看一般来说哈，单步loss震荡会比较剧烈

（2）对batch进行遍历，

model（token_index）--调用的是forward方法

交叉熵，二分类概率值，和概率值一样形状的标签值（硬标签）

ema系数，比较大的系数乘以上一步系数加上一个比较小的系数乘一个当前系数（指数平均）

**backward函数能算出节点的梯度。**

nn.utils.clip_grad_norm这步就是对梯度的模进行一个截断，保证训练更加稳定一点,让它最大值变成一个0.1

.step根据优化器的参数进行更新

![image-20240415170555436](D:\Work_APP\Typora\assets\image-20240415170555436.png)

3.打印日志

![image-20240415171302427](D:\Work_APP\Typora\assets\image-20240415171302427.png)

4.判断当前是否需要保存模型

<img src="D:\Work_APP\Typora\assets\image-20240415171335492.png" alt="image-20240415171335492" style="zoom:50%;" />

5.判断当前是否需要验证集的计算

※调用model.eval（）他就不会计算每个节点的反向传播值，可以跑的更快点，没有调用model.train的原因是nn.moduel默认打开了model.train。

![image-20240415171438524](D:\Work_APP\Typora\assets\image-20240415171438524.png)

打印日志的时候尽量别用张量，item保存为python的数据类型，不会影像性能

![image-20240415172553714](D:\Work_APP\Typora\assets\image-20240415172553714.png)

# 2024.4.16 继续尝试单GPU下的评估

好像是改出来了，对比了一下分布式训练最终的results和单GPU下的results，发现类型不太一样。

```Python
    if not is_train:
        if dist_size is not None:
            term_list = ['keypoints_gt', 'keypoints_3d', 'proj_matricies_batch', 'indexes']
            for term in term_list:
                results[term] = np.concatenate(results[term])
                # 准备缓冲区，其中dist_size[-1]代表每个进程需要接受的数据的最大尺寸
                # results[term].shape[1:]提供每个张量的形状（除了第一维之外）
                # .cuda()确保这些张量在GPU上分配
                buffer = [torch.zeros(dist_size[-1], *results[term].shape[1:]).cuda() for i in range(len(dist_size))]
                # 创建一个与buffer中的张量形状相同的 scatter_tensor,并将聚合后的数据（‘results[term]’）复制到这个张量中，
                # 这个张量用作全局聚合操作
                scatter_tensor = torch.zeros_like(buffer[0])
                scatter_tensor[:results[term].shape[0]] = torch.tensor(results[term]).cuda()
                # 全局聚合
                torch.distributed.all_gather(buffer, scatter_tensor)
                # 从缓冲区提取并拼接数据
                results[term] = torch.cat([tensor[:n] for tensor, n in zip(buffer, dist_size)], dim = 0).cpu().numpy()
        # 做一个补充好像是可以了
        else:
            results['keypoints_3d'] = np.concatenate(results['keypoints_3d'])
            results['keypoints_gt'] = np.concatenate(results['keypoints_gt'])
            results['proj_matricies_batch'] = np.concatenate(results['proj_matricies_batch'])
```

学习一下视觉的TF


# 2024.4.17 Contextpose论文的阅读

1.准备的缝合的思路：将contextpose的GNN或PSM进行替代和probailistic中的相机姿态分布模块进行改进缝合成一个



理解代码中的GA图

```python
#1.实例化数据集
#2.用dataloader加载数据集形成batch
#3.构建坐标体积(缺啥补啥吧，这边只是一个示例)
from mvn.utils import op, multiview, img, misc, volumetric
coord_volumes = torch.zeros(4, 64, 64, 64, 3, device="cuda")
coord_volumes_aux = torch.zeros(4, 64// 4, 64 // 4, 64 // 4, 3,
base_points = torch.zeros(4, 3, device='cuda')
for batch_i in range(4):
    keypoints_3d = batch['pred_keypoints_3d'][batch_i]
    base_point = keypoints_3d[6, :3]
    base_points[batch_i] = torch.from_numpy(base_point).to("cuda")
    sizes = np.array([2500, 2500, 2500])
    aux_sizes = sizes - 3 * sizes / (64 - 1)
    position = base_point - sizes / 2
    cuboid = volumetric.Cuboid3D(position, sizes)
    cuboids.append(cuboid)
    theta = np.random.uniform(0.0, 2 * np.pi)
    axis = [0, 0, 1]  # z axis绕z旋转
    coord_volumes[batch_i] = build_coord_volume(64, position, sizes, base_point, theta, axis, "cuda")
    coord_volumes_aux[batch_i] = build_coord_volume(64//4, position, aux_sizes, base_point, theta, axis, "cuda") 
#4.可视化坐标体积

import matplotlib.pyplot as plt
import numpy as np
batch_index = 0
slice_index = 32
# 可视化某个批次中的一个 XY 平面切片
plt.imshow(coord_volumes[batch_index, :, :, slice_index, :].to("cpu"))
plt.title(f'Batch {batch_index}, Z-Slice {slice_index}')
plt.show() 
# 同样，你可以选择不同的方向切片（XZ或YZ平面）                              
plt.imshow(coord_volumes[batch_index, :, :, slice_index, :].to("cpu"))
plt.title(f'Batch {batch_index}, Y-Slice {slice_index}')
plt.show()
```

![image-20240417211921494](D:\Work_APP\Typora\assets\image-20240417211921494.png)

![image-20240417212007720](D:\Work_APP\Typora\assets\image-20240417212007720.png)

补充一个列表的知识点【(a,b)for a,b in c,d】

```python
import collections
from collections import defaultdict
import pickle
import h5py
import numpy as np
np.set_printoptions(suppress=True)
import cv2
import torch
from torch.utils.data import Dataset
from mvn.utils.multiview import Camera, project_3d_points_to_image_plane_without_distortion
from mvn.utils.img import get_square_bbox, resize_image, crop_image, normalize_image, scale_bbox, erase_image
from mvn.utils import volumetric



root_path = "E:\\test\ContextPose-PyTorch-release-master\data\human36m\processed"
labels_path = "E:\\test\ContextPose-PyTorch-release-master\data\human36m\extra\human36m-multiview-labels-GTbboxes.npy"
# image_shape = [256, 256]
# # aa = Human36MKeypointDataset(root=root_path, labels_path=labels_path, image_shape=image_shape, test=True)
#
# root_path = "E:\\test\ContextPose-PyTorch-release-master\data\human36m\processed"
# labels_path = "E:\\test\ContextPose-PyTorch-release-master\data\human36m\extra\human36m-multiview-labels-GTbboxes.npy"
image_shape = [384, 384]
scale_bbox = 1.0
dataset_kind = "human36m"
data_format = ""
#
#
cc = Human36MMultiViewDataset(root=root_path,
                              pred_results_path="E:\\test\ContextPose-PyTorch-release-master\data\pretrained\human36m\human36m_alg_10-04-2019\checkpoints\\0060\\results\\train.pkl",
                              train=True,
                              test=False,
                              image_shape=image_shape,
                              labels_path=labels_path,
                              with_damaged_actions=True,
                              retain_every_n_frames_in_test=1,
                              scale_bbox=scale_bbox,
                              kind=dataset_kind,
                              undistort_images=True,  # true
                              ignore_cameras=[],
                              crop=True,
                              erase=False,
                              data_format=data_format)
from mvn.utils.img import get_square_bbox, resize_image, crop_image, normalize_image, scale_bbox, erase_image
import os
import shutil
import argparse
import time
import json
from datetime import datetime
from collections import defaultdict
from itertools import islice
import pickle
import copy
from tqdm import tqdm
import h5py
from PIL import Image
import numpy as np
np.set_printoptions(suppress=True)
import cv2
import prettytable
import torch
from torch import nn
from torch import autograd
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.nn.parallel import DistributedDataParallel
from tensorboardX import SummaryWriter
from mvn.models.triangulation import VolumetricTriangulationNet
from mvn.models.loss import KeypointsMSELoss, KeypointsMSESmoothLoss, KeypointsMAELoss, KeypointsL2Loss, VolumetricCELoss, LimbLengthError
from mvn.utils import img, multiview, op, vis, misc, cfg
from mvn.utils.cfg import config, update_config, update_dir
from mvn import datasets
from mvn.datasets import utils as dataset_utils
from mvn.utils.vis import JOINT_NAMES_DICT
train_dataloader = DataLoader(
    cc,
    batch_size=4,
    shuffle=True,  # debatable
    sampler=None,
    collate_fn=dataset_utils.make_collate_fn(randomize_n_views=config.train.randomize_n_views,
                                             min_n_views=config.train.min_n_views,
                                             max_n_views=config.train.max_n_views),
    num_workers=0,
    worker_init_fn=dataset_utils.worker_init_fn,  # 随机种子
    pin_memory=True
)
iterator = enumerate(train_dataloader)
i = 0
for iter_i, batch in iterator:
    if i==0:
        batch_l = batch
        i+=1
        break
def build_coord_volume(coord_volume_size, position, sizes, base_point, theta, axis, device):
    # build coord volume
    xxx, yyy, zzz = torch.meshgrid(torch.arange(coord_volume_size, device=device),
                                   torch.arange(coord_volume_size, device=device),
                                   torch.arange(coord_volume_size, device=device))
    # xxx→tensor(64,64,64)，dim=-1说明新的维度添加到最后
    grid = torch.stack([xxx, yyy, zzz], dim=-1).type(torch.float)
    # grid→tensor(64,64,64,3)最后一个3是因为正在堆叠3个3维张量
    grid = grid.reshape((-1, 3))
    # 重构之后的网格变成了（262144,3）
    # 这样得到的grid在三个维度上都变成了从0一直到63的一维张量了，实现了离散化
    # 这个变量是用来装离散之后的跟关节点
    grid_coord = torch.zeros_like(grid)
    # position = base_point -size/2(平移到2500*2500*2500这个立方体中心后的根关节位置)
    # grid[:,0]..这些都是[0-63]的张量
    #
    grid_coord[:, 0] = position[0] + (sizes[0] / (coord_volume_size - 1)) * grid[:, 0]
    grid_coord[:, 1] = position[1] + (sizes[1] / (coord_volume_size - 1)) * grid[:, 1]
    grid_coord[:, 2] = position[2] + (sizes[2] / (coord_volume_size - 1)) * grid[:, 2]
    # 根据变回（64*64*64*3），
    coord_volume = grid_coord.reshape(coord_volume_size, coord_volume_size, coord_volume_size, 3)
    # 根关节作为中心1
    center = torch.from_numpy(base_point).type(torch.float).to(device)
    # rotate旋转
    coord_volume = coord_volume - center
    coord_volume = volumetric.rotate_coord_volume(coord_volume, theta, axis)
    coord_volume = coord_volume + center
    return coord_volume
def calc_ga_mask(keypoints_3d_gt, coord_volume, sigma=2):
    """
    :param keypoints_3d_gt:  (batch_size, n_joints, 3)
    :param coord_volume: (batch_size, d, h, w, 3)
    :return: ga_mask_gt: (batch_size, n_joints, d, h, w)
    """
    shape = coord_volume.shape
    # .view不改变数据的情况下重塑张量==reshape, unsqueeze(2)用于在对应位置增加维数
    # 该张量捕获体积中的每个点与每个关键点的相对位置
    delta = coord_volume.view(shape[0], 1, -1, 3) - keypoints_3d_gt.unsqueeze(2)  # (b, j, d*h*w, 3)
    # 体积中每个点与关键点之间的欧式距离从而得出3D空间中空间分离的度量
    dist = torch.norm(delta, dim=3)  # (b, j, d*h*w)
    # sigma按坐标体积中相邻点之间的距离进行缩放，在体积不同分辨率下保持一致注意力比例
    sigma *= torch.norm(coord_volume[0, 0, 0, 1] - coord_volume[0, 0, 0, 0])
    tmp_size = sigma * 2.5
    # softmax
    ga_mask_gt = F.softmax((-torch.square(dist) / (2 * sigma ** 2)), dim=2).float()  # (b, j, d*h*w)
    return ga_mask_gt
batch_size=4
device='cpu'
base_points = torch.zeros(batch_size, 3, device=device)
coord_volumes = torch.zeros(batch_size, 64, 64, 64, 3, device=device)
coord_volumes_aux = torch.zeros(batch_size, 64 // 4,64 // 4, 64 // 4, 3,
                                device=device)
cuboids = []
import torch
from torch import nn
import torch.nn.functional as F
from mvn.utils import op, multiview, img, misc, volumetric
from mvn.models import pose_resnet
from mvn.models.v2v_net import V2VNet
for batch_i in range(batch_size):
    keypoints_3d = batch['pred_keypoints_3d'][batch_i]
    base_point = keypoints_3d[6, :3]
    base_points[batch_i] = torch.from_numpy(base_point).to(device)
    sizes = np.array([2500, 2500, 2500])
    aux_sizes = sizes - 3 * sizes / (64 - 1)
    position = base_point - sizes / 2
    cuboid = volumetric.Cuboid3D(position, sizes)
    cuboids.append(cuboid)
    theta = np.random.uniform(0.0, 2 * np.pi)
    axis = [0, 0, 1]  # z axis绕z旋转
    coord_volumes[batch_i] = build_coord_volume(64, position, sizes, base_point, theta, axis, device)
    coord_volumes_aux[batch_i] = build_coord_volume(64 // 4, position, aux_sizes, base_point, theta,
                                                         axis, device)    
```



![image-20240418085044204](D:\Work_APP\Typora\assets\image-20240418085044204.png)

![image-20240418085819923](D:\Work_APP\Typora\assets\image-20240418085819923.png)









## 模型分布式训练方法

## 一、单机单卡

1.模型拷贝 model.cuda()

2.数据拷贝（每步） data=data.cuda()

3.基于torch.cuda.is_available()判断是否可用

4.模型保存与加载：（1）torch.save 模型、优化器、其他变量

​								(2)torch.load(file.pt,map_location=torch.device("cuda"/"cuda:0"/"cpu"))	

第一种通过命令行将CUDA_VISIBLE_DEVICES设置为

![image-20240416101922420](D:\Work_APP\Typora\assets\image-20240416101922420.png)第二种方式在代码里做os.environ

![image-20240416102021972](D:\Work_APP\Typora\assets\image-20240416102021972.png)					

模型拷贝

![image-20240416102324538](D:\Work_APP\Typora\assets\image-20240416102324538.png)

数据拷贝，在训练里



![image-20240416102410596](D:\Work_APP\Typora\assets\image-20240416102410596.png)

它调用的是tensor.cuda()

![image-20240416102446805](D:\Work_APP\Typora\assets\image-20240416102446805.png)

![image-20240416102552087](D:\Work_APP\Typora\assets\image-20240416102552087.png)

模型保存就不用变了

加载的时候要补充一下



![image-20240416102736796](D:\Work_APP\Typora\assets\image-20240416102736796.png)

```
checkpoints这行要加一个map_location
```

## 二、单机多卡

1. torch.nn.DataParallel
   1）好处，简单一行代码，包裹model即可 

   ```
   model = DataParallel(model.cuda(),device_ids=[0,1,2,3])
   data=data.cuda()
   ```

   2)模型保存加载

   ```
   torch.save注意模型需要调用model.module.state_dict
   torch.load需要注意map_location的使用
   ```

   3）缺点：单进程，效率慢，不支持多机，不支持模型并行

   4）注意事项：此处batch_size应该是每个GPU的batch_size的总和

   首先先进行判断，计算一下设备的数量

![image-20240416103721516](D:\Work_APP\Typora\assets\image-20240416103721516.png)

（1）变成多卡的模型

![image-20240416103934561](D:\Work_APP\Typora\assets\image-20240416103934561.png) （2）对于模型保存。因为DataParallel会把原来的model变成model.moduel，所以这边要变成

```
model.module.state_dict()
```

![image-20240416104214100](D:\Work_APP\Typora\assets\image-20240416104214100.png)

（3）对于模型加载

这里的就不用加.module了，在DataParallel执行之后

![image-20240416104438762](D:\Work_APP\Typora\assets\image-20240416104438762.png)

改一下设备

![image-20240416104537777](D:\Work_APP\Typora\assets\image-20240416104537777.png)

（4）batch_size（用的两张卡所以*2）

![image-20240416104712001](D:\Work_APP\Typora\assets\image-20240416104712001.png)

2.torch.nn.parallel.DistributedDataParallel(多进程多卡)

1）代码编写流程

```
torch.distributed.init_process_group("nccl", world_size=n_gpus, rank=args.local_rank)#初始化进程组，第一个参数GPU的通信方式。第二个，当前机子上的GPU卡的数量，rank在哪个GPU卡上
torch.cuda.set_device(args.local_rank) #该语句作用相当于CUDA_VISIBLE_DEVICES环境变量
model = DistributedDataParallel(model.cuda(args.local_rank),device_ids=[args.local_rank])# 模型包裹
train_sampler = DistributedSampler(train_dataset) #源码位于 torch/utils/data/distributed.py
train_dataloader = DataLoader(..., sampler=train_sampler)，# sampler之后给了打乱的顺序了，就不需要再shuffle，在这里两者互斥的
data=data.cuda(args.local_rank)#local_rank 命令行传入，表示当前这个节点的第几张卡
```

以下是DistributedSampler的部分源码

num_sampled是每张卡上得到的数据的数量，dataset的总数量/卡数，seed是（索引）打乱的随机种子

![image-20240416105909150](D:\Work_APP\Typora\assets\image-20240416105909150.png)

这个函数最主要的方法是，iter，它会返回一串数据的索引，然后这个索引给dataloader，dataloader构建batch

![image-20240416105956670](D:\Work_APP\Typora\assets\image-20240416105956670.png)

seed是固定，epoch=0，一开始的时候，希望每个周期训练数据都有被打乱。改变seed或者传入epoch

![image-20240416110124460](D:\Work_APP\Typora\assets\image-20240416110124460.png)

从第rank索引的数据开始，每隔num_replicas去取数据

举个例子，如果有4张卡，第一张卡的索引数据就是1,5,9，第二张卡2,6,10.这个索引被打乱了，一旦种子变了，分配到每个卡的数据也变了

![image-20240416110404294](D:\Work_APP\Typora\assets\image-20240416110404294.png)

2.执行命令

```
python -m torch.distributed.launch --nproc_per_node=n_gpus train.py## 第二个会构建很多进程，进程数由nproc_per_node决定
```

3.模型保存与加载

```
torch.save 在local_rank=O的位置进行保存，同样注意调用model.module.state_dict()
torch.load 注意map_location 指定哪个设备上
```

4.注意事项

```
train.py中要有接受local_rank的参数选项,launch会传入这个参数如
每个进程的batch_size应该是一个GPU所需要的batch_size大小
在每个周期开始处，调用train_sampler.set_epoch(epoch)可以使得数据充分打乱，可以使得在每个周期运行中，每张卡的数据索引是不一样的
有了sampler，就不要在DataLoader中设置shuffle=True了
```

代码修改，（1）需要用parser将local_rank，通过torch.distributed.launch自动传入一个local参数进来，所以要用parser接着

![image-20240416111943906](D:\Work_APP\Typora\assets\image-20240416111943906.png)

（2）相当于修改了环境变量可用的CUDA的那个

![image-20240416112307535](D:\Work_APP\Typora\assets\image-20240416112307535.png)

（3）包一层接口

![image-20240416112454517](D:\Work_APP\Typora\assets\image-20240416112454517.png)

(4)创建一个trian sampler

传入dataset去构建dataloader

![image-20240416112605756](D:\Work_APP\Typora\assets\image-20240416112605756.png)

![image-20240416112818435](D:\Work_APP\Typora\assets\image-20240416112818435.png)

（5）evaluation，不需要sampler

![image-20240416112923747](D:\Work_APP\Typora\assets\image-20240416112923747.png)

（6）数据拷贝到指定GPU上

![image-20240416113002431](D:\Work_APP\Typora\assets\image-20240416113002431.png)

并且local_rank也等于0才执行，在GPU0上进行保存就好不需要再每个都进行保存

![image-20240416113039433](D:\Work_APP\Typora\assets\image-20240416113039433.png)

（7）每个周期开始要打乱

![image-20240416113246213](D:\Work_APP\Typora\assets\image-20240416113246213.png)

这里应该写成trainloader.sampler.set_epoch()， 否则loss产生周期性的震荡

# 2024.4.18 继续学习多头注意力机制的源码

继续结合源码阅读contextpose文献

1.先完成了3D骨架的可视化

![image-20240418161325494](D:\Work_APP\Typora\assets\image-20240418161325494.png)

## 学习Resnet

一、概述残差结构

![image-20240418215725555](D:\Work_APP\Typora\assets\image-20240418215725555.png)

左边这幅图原本以输入channel为64,3×3卷积层卷积核个数也是64为例的，这力为了方便对比改成256。

加号之前，输入矩阵和输出矩阵的高宽维度要相同

节省参数个数

![image-20240418220104577](D:\Work_APP\Typora\assets\image-20240418220104577.png)

二、残差结构的总体框架

整个框架一样，残差结构conv2、3、4、5

![image-20240418220207871](D:\Work_APP\Typora\assets\image-20240418220207871.png)

以下以resnet34为例，×几就代表堆叠几层

![image-20240418220334454](D:\Work_APP\Typora\assets\image-20240418220334454.png)

![image-20240418220355270](D:\Work_APP\Typora\assets\image-20240418220355270.png)

![image-20240418220410319](D:\Work_APP\Typora\assets\image-20240418220410319.png)

![image-20240418220417147](D:\Work_APP\Typora\assets\image-20240418220417147.png)

![image-20240418220423866](D:\Work_APP\Typora\assets\image-20240418220423866.png)

三、浅层和深层中的虚实残差结构的对比

详解实线残差结构和虚线残差结构，区别是：实线的残差结构输入shape和输出shape是一样的

（1）较浅的（50层以下的）残差结构而言

实线残差结构

![image-20240418220534914](D:\Work_APP\Typora\assets\image-20240418220534914.png)

虚线残差结构，输入的shape是[56,56,64]输出特征矩阵的shape是[28,28,128]， 通过stride将高和宽缩减为原来的一半，将128个卷积核来改变特征矩阵的深度，捷径处采用stride=2，它的高和宽也缩减为原来的一半

![image-20240418220548681](D:\Work_APP\Typora\assets\image-20240418220548681.png)



![image-20240418220754278](D:\Work_APP\Typora\assets\image-20240418220754278.png)

只有通过虚线残差结构得到输出之后，再将输出输入到实线残差结构当中才能保证输入特征矩阵的shape和输出特征矩阵的shape相等

2.深层的残差结构

（a）虚线的

![image-20240418221522808](D:\Work_APP\Typora\assets\image-20240418221522808.png)

​     注意原论文中：右侧虚线残差结构的主分支上，第一个1×1卷积层的布局为2，第二个3×3的卷积层步距是1，但是pytorch官方实现过程中是第一个1×1布局为1，第二个3×3卷积层步距为2，这样能够在imagenet的top1上提升大概0.5%的准确率。

1×1的卷积层只起到降维的作用，并没有改变高和宽。第二层3×3的步距为2，第三层增加深度，选项B（option）



虚线的残差结构，会将高宽深度都都变化，所以conv3-5的残差结构的第一次都是虚线残差结构，需要将上一层的高和宽以及深度调整为当前层所需要。

下采样，是通过conv第一层实现的。

​       区别在于，（50-152层）不太一样，因为对浅层结构而言（18-34）经过最大池化下采样之后就是[56,56,64]刚好是实现残差结构所需要的shape，所以对于浅层结构第一个conv2的第一层而言不需要采用虚线残差结构。但是对于深层结构，最大池化之后，conv2对应的第一个虚线残差层仅调整特诊矩阵的深度，不改变高宽。

<img src="D:\Work_APP\Typora\assets\image-20240418221801390.png" alt="image-20240418221801390" style="zoom:25%;" />

![image-20240418221529372](D:\Work_APP\Typora\assets\image-20240418221529372.png)

四、BN层，

目的：使一批Batch的feature满足均值为0方差为1的分布规律，加速训练提升准确性

<img src="D:\Work_APP\Typora\assets\image-20240418222745071.png" alt="image-20240418222745071" style="zoom:50%;" />

就是图片满足了均值为0方差为1的分布，但不能保证经过卷积层后也能，满足，调整一批输出feature map满足的分布，并不是调整某一个图像。

计算每个通道对应的均值和方差（一批数据同一个通道），ε的作用是防止分母为零。

γ和β是有些人认为，均值为0方差为1的效果不是最好的，这两个学习参数是通过反向传播学习来的。而均值方差是通过批数据计算统计得到的。

<img src="D:\Work_APP\Typora\assets\image-20240418223712641.png" alt="image-20240418223712641" style="zoom:50%;" />

![image-20240418223039927](D:\Work_APP\Typora\assets\image-20240418223039927.png)

举例子，计算BN处理

![image-20240418223426049](D:\Work_APP\Typora\assets\image-20240418223426049.png)

![image-20240418223447887](D:\Work_APP\Typora\assets\image-20240418223447887.png)

对应于深度的维度，channel1

注意BN的事项

![image-20240418223612240](D:\Work_APP\Typora\assets\image-20240418223612240.png)

![image-20240418223624043](D:\Work_APP\Typora\assets\image-20240418223624043.png)

迁移学习，要注意

有一些底层通用的网络是学习到的特征是通用的。

![image-20240418223819508](D:\Work_APP\Typora\assets\image-20240418223819508.png)

![image-20240418223913063](D:\Work_APP\Typora\assets\image-20240418223913063.png)

![image-20240418223926647](D:\Work_APP\Typora\assets\image-20240418223926647.png)

五、代码部分

1.基础模块

![image-20240418195834344](D:\Work_APP\Typora\assets\image-20240418195834344.png)



![image-20240418195817530](D:\Work_APP\Typora\assets\image-20240418195817530.png)

**对于50、101、152 他们的conv2所对应的残差结构的第一层只需要调整特征矩阵的深度，高度和宽度不需要调整**
**对于上述 conv3-conv5的虚线残差结构，不仅需要调整深度，要需要调整高度和宽度(缩减为原来的一半)**

![image-20240418213017147](D:\Work_APP\Typora\assets\image-20240418213017147.png)

# 2024.4.19继续学习contextpose程序运行机制

1.在2d图片的batch中生成一组关键点

![image-20240419095754470](D:\Work_APP\Typora\assets\image-20240419095754470.png)

# 2024.4.20 继续学习contextpose程序（二）

1. 导出二维关节热图
   看test文件，主要学习RGB图像的显示。

![image-20240420160950953](C:\Users\Admin\Desktop\image-20240420160950953.png)

2.聚合体积





# 2024.4.22 继续学习context pose程序（三）

1.进行体积聚合可视化

![image-20240423154725450](D:\Work_APP\Typora\assets\image-20240423154725450.png)



知识点:PCKh计算的是预测关键点与真实关键点之间距离的百分比。如果预测的关键点与真实关键点之间的距离小于某个阈值（通常是头部大小的一定比例），则该关键点被认为是正确的。

<img src="D:\Work_APP\Typora\assets\image-20240423160711638.png" alt="image-20240423160711638" style="zoom:50%;" />

# 2024.4.23 Contextpose的模型学习，attention_conv

补充知识点：分组卷积（在源码中的W_Pi有用到）

<img src="https://img-blog.csdnimg.cn/direct/723f878ef1664213a7969942f523e463.jpeg" alt="img" style="zoom: 33%;" />

分组卷积，就是将输入通道和输出通道划分成同样的组数，然后仅让处于相同组号的输入通道和输出通道相互进行全连接。如果记g为输入/输出通道所分的组数，则分组卷积能够将卷积操作的参数量和计算量都降低为普通卷积的1/g

# 2024.4.24 继续解析并归纳Contextpose中的attention_conv的结构

1.contextpose代码阅读和测试

详细的请见attention_conv.py文件和纸质笔记

2.测试本科毕设的源代码，以深蹲为例复现一下

# 2024.4.25赶紧把本科的毕设弄完

差不读弄完了，可以实现角度读取了

数据集注释文件的readme

```

```



# 2024.4.30重写LSTM模型并训练

```
MarkerData
标记位置（100hz）从63个标记
	视频关键点(20个标记)从同步的2D视频关键点三角化。
	使用四阶零滞后巴特沃斯滤波器对2D视频关键点进行滤波。
	2D视频关键点低通滤波(30hz)。
	使用OpenPose从两个智能手机摄像头估计2D视频关键点位置。
	从三角视频关键点预测的解剖标记(43个标记)。
- Video keypoints:
	- Neck, RShoulder, RElbow, RWrist, LShoulder, LElbow, LWrist, midHip, RHip, RKnee, RAnkle, LHip, LKnee, LAnkle, LBigToe, LSmallToe, LHeel, RBigToe, RSmallToe, RHeel.
	Anatomical markers predicted from the triangulated video keypoints:
	- C7_study, r_shoulder_study, L_shoulder_study, r.ASIS_study, L.ASIS_study, r.PSIS_study, L.PSIS_study, r_knee_study, L_knee_study, r_mknee_study, L_mknee_study, r_ankle_study, L_ankle_study, r_mankle_study, L_mankle_study, r_calc_study, L_calc_study, r_toe_study, L_toe_study, r_5meta_study, L_5meta_study, r_thigh1_study, r_thigh2_study, r_thigh3_study, L_thigh1_study, L_thigh2_study, L_thigh3_study, r_sh1_study, r_sh2_study, r_sh3_study, L_sh1_study, L_sh2_study, L_sh3_study, RHJC_study, LHJC_study, r_lelbow_study, L_lelbow_study, r_melbow_study, L_melbow_study, r_lwrist_study, L_lwrist_study, r_mwrist_study, L_mwrist_study.

```

# 2024.5.3学会使用Opencap给的数据集使用LSTM预测解剖标记点

dict_keys(['calibrationSettings', 'checkerBoard', 'height_m', 'iphoneModel', 'markerAugmentationSettings', 'mass_kg', 'openSimModel', 'sex', 'subjectID'])

对会话的键进行阅读

...没咋学就去爬山了

# 2024.5.4青年节！！！

今天的任务是，研究程序，能用自带数据库中的关键点

# 2024.5.6重新开始吧

今日任务，

1.用Opencap的3D坐标估计出标记点

2.继续再读3D Kinematics Estimation from Video with a Biomechanical Model and
Synthetic Training Data文献，做笔记总结

任务一：

把utilsAugmenter.py给复现了一下，遇到的问题的：

这个地方会报错，初步认为是读取Openpose的.trc文件中已经有了这个标签，不能再重复创建

```Python
# 把mark点添加到.trc文件中，因为这个文件已经有了这几个
for c, marker in enumerate(response_markers):
	x = unnorm2_outputs[:, c * 3]
	y = unnorm2_outputs[:, c * 3 + 1]
	z = unnorm2_outputs[:, c * 3 + 2]
	trc_file.add_marker(marker, x, y, z)
```

<img src="D:\Work_APP\Typora\assets\image-20240506145619841.png" alt="image-20240506145619841" style="zoom: 50%;" />

<img src="D:\Work_APP\Typora\assets\image-20240506153629830.png" alt="image-20240506153629830" style="zoom:67%;" />

用openpose默认的低分辨率模式下的解剖标记点：

<img src="D:\Work_APP\Typora\assets\image-20240506153526628.png" alt="image-20240506153526628" style="zoom:50%;" />

蓝色为解剖标记点，红色为openpose对应的关键点

![image-20240506170433845](D:\Work_APP\Typora\assets\image-20240506170433845.png)

<img src="D:\Work_APP\Typora\assets\image-20240506171059620.png" alt="image-20240506171059620" style="zoom:50%;" />

# 2024.5.7继续复现Opencap的部分代码



![image-20240507084011389](D:\Work_APP\Typora\assets\image-20240507084011389.png)

![image-20240507084018385](D:\Work_APP\Typora\assets\image-20240507084018385.png)

![image-20240507084027244](D:\Work_APP\Typora\assets\image-20240507084027244.png)

![image-20240507084040084](D:\Work_APP\Typora\assets\image-20240507084040084.png)

解剖标记点和

![image-20240507221710144](D:\Work_APP\Typora\assets\image-20240507221710144.png)



# 2024.5.8用opensim去可视化Opencap模型中的.trc

![image-20240508113516479](D:\Work_APP\Typora\assets\image-20240508113516479.png)

![image-20240513092755768](D:\Work_APP\Typora\assets\image-20240513092755768.png)

进行IK分析，

下面是膝关节在深蹲过程的角度

![image-20240508155833166](D:\Work_APP\Typora\assets\image-20240508155833166.png)

下面是踝关节在深蹲过程中的角度

![image-20240508160852973](D:\Work_APP\Typora\assets\image-20240508160852973.png)



接下来的任务：

1.将受试者的视频拆解成帧

2.将相机参数和骨长参数整理出来，作为模型的输入。

3.估计出3D坐标



# 2024.5.9完成昨天布置的任务



![image-20240509093713355](D:\Work_APP\Typora\assets\image-20240509093713355.png)

# 2024.5.10对视频和光学动捕进行帧对齐

1.对齐视频帧和光学动帧

使用的方法是：通过2D检测器检测到的关键点的轨迹（位置），去计算关键点的速度，找速度同步的点，就可以对齐对视角的视频

# 2024.5.11对视频和光学动捕进行帧对齐

1.训练时需要的文件

```

train_labels --放边界框的（bbox）
limb_length --放骨长
image_batch --图片的批次
proj_matricies_batch  --投影矩阵的批次
```

# 2024.5.13将opencap的模型整理成contextpose的输入形式

1.利用maskRCNN获取人的边界框信息。

阅读maskrcnn，benchmark相关的.md文件。

摘要文件，记录其中的一些重要变量

```
##Imagelist 
为了在同一批次处理中支持不同大小和宽高比的图像。使用0填充弄到一样
image list 的作用是 将张量、列表输入转化成imagelist对象
##boxlist
保存特定一组边界框（N*4张量），以及图片大小（宽高）允许对边界框执行集合变化（裁剪缩放和翻转）。该类接受来自两种不同输入格式的边界框
XyXy--四个顶点形式
XyWh--两个顶点+图片宽高

```

![img](https://img-blog.csdnimg.cn/img_convert/894f3726dfa2bc1fa01841114b356f16.png)

# 2024.5.15先把poseresnet的输出通道数改一下看一下它预测出来的特征有无变多，多的是哪一些

上述方法不太行呀，因为，足部和其他关节应该是存在约束关系的，

# 2024.5.21训练二维模型，目标是将输出为17的二维模型变成输出为23的

学习损失函数，

![image-20240521090641656](D:\Work_APP\Typora\assets\image-20240521090641656.png)



![image-20240521091517359](D:\Work_APP\Typora\assets\image-20240521091517359.png)

需要调参的情况

![image-20240521091527416](D:\Work_APP\Typora\assets\image-20240521091527416.png)

红色的正常拟合，

![image-20240521091636554](D:\Work_APP\Typora\assets\image-20240521091636554.png)



![image-20240521091658444](D:\Work_APP\Typora\assets\image-20240521091658444.png)

震荡

![image-20240521091950737](D:\Work_APP\Typora\assets\image-20240521091950737.png)

恰好拟合

![image-20240521091723489](D:\Work_APP\Typora\assets\image-20240521091923911.png)

调参

过拟合：样本数量太少了？

1.（数据增强）

2.早停法

3.dropout丢参数量（让它的参数量少点，学的少点）

<img src="D:\Work_APP\Typora\assets\image-20240521092248959.png" alt="image-20240521092248959" style="zoom: 50%;" />

4.学习率0.1 0.001

5.epoch

欠拟合：没学到位

1.加深网络的层数

2.尽量用一些非线性激活函数比如relu

拟合但震荡，200-20000图片增加太多了，学习率是不是太高了，

恰好拟合

1.大部分应该是

不收敛

是不是模型和数据不适配，数据预处理和标签有问题，网络设计有问题



过程，

搭建模型，先用一些小样本试一下，比如原来有10000张图片，我先只用1000张，调参解决不了就换模型







## 解决问题，重构foot数据验证集

1.获取foot数据中，有足部标记的imageid

2.用image_id检索完整的keypoints验证集中的相应数据，获取其中的身体的17个关键点的坐标信息，bbox信息，分割信息

可以训练了

但是验证的时候遇到错误

```
Traceback (most recent call last):
  File "/mnt/e/test/human-pose-estimation.pytorch-master/pose_estimation/train.py", line 229, in <module>
    main()
  File "/mnt/e/test/human-pose-estimation.pytorch-master/pose_estimation/train.py", line 203, in main
    writer_dict)
  File "/mnt/e/test/human-pose-estimation.pytorch-master/pose_estimation/../lib/core/function.py", line 182, in validate
    filenames, imgnums)
  File "/mnt/e/test/human-pose-estimation.pytorch-master/pose_estimation/../lib/dataset/coco.py", line 374, in evaluate
    oks_thre)
  File "/mnt/e/test/human-pose-estimation.pytorch-master/pose_estimation/../lib/nms/nms.py", line 117, in oks_nms
    oks_ovr = oks_iou(kpts[i], kpts[order[1:]], areas[i], areas[order[1:]], sigmas, in_vis_thre)
  File "/mnt/e/test/human-pose-estimation.pytorch-master/pose_estimation/../lib/nms/nms.py", line 88, in oks_iou
    e = (dx ** 2 + dy ** 2) / vars / ((a_g + a_d[n_d]) / 2 + np.spacing(1)) / 2
ValueError: operands could not be broadcast together with shapes (23,) (17,) 
```

评估方法有问题，sigmas只有（17）个关键点的评估，因此要找一种方法去评估23个关键点，

https://github.com/jin-s13/COCO-WholeBody/blob/master/evaluation/myeval_foot.py

对下面代码重新进行评估部分的重写

![image-20240521230218055](D:\Work_APP\Typora\assets\image-20240521230218055.png)

# 2024.5.22对足部进行评估使得程序能够正常运行

解决问题1：完整训练评估

```
for idx, kpt in enumerate(preds):
    _kpts.append(
        {
            'keypoints': kpt,
            'center': all
        }
    )
    包含，'keypoints','center'，'scale','are','score','image'等几个标签
```

通过更改评估文件已经可以运行了，

解决问题2：调试过程的路径问题

![image-20240522153113888](D:\Work_APP\Typora\assets\image-20240522153113888.png)

明明在这个路径下，比如说要调用lib.models.pose_resnet的里面的函数，但是会报错  ModuleNotFoundError: No module named 'models.pose_resnet'

解决方法：在//models// /__init__/.py中加入import pose_resnet告诉编译器路径



# 2024.5.23用cocowholebody进行实验

1.对数据集的关键点进行增加，并修改num_joint等参数（在config文件和coco.py文件中均有）

2.在云服务器上进行了一个epoch的训练

3.测试自测数据图片，大致看看准确率（关键点定位是否正确）

这个是网络的结构:

```python
PoseResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (6): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (7): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (6): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (7): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (8): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (9): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (10): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (11): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (12): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (13): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (14): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (15): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (16): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (17): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (18): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (19): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (20): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (21): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (22): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (23): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (24): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (25): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (26): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (27): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (28): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (29): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (30): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (31): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (32): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (33): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (34): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (35): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (deconv_layers): Sequential(
    (0): ConvTranspose2d(2048, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
  )
  (final_layer): Conv2d(256, 23, kernel_size=(1, 1), stride=(1, 1))
)

```

## 模型加载测试代码如下

```python
import torch
from lib.core.config import config
from lib.core.config import update_config
# 加载训练好的权重文件
checkpoint = torch.load('/mnt/e/test/human-pose-estimation.pytorch-master/output/coco/pose_resnet_152/256x192_d256x3_adam_lr1e-3/model_best.pth.tar')
state_dict = checkpoint['state_dict']
new_state_dict = {}

prefixes_to_remove = ['backbone.', 'keypoint_head.']

# 遍历原始权重字典
for key, value in state_dict.items():
    new_key = key
    # 去掉指定前缀
    for prefix in prefixes_to_remove:
        if key.startswith(prefix):
            new_key = key[len(prefix):]
    new_state_dict[new_key] = value
for k, v in state_dict.items():
    new_state_dict[k.replace('module.', '')] = v  # 存储加载新权重
    
    
class Args:
    def __init__(self):
        self.cfg = '/mnt/e/test/human-pose-estimation.pytorch-master/experiments/coco/resnet152/384x288_d256x3_adam_lr1e-3.yaml'
        self.gpus = '0'
        self.workers = 1
args = Args()
update_config(args.cfg)
# 加载模型
from lib.models.posenet.py import *
model = get_pose_net(config, False)
model.cuda()
# 

# 加载权重
model.load_state_dict(new_state_dict, strict=True)
# 
from PIL import Image
import torch
from torchvision import transforms
image = Image.open('data/S9_Directions_1.54138969_000001.jpg')
#保留图片原始尺寸
import numpy as np
width = np.array(image).shape[0]
height = np.array(image).shape[1]

Transforms = transforms.Compose([transforms.Resize((384,288)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])])
resized_image = Transforms(image)

model = model.to('cuda')
resized_image = resized_image.cuda()
resized_image = resized_image.view(1,3,384,288)
output = model(resized_image)

# 对热图进行
# 采用最值形式
max_vals, max_indices = torch.max(output.view(1,23,-1), dim=2)

# 转换为坐标
max_indices = max_indices.view(1, 23)
pred_coords = torch.zeros(1, 23, 2)

for i in range(23):
    idx = max_indices[0, i]
    # 先记住吧，线性索引变成二维索引，就是行索引线性索引除以‘//’图片的高，列索引x就是线性索引取余‘高’
    # 行索引
    y = idx // 48
    # 列索引
    x = idx % 48
    pred_coords[0, i, 0] = x
    pred_coords[0, i, 1] = y
 	
    # 假设热图大小和原始图像大小不同，进行比例缩放(恢复成原始图片的像素大小)
    scale_factor_x = width / config.MODEL.EXTRA.HEATMAP_SIZE[1]
    scale_factor_y = height / config.MODEL.EXTRA.HEATMAP_SIZE[0]
    pred_coords[:, i, 0] *= scale_factor_x
    pred_coords[:, i, 1] *= scale_factor_y
pred_coords = pred_coords.numpy().reshape(23,2)
import cv2
image = cv2.imread('data/S9_Directions_1.54138969_000001.jpg')
for joint in pred_coords:
    x, y = joint
    cv2.circle(image, (int(x), int(y)), 5, (0, 0, 255), -1)  # 在 (x, y) 位置绘制一个半径为5的红色实心圆
cv2.imshow('Image with Joints', image)
cv2.waitKey(0)
cv2.destroyAllWindows()

# 原图尺寸
orig_height = 900
orig_width = 1159

# 热图尺寸
heatmap_height = 64
heatmap_width = 48

# 计算缩放比例
scale_x = orig_width / heatmap_width
scale_y = orig_height / heatmap_height

# 按比例缩放坐标
orig_coords = pred_coords.clone()
orig_coords[..., 0] *= scale_x
orig_coords[..., 1] *= scale_y

# 900*1159 显示图片
import matplotlib.pyplot as plt
image_np = np.array(image)
plt.figure(figsize=(8, 6))
plt.imshow(image_np)
plt.scatter(orig_coords[:, 0], orig_coords[:, 1], c='red', marker='o')
plt.title('2D Scatter Plot on Original Image')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.show()

```

## 使用边界框裁剪图片

```python
import cv2 
import matplotlib.pyplot as plt
image_path = 'data/S9_Directions_1.54138969_000001.jpg'
image = Image.open('data/S9_Directions_1.54138969_000001.jpg')
bboxes = np.load('/mnt/e/test/human-pose-estimation.pytorch-master/data/h36m/bboxes-Human36M-GT.npy',allow_pickle=True)

bboxes = bboxes.item()
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda img: img[:, bbox_S9_1[1]:bbox_S9_1[3], bbox_S9_1[0]:bbox_S9_1[2]]),
    #img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225]),
    transforms.Resize((384,288)),
    transforms.ToPILImage()
])
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.ToPILImage()
])
#transform = transforms.Compose([
#    transforms.ToTensor(),
#    transforms.Lambda(lambda img: img[:, (bbox[1]-45):(bbox[3]+45), bbox[0]:bbox[2]]),
#    transforms.ToPILImage()
#])
cropped_image = transform(image)
transform_2 = transforms.Compose([transforms.ToTensor()])
image_tensor = transform_2(cropped_image)
# 应用变换
image_tensor = image_tensor.view(1,3,384,288)
image_tensor = image_tensor.cuda()
output = model(image_tensor)
# human3.6 有用到的
index_v= [0,1,2,3,6,7,8,13,16,17,18,19,20,24,25,26,27,28]

# 将热图转化为关节点坐标
def heatmaps_to_coordinates(heatmaps):
    heatmaps = torch.squeeze(heatmaps)
    coords = []
    for heatmap in heatmaps:
        max_val, max_idx = torch.max(heatmap.view(-1), 0)
        y, x = divmod(max_idx.item(), heatmap.size(1))
        coords.append([x, y])
    return coords
def get_keypoints_from_heatmaps(heatmaps):
    N, C, H, W = heatmaps.shape
    keypoints = []

    for n in range(N):
        keypoints_n = []
        for c in range(C):
            # 获取当前关节点的热力图
            heatmap = heatmaps[n, c]

            # 找到最大值索引
            max_val, max_idx = torch.max(heatmap.view(-1), 0)
            y, x = divmod(max_idx.item(), W)

            keypoints_n.append((x, y))
        keypoints.append(keypoints_n)
    
    return keypoints


# 显示原始图片和裁剪后的图片
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.imshow(image)
plt.title('Original Image')

plt.subplot(1, 2, 2)
plt.imshow(cropped_image)
plt.title('Cropped Image')

#plt.show()
orig_width, orig_height = original_image.size
heatmap_width, heatmap_height = 96, 72

scale_x = orig_width / heatmap_width
scale_y = orig_height / heatmap_height

# 绘制关节点
from PIL import Image, ImageDraw
draw = ImageDraw.Draw(image)
# 在每个关节坐标处绘制一个圆点
for keypoints_n in keypoints:
    for (x, y) in keypoints_n:
        # 将热力图中的坐标转换为原图中的坐标
        orig_x = int(x * scale_x)
        orig_y = int(y * scale_y)
        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))

# 显示图片
image.show()
```

<img src="D:\Work_APP\Typora\assets\image-20240617153349524.png" alt="image-20240617153349524" style="zoom:50%;" />

直接放进去图片会被压扁，不利于预测

<img src="D:\Work_APP\Typora\assets\image-20240617153754441.png" alt="image-20240617153754441" style="zoom:50%;" />

调整成合适的高宽比，然后经过归一化的后的

![image-20240617160802115](D:\Work_APP\Typora\assets\image-20240617160802115.png)

coco的基准真值

![image-20240618205236164](D:\Work_APP\Typora\assets\image-20240618205236164.png)

## 平移所有关键点中心，然后放大相应倍数

<img src="D:\Work_APP\Typora\assets\image-20240620170507493.png" alt="image-20240620170507493" style="zoom:50%;" />

![image-20240620171225904](D:\Work_APP\Typora\assets\image-20240620171225904.png)

# 2024.5.24用MPII进行训练，学习颈部特征



将MPII和human3.6m都转化成COCO数据集格式

1.human3.6m的coco格式

```Python
# 下面这段代码是做个测试，用于画出mpii对应的3D坐标
import h5py
import numpy as np
p_1 = 'F:\E\ContextPose-PyTorch-release-master\data\human36m\extra\\una-dinosauria-data\h36m\S5\MyPoses\\3D_positions\Directions 1.h5'
with h5py.File( p_1, 'r') as poses_file:
    a = poses_file
    b = np.array(a['3D_positions']).T.reshape(-1, 32, 3)
    print(b.shape)
valid_joints = (3,2,1,6,7,8,0,12,13,15,27,26,25,17,18,19) + (14,)
CONNECTIVITY_DICT = {
    'cmu': [(0, 2), (0, 9), (1, 0), (1, 17), (2, 12), (3, 0), (4, 3), (5, 4), (6, 2), (7, 6), (8, 7), (9, 10), (10, 11), (12, 13), (13, 14), (15, 1), (16, 15), (17, 18)],
    'coco': [(0, 1), (0, 2), (1, 3), (2, 4), (5, 7), (7, 9), (6, 8), (8, 10), (11, 13), (13, 15), (12, 14), (14, 16), (5, 6), (5, 11), (6, 12), (11, 12)],
    "mpii": [(0, 1), (1, 2), (2, 6), (5, 4), (4, 3), (3, 6), (6, 7), (7, 8), (8, 9), (8, 12), (8, 13), (10, 11), (11, 12), (13, 14), (14, 15)],
    "human36m": [(0, 1), (1, 2), (2, 6), (5, 4), (4, 3), (3, 6), (6, 7), (7, 8), (8, 16), (9, 16), (8, 12), (11, 12), (10, 11), (8, 13), (13, 14), (14, 15)],
    "kth": [(0, 1), (1, 2), (5, 4), (4, 3), (6, 7), (7, 8), (11, 10), (10, 9), (2, 3), (3, 9), (2, 8), (9, 12), (8, 12), (12, 13)],
}
c = b[0]
# 归一化

[y_max,y_min] = [np.max(c[:,1]),np.min(c[:,1])]
[x_max,x_min] = [np.max(c[:,0]),np.min(c[:,0])]
[z_max,z_min] = [np.max(c[:,2]),np.min(c[:,2])]
for i in enumerate(range(32)):
    c[i,1] = (c[i,1] - y_min) /(y_max - y_min)
    c[i,0] = (c[i,0] - x_min) / (x_max - x_min)
    c[i, 2] = (c[i, 2] - z_min) / (z_max - z_min)
keypoints_test = []
for index in valid_joints:
    keypoints_test.append(c[0][index])
from matplotlib import pylab as plt
fig = plt.figure()
ax1 = fig.add_subplot(121, projection='3d')



ax1.set_xlim3d(-1, 1)
ax1.set_ylim3d(-1, 1)
ax1.set_zlim3d(-1, 1)
for group in CONNECTIVITY_DICT["mpii"]:
    plotX_g = [keypoints_test[i][0] for i in group]
    plotY_g = [keypoints_test[i][1] for i in group]
    plotZ_g = [keypoints_test[i][2] for i in group]
ax1.plot(plotX_g, plotY_g, plotZ_g)
ax1.set_xlabel('X Coordinate')
ax1.set_ylabel('Y Coordinate')
ax1.set_zlabel('Z Coordinate')
plt.show()

```



# 2024.5.25接着训练训练过的模型，并继续制作h36m的COCO形式



```
      switch part
        case 'rootpos'
          joints = 1;
        case 'rootrot'
          joints = 1;
        case 'leftarm'
          joints = 17:24;% p/p2/a fine
        case 'rightarm'
          joints = 25:32;% p/p2/a fine
        case 'head'
          joints = 14:16;% p/p2/a fine
        case 'rightleg'
          joints = 2:6;% p/p2/a fine
        case 'leftleg'
          joints = 7:11;% p/p2/a fine
        case 'upperbody'
          joints = [14:32];% p/p2/a fine
        case 'arms'
          joints = [16:32];% p/p2/a fine
        case 'legs'
          joints = 1:11;% p/p2/a fine
        case 'body'
          joints = [1 2 3 4 7 8 9 12 13 14 15 16 17 18 19 20 25 26 27 28];% p/p2/a fine
				case 'full'
          joints = 1:32;% p/p2/a fine
        otherwise
          error('Unknown');
      end
```

<img src="D:\Work_APP\Typora\assets\image-20240525152018605.png" alt="image-20240525152018605" style="zoom:50%;" />

为了制作标签方便呢，我这边先按照（mmpose中的tools/dataset_converters/preprocess_h36m.py）h36m的方式进行数据处理吧

```
python tools/dataset_converters/preprocess_h36m.py --metadata F:\data\h36m_raw_data\metadata.xml --original F:\data\h36m_raw_data /
--extracted F:\data\processed_h36m\extracted --processed F:\data\processed_h36m\processed
```

# 2024.5.27继续将MPII和human3.6变成一个联合数据集

下面是h36m_to_coco

```
python tools/dataset_converters/h36m_to_coco.py --ann-file F:\data\processed_h36m\processed\annotation_body3d\fps10\h36m_train.npz --camera-param-file F:\data\processed_h36m\processed\annotation_body3d\cameras.pkl --out-file F:\data\processed_h36m\processed\annotation_body3d\h36m_train_coco.json
```

报了很奇怪的错误

![image-20240527083109934](D:\Work_APP\Typora\assets\image-20240527083109934.png)

```
python tools/dataset_converters/h36m_to_coco.py --ann-file tests/data/h36m/test_h36m_body3d.npz --camera-param-file tests/data/h36m/cameras.pkl --out-file tests/data/h36m/h36m_coco.json
```

# 2024.5.28赶紧了！再试试转coco，看看哪里出问题了

```
'F:\\data\\processed_h36m\\processed\\annotation_body3d\\fps10\\h36m_test.npz'
'F:\\data\\processed_h36m\\processed\\images'
'F:\data\processed_h36m\processed\annotation_body3d\cameras.pkl'

h36m_data = np.load('F:\\data\\processed_h36m\\processed\\annotation_body3d\\fps10\\h36m_test.npz')
h36m_camera_params = mmengine.load('F:\data\processed_h36m\processed\\annotation_body3d\cameras.pkl')
imgnames = h36m_data['imgname']
tasks = [(idx, fn, 'F:\\data\\processed_h36m\\processed\\images') for idx, fn in enumerate(imgnames)]

```



很烦，又报错- -！，初步找寻原因可能问题出现在可能是这个多进程处理默认的路径是在['E:\\test\\mmpose-main']下，而这个路径下没有图片

```
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Work_APP\Anconda\envs\openmmlab\lib\multiprocessing\spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "D:\Work_APP\Anconda\envs\openmmlab\lib\multiprocessing\spawn.py", line 125, in _main
    prepare(preparation_data)
  File "D:\Work_APP\Anconda\envs\openmmlab\lib\multiprocessing\spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Work_APP\Anconda\envs\openmmlab\lib\multiprocessing\spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "D:\Work_APP\Anconda\envs\openmmlab\lib\runpy.py", line 264, in run_path
    code, fname = _get_code_from_file(run_name, path_name)
  File "D:\Work_APP\Anconda\envs\openmmlab\lib\runpy.py", line 234, in _get_code_from_file
    with io.open_code(decoded_path) as f:
OSError: [Errno 22] Invalid argument: 'E:\\test\\mmpose-main\\<input>'
```

不支持多进程干活

所以在windows下我改成了单进程

```python
from mmengine.utils import (Timer, mkdir_or_exist, track_parallel_progress,
                            track_progress)
h36m_imgs = mmengine.track_progress(
    _get_img_info, tasks)
```

所以在linux下运行程序

```
python tools/dataset_converters/h36m_to_coco.py --ann-file /mnt/f/data/processed_h36m/processed/annotation_body3d/fps10/h36m_train.npz --camera-param-file /mnt/f/data/processed_h36m/processed/annotation_body3d/cameras.pkl --img-root /mnt/f/data/processed_h36m/processed/images --out-file  /mnt/f/data/processed_h36m/processed/annotation_body3d/h36m_train_coco.json
```

命令行如下

报错了

```
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/openmmlab/lib/python3.8/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "tools/dataset_converters/h36m_to_coco.py", line 56, in wrapped_func
    return func(*args)
  File "tools/dataset_converters/h36m_to_coco.py", line 81, in _get_ann
    kpt_3d = _keypoint_camera_to_world(kpt_3d, camera_params, imgname)
  File "tools/dataset_converters/h36m_to_coco.py", line 37, in _keypoint_camera_to_world
    camera = SimpleCamera(camera_params[cam_key])
KeyError: ('S9\\S9', '54138969\\S9')
"""
```

观察键值错误，分析原因感觉可能是因为在windows下用'\\\\'双斜杠表示地址，在linux下都是用反斜杠'/'作为输入的



错误二：不知道为啥呀，错误现象，在Linux中跑通了，但是保存下来的文件没有images，和annotations这两个值

![image-20240528143840850](D:\Work_APP\Typora\assets\image-20240528143840850.png)

但是在前面保存却有

![image-20240528144607183](D:\Work_APP\Typora\assets\image-20240528144607183.png)

# 2024.5.29使用MPII和human3.6m进行主干网络的联合微调

```
以COCO的为例，目标检测部分
包含：'bbox'、'category_id'、'score'
```



yolov标注格式和VOC标注格式的转换：blog.csdn.net/didiaopao/article/details/120022845

利用训练好的yolv5:blog.csdn.net/didiaopao/article/details/120022845

# 2024.5.30制作MPII边界框

```
python train.py --data data/coco.yaml --cfg models/yolov5s.yaml --weight pretrained/yolov5s.pt  --epoch 100 --batch-size 16 --device 0
```

试试怎么在human3.6m上微调backbone



# 2024.5.31赶紧弄呀，将h36m送入resnet进行微调

  TEST_SET: ['S9', 'S11']
  TRAIN_SET: ['S1', 'S5', 'S7', 'S8']

# 2024.6.1继续将h36m送入resnet模型



以下代码是读取coco数据然后把keypoints标记到图片上

```python
from pycocotools.coco import COCO
import cv2
import numpy as np
import os
path = 'data/h36m/h36m_test_coco.json'
path_coco = 'data/coco/annotations/person_keypoints_val2017_foot_v1.json'
# h36m
test =  COCO(path)
imgIds = test.getImgIds(catIds=test.getCatIds(catNms=['person']))
imgs = test.loadImgs(imgIds)
image_file = imgs[0]['file_name'].replace('\\', '/')


# coco
test_coco = COCO(path_coco)
imgIds = test_coco.getImgIds(catIds=test_coco.getCatIds(catNms=['person']))
imgs_coco = test_coco.loadImgs(imgIds)
image_file = imgs_coco[0]['file_name'].replace('\\', '/')
# h36m
data_root = '/mnt/f/data/processed_h36m/processed/images'
image_path = os.path.join(data_root, image_file)

# coco
data_root = 'data/coco/images'
image_path = os.path.join(data_root,'val2017', image_file)

# h36m
imgIds=imgs[0]['id']
catIds = test.getCatIds(catNms=['person'])[0]
ann_ids = test.getAnnIds(1, 1, iscrowd=None)
ann =  test.loadAnns([1])
image = cv2.imread(image_path)
# coco
imgIds=imgs_coco[0]['id']
catIds = test_coco.getCatIds(catNms=['person'])[0]
ann_ids = test_coco.getAnnIds(imgIds,catIds, iscrowd=None)
ann =  test_coco.loadAnns(ann_ids)
image = cv2.imread(image_path)


joints_2d = np.zeros((21, 2), dtype=np.float32)
#将列表变成
for ipt in range(21):
    joints_2d[ipt, 0] = ann[0]['keypoints'][ipt * 3 + 0]
    joints_2d[ipt, 1] = ann[0]['keypoints'][ipt * 3 + 1]
for joint in joints_2d:
    x, y = joint
    cv2.circle(image, (int(x), int(y)), 5, (0, 0, 255), -1)  # 在 (x, y) 位置绘制一个半径为5的红色实心圆
cv2.imshow('Image with Joints', image)
cv2.waitKey(0)
cv2.destroyAllWindows()



```

下图是h36m的结果

<img src="D:\Work_APP\Typora\assets\image-20240601150222761.png" alt="image-20240601150222761" style="zoom:50%;" />

<img src="D:\Work_APP\Typora\assets\image-20240603091452966.png" alt="image-20240603091452966" style="zoom:50%;" />

以下是coco上的结果，23个关键点的结果：

![image-20240601163544007](D:\Work_APP\Typora\assets\image-20240601163544007.png)



找出现在输出通道的一个顺序：

将h36m放入COCO训练好的网络中去看输出结果并确定输出通道顺序。

效果很差，偏移多，不知道问题出现在映射算法还是模型不对。









写出输出通道顺序和

```
        self.actual_joints = {
 		0: "nose",
        1: "left_eye",
        2: "right_eye",
        3: "left_ear",
        4: "right_ear",
        5: "left_shoulder",
        6: "right_shoulder",
        7: "left_elbow",
        8: "right_elbow",
        9: "left_wrist",
        10: "right_wrist",
        11: "left_hip",
        12: "right_hip",
        13: "left_knee",
        14: "right_knee",
        15: "left_ankle",
        16: "right_ankle"
        # 足部的
        17: "LBigToe",
        18: "LSmallToe",
        19: "LHeel",
        20: "RBigToe",
        21: "RSmallToe",
        22: "RHeel"
            }
        
        self.union_joints = {
            0: 'root',
            1: 'rhip',
            2: 'rkne',
            3: 'rank',
            4: 'lhip',
            5: 'lkne',
            6: 'lank',
            7: 'belly',
            8: 'neck',
            9: 'nose',
            10: 'head',
            11: 'lsho',
            12: 'lelb',
            13: 'lwri',
            14: 'rsho',
            15: 'relb',
            16: 'rwri'
        }
   
   
         
          
```

```python
/home/ff/anaconda3/envs/pytorchgpu/bin/python3 /opt/pycharm-community-2022.3.3/plugins/python-ce/helpers/pydev/pydevconsole.py --mode=client --host=127.0.0.1 --port=41925 
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/mnt/e/test/human-pose-estimation.pytorch-master'])
PyDev console: starting.
Python 3.7.16 (default, Jan 17 2023, 22:20:44) 
[GCC 11.2.0] on linux
import torch
checkpoint = torch.load('/mnt/e/test/human-pose-estimation.pytorch-master/output/coco/pose_resnet_152/384x288_d256x3_adam_lr1e-3/checkpoint.pth.tar')
state_dict = checkpoint['state_dict']
new_state_dict = {}
for k, v in state_dict.items():
    new_state_dict[k.replace('module.', '')] = v  # 存储加载新权重
    
model = get_pose_net(config, True)
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
NameError: name 'get_pose_net' is not defined
# ------------------------------------------------------------------------------
# Copyright (c) Microsoft
# Licensed under the MIT License.
# Written by Bin Xiao (Bin.Xiao@microsoft.com)
# ------------------------------------------------------------------------------
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import os
import logging
import torch
import torch.nn as nn
from collections import OrderedDict
BN_MOMENTUM = 0.1
logger = logging.getLogger(__name__)
def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)
class BasicBlock(nn.Module):
    expansion = 1
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.downsample = downsample
        self.stride = stride
    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
class Bottleneck(nn.Module):
    expansion = 4
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,
                               bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion,
                                  momentum=BN_MOMENTUM)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride
    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
class Bottleneck_CAFFE(nn.Module):
    expansion = 4
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck_CAFFE, self).__init__()
        # add stride to conv1x1
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,
                               bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion,
                                  momentum=BN_MOMENTUM)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride
    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
class PoseResNet(nn.Module):
    def __init__(self, block, layers, cfg, **kwargs):
        self.inplanes = 64
        extra = cfg.MODEL.EXTRA
        self.deconv_with_bias = extra.DECONV_WITH_BIAS
        super(PoseResNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        # used for deconv layers
        self.deconv_layers = self._make_deconv_layer(
            extra.NUM_DECONV_LAYERS,
            extra.NUM_DECONV_FILTERS,
            extra.NUM_DECONV_KERNELS,
        )
        self.final_layer = nn.Conv2d(
            in_channels=extra.NUM_DECONV_FILTERS[-1],
            out_channels=cfg.MODEL.NUM_JOINTS,
            kernel_size=extra.FINAL_CONV_KERNEL,
            stride=1,
            padding=1 if extra.FINAL_CONV_KERNEL == 3 else 0
        )
    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),
            )
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        return nn.Sequential(*layers)
    def _get_deconv_cfg(self, deconv_kernel, index):
        if deconv_kernel == 4:
            padding = 1
            output_padding = 0
        elif deconv_kernel == 3:
            padding = 1
            output_padding = 1
        elif deconv_kernel == 2:
            padding = 0
            output_padding = 0
        return deconv_kernel, padding, output_padding
    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):
        assert num_layers == len(num_filters), \
            'ERROR: num_deconv_layers is different len(num_deconv_filters)'
        assert num_layers == len(num_kernels), \
            'ERROR: num_deconv_layers is different len(num_deconv_filters)'
        layers = []
        for i in range(num_layers):
            kernel, padding, output_padding = \
                self._get_deconv_cfg(num_kernels[i], i)
            planes = num_filters[i]
            layers.append(
                nn.ConvTranspose2d(
                    in_channels=self.inplanes,
                    out_channels=planes,
                    kernel_size=kernel,
                    stride=2,
                    padding=padding,
                    output_padding=output_padding,
                    bias=self.deconv_with_bias))
            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))
            layers.append(nn.ReLU(inplace=True))
            self.inplanes = planes
        return nn.Sequential(*layers)
    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.deconv_layers(x)
        x = self.final_layer(x)
        return x
    def init_weights(self, pretrained=''):
        if os.path.isfile(pretrained):
            logger.info('=> init deconv weights from normal distribution')
            for name, m in self.deconv_layers.named_modules():
                if isinstance(m, nn.ConvTranspose2d):
                    logger.info('=> init {}.weight as normal(0, 0.001)'.format(name))
                    logger.info('=> init {}.bias as 0'.format(name))
                    nn.init.normal_(m.weight, std=0.001)
                    if self.deconv_with_bias:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    logger.info('=> init {}.weight as 1'.format(name))
                    logger.info('=> init {}.bias as 0'.format(name))
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
            logger.info('=> init final conv weights from normal distribution')
            for m in self.final_layer.modules():
                if isinstance(m, nn.Conv2d):
                    # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    logger.info('=> init {}.weight as normal(0, 0.001)'.format(name))
                    logger.info('=> init {}.bias as 0'.format(name))
                    nn.init.normal_(m.weight, std=0.001)
                    nn.init.constant_(m.bias, 0)
            # pretrained_state_dict = torch.load(pretrained)
            logger.info('=> loading pretrained model {}'.format(pretrained))
            # self.load_state_dict(pretrained_state_dict, strict=False)
            checkpoint = torch.load(pretrained)
            if isinstance(checkpoint, OrderedDict):
                state_dict = checkpoint
            elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                state_dict_old = checkpoint['state_dict']
                state_dict = OrderedDict()
                # delete 'module.' because it is saved from DataParallel module
                for key in state_dict_old.keys():
                    if key.startswith('module.'):
                        # state_dict[key[7:]] = state_dict[key]
                        # state_dict.pop(key)
                        state_dict[key[7:]] = state_dict_old[key]
                    else:
                        state_dict[key] = state_dict_old[key]
            else:
                raise RuntimeError(
                    'No state_dict found in checkpoint file {}'.format(pretrained))
            self.load_state_dict(state_dict, strict=False)
        else:
            logger.error('=> imagenet pretrained model dose not exist')
            logger.error('=> please download it first')
            raise ValueError('imagenet pretrained model does not exist')
resnet_spec = {18: (BasicBlock, [2, 2, 2, 2]),
               34: (BasicBlock, [3, 4, 6, 3]),
               50: (Bottleneck, [3, 4, 6, 3]),
               101: (Bottleneck, [3, 4, 23, 3]),
               152: (Bottleneck, [3, 8, 36, 3])}
def get_pose_net(cfg, is_train, **kwargs):
    num_layers = cfg.MODEL.EXTRA.NUM_LAYERS
    style = cfg.MODEL.STYLE
    block_class, layers = resnet_spec[num_layers]
    if style == 'caffe':
        block_class = Bottleneck_CAFFE
    model = PoseResNet(block_class, layers, cfg, **kwargs)
    if is_train and cfg.MODEL.INIT_WEIGHTS:
        print('hahah', cfg.MODEL.PRETRAINED)
        path = '/mnt/e/test/human-pose-estimation.pytorch-master/models/pytorch/imagenet/resnet152-b121ed2d.pth'
        # model.init_weights(cfg.MODEL.PRETRAINED)
        model.init_weights(path)
    if cfg.TRAIN.RESUME:
        last_result_path = ''
        model_state_dict = model.state_dict()
        print("Loading pretrained weights from: {}".format('checkpoint.pth.tar'))
        import os
        check_path = os.path.join(cfg.TRAIN.RESUME_ROOT, 'checkpoint.pth.tar')
        # check_path = '/mnt/e/test/human-pose-estimation.pytorch-master/output/coco/pose_resnet_152/384x288_d256x3_adam_lr1e-3/checkpoint.pth.tar'
        pretrained_state_dict = torch.load(check_path, map_location='cpu')
        if 'state_dict' in pretrained_state_dict:
            pretrained_state_dict = pretrained_state_dict['state_dict']
        prefix = "module."
        new_pretrained_state_dict = {}
        for k, v in pretrained_state_dict.items():
            #
            if k.replace(prefix, "") in model_state_dict and v.shape == model_state_dict[k.replace(prefix, "")].shape:
                new_pretrained_state_dict[k.replace(prefix, "")] = v
            elif k.replace(prefix, "") == "final_layer.weight":  # TODO
                print("Reiniting final layer filters:", k)
                o = torch.zeros_like(model_state_dict[k.replace(prefix, "")][:, :, :, :])
                nn.init.xavier_uniform_(o)
                n_filters = min(o.shape[0], v.shape[0])
                o[:n_filters, :, :, :] = v[:n_filters, :, :, :]
                new_pretrained_state_dict[k.replace(prefix, "")] = o
            elif k.replace(prefix, "") == "final_layer.bias":
                print("Reiniting final layer biases:", k)
                o = torch.zeros_like(model_state_dict[k.replace(prefix, "")][:])
                nn.init.zeros_(o)
                n_filters = min(o.shape[0], v.shape[0])
                o[:n_filters] = v[:n_filters]
                new_pretrained_state_dict[k.replace(prefix, "")] = o
        model.load_state_dict(new_pretrained_state_dict, strict=True)
        print("Successfully loaded pretrained weights for backbone")
    return model
class Args:
    def __init__(self):
        self.cfg = '/mnt/e/test/human-pose-estimation.pytorch-master/experiments/coco/resnet152/384x288_d256x3_adam_lr1e-3.yaml'
        self.gpus = '0'
        self.workers = 1
from lib.core.config import config
from lib.core.config import update_config
args = Args()
update_config(args.cfg)
<_io.TextIOWrapper name='/mnt/e/test/human-pose-estimation.pytorch-master/experiments/coco/resnet152/384x288_d256x3_adam_lr1e-3.yaml' mode='r' encoding='UTF-8'>
model = get_pose_net(config, False)
Loading pretrained weights from: checkpoint.pth.tar
Reiniting final layer filters: module.final_layer.weight
Reiniting final layer biases: module.final_layer.bias
Successfully loaded pretrained weights for backbone
model.cuda()
model.final_layer = nn.Conv2d(256, 23, kernel_size=(1, 1), stride=(1, 1))
model.load_state_dict(new_state_dict, strict=True)
<All keys matched successfully>
from PIL import Image
import torch
from torchvision import transforms
image = Image.open('/data/S9_Directions_1.54138969_000001.jpg')
Transforms = transforms.Compose([transforms.Resize((256,192)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])])
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/site-packages/PIL/Image.py", line 3227, in open
    fp = builtins.open(filename, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '/data/S9_Directions_1.54138969_000001.jpg'
image = Image.open('data/S9_Directions_1.54138969_000001.jpg')
Transforms = transforms.Compose([transforms.Resize((256,192)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])])
Transforms = transforms.Compose([transforms.Resize((384,288)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])])
resized_image = Transforms(image)
model = model.to('cuda')
resized_image = resized_image.cuda()
resized_image = resized_image.view(1,3,256,192)
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
RuntimeError: shape '[1, 3, 256, 192]' is invalid for input of size 331776
resized_image = resized_image.view(1,3,384,288)
output = model(resized_image)
max_vals, max_indices = torch.max(output.view(1,23,-1), dim=2)
max_indices = max_indices.view(1, 23)
pred_coords = torch.zeros(1, 23, 2)
for i in range(23):
    idx = max_indices[0, i]
    # 先记住吧，线性索引变成二维索引，就是行索引线性索引除以‘//’图片的高，列索引x就是线性索引取余‘高’
    # 行索引
    y = idx // 48
    # 列索引
    x = idx % 48
    pred_coords[0, i, 0] = x
    
<input>:7: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
max_indices = max_indices.view(1, 23)
pred_coords = torch.zeros(1, 23, 2)
for i in range(23):
    idx = max_indices[0, i]
    # 先记住吧，线性索引变成二维索引，就是行索引线性索引除以‘//’图片的高，列索引x就是线性索引取余‘高’
    # 行索引
    y = idx // 48
    # 列索引
    x = idx % 48
    pred_coords[0, i, 0] = x
    pred_coords[i, 0, 0] = y
    
<input>:7: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 11, in <module>
IndexError: index 1 is out of bounds for dimension 0 with size 1
pred_coords = torch.zeros(1, 23, 2)
for i in range(23):
    idx = max_indices[0, i]
    # 先记住吧，线性索引变成二维索引，就是行索引线性索引除以‘//’图片的高，列索引x就是线性索引取余‘高’
    # 行索引
    y = idx // 48
    # 列索引
    x = idx % 48
    pred_coords[0, i, 0] = x
    pred_coords[0, i, 1] = y
    
<input>:5: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
image = Image.open('/mnt/e/test/human-pose-estimation.pytorch-master/data/test.jpg')
width = image.shape[0]
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/site-packages/PIL/Image.py", line 529, in __getattr__
    raise AttributeError(name)
AttributeError: shape
width = image.shape[0]
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/site-packages/PIL/Image.py", line 529, in __getattr__
    raise AttributeError(name)
AttributeError: shape
width = image.shape[0]
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/site-packages/PIL/Image.py", line 529, in __getattr__
    raise AttributeError(name)
AttributeError: shape
import numpy as np
width = np.array(image).shape[0]
height = np.array(image).shape[1]
image = Image.open('/mnt/e/test/human-pose-estimation.pytorch-master/data/S9_Directions_1.54138969_000001.jpg')
width = np.array(image).shape[0]
height = np.array(image).shape[1]
for i in range(23):
    idx = max_indices[0, i]
    # 先记住吧，线性索引变成二维索引，就是行索引线性索引除以‘//’图片的高，列索引x就是线性索引取余‘高’
    # 行索引
    y = idx // 48
    # 列索引
    x = idx % 48
    pred_coords[0, i, 0] = x
    pred_coords[0, i, 1] = y
    # 假设热图大小和原始图像大小不同，进行比例缩放
    scale_factor_x = width / 96
    scale_factor_y = height / 72
    pred_coords[:, :, 0] *= scale_factor_x
    pred_coords[:, :, 1] *= scale_factor_y
    
<input>:5: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
for i in range(23):
    idx = max_indices[0, i]
    # 先记住吧，线性索引变成二维索引，就是行索引线性索引除以‘//’图片的高，列索引x就是线性索引取余‘高’
    # 行索引
    y = idx // 48
    # 列索引
    x = idx % 48
    pred_coords[0, i, 0] = x
    pred_coords[0, i, 1] = y
    # 假设热图大小和原始图像大小不同，进行比例缩放
    scale_factor_x = width / 96
    scale_factor_y = height / 72
    pred_coords[:, i, 0] *= scale_factor_x
    pred_coords[:, i, 1] *= scale_factor_y
    
    
    
<input>:5: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
config.MODEL.EXTRA.HEATMAP_SIZE
array([72, 96])
config.MODEL.EXTRA.HEATMAP_SIZE.shape[3]
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
IndexError: tuple index out of range
config.MODEL.EXTRA.HEATMAP_SIZE[1]
96
pred_coords = pred_coords.numpy()
pred_coords = pred_coords.numpy().reshape(23,2)
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
AttributeError: 'numpy.ndarray' object has no attribute 'numpy'
pred_coords = pred_coords.reshape(23,2)
import cv2
for joint in pred_coords:
    x, y = joint
    cv2.circle(image, (int(x), int(y)), 5, (0, 0, 255), -1)  # 在 (x, y) 位置绘制一个半径为5的红色实心圆
cv2.imshow('Image with Joints', image)
cv2.waitKey(0)
cv2.destroyAllWindows()
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 4, in <module>
TypeError: img is not a numpy array, neither a scalar
image = cv2.imshow('data/S9_Directions_1.54138969_000001.jpg')
for joint in pred_coords:
    x, y = joint
    cv2.circle(image, (int(x), int(y)), 5, (0, 0, 255), -1)  # 在 (x, y) 位置绘制一个半径为5的红色实心圆
cv2.imshow('Image with Joints', image)
cv2.waitKey(0)
cv2.destroyAllWindows()
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
TypeError: imshow() missing required argument 'mat' (pos 2)
image = cv2.imshow('data/S9_Directions_1.54138969_000001.jpg')
Traceback (most recent call last):
  File "/home/ff/anaconda3/envs/pytorchgpu/lib/python3.7/code.py", line 90, in runcode
    exec(code, self.locals)
  File "<input>", line 1, in <module>
TypeError: imshow() missing required argument 'mat' (pos 2)
image = cv2.imread('data/S9_Directions_1.54138969_000001.jpg')
for joint in pred_coords:
    x, y = joint
    cv2.circle(image, (int(x), int(y)), 5, (0, 0, 255), -1)  # 在 (x, y) 位置绘制一个半径为5的红色实心圆
cv2.imshow('Image with Joints', image)
cv2.waitKey(0)
cv2.destroyAllWindows()
Process finished with exit code 0

```

# 2024.6.2继续尝试将h36m输入模型

# 2024.6.3写小论文并且继续验证coco数据集训练的模型在h36m模型的效果

```python
import numpy as np
import matplotlib.pyplot as plt

# 示例热图数据，假设每个热图的大小为 96x72
# 用随机数据填充示例数组，可以替换为实际的热图数据
heatmaps = np.random.rand(17, 96, 72)

# 创建一个 4x5 的网格来显示 17 张热图（网格大小可以根据需要调整）
fig, axes = plt.subplots(4, 5, figsize=(15, 12))

# Flatten the axes array for easy iteration
axes = axes.flatten()

for i in range(17):
    ax = axes[i]
    ax.imshow(heatmaps[i], cmap='hot', interpolation='nearest')
    ax.set_title(f'Heatmap {i+1}')
    ax.axis('off')  # 隐藏坐标轴

# 删除多余的子图
for j in range(17, len(axes)):
    fig.delaxes(axes[j])

# 调整子图之间的间距
plt.tight_layout()

# 显示热图
plt.show()

```

<img src="D:\Work_APP\Typora\assets\image-20240603144314814.png" alt="image-20240603144314814" style="zoom:50%;" />



<img src="D:\Work_APP\Typora\assets\image-20240603151250475.png" alt="image-20240603151250475" style="zoom:50%;" />



# 2024.6.4绘制小论文的图



```Python
# 插入高斯分布用于表示二维特
import numpy as np
import matplotlib.pyplot as plt
def gaussian_ellipse(x, y, x0, y0, sigma_x, sigma_y, theta):
    a = (np.cos(theta) ** 2) / (2 * sigma_x ** 2) + (np.sin(theta) ** 2) / (2 * sigma_y ** 2)
    b = -np.sin(2 * theta) / (4 * sigma_x ** 2) + np.sin(2 * theta) / (4 * sigma_y ** 2)
    c = (np.sin(theta) ** 2) / (2 * sigma_x ** 2) + (np.cos(theta) ** 2) / (2 * sigma_y ** 2)
    return np.exp(-(a * (x - x0) ** 2 + 2 * b * (x - x0) * (y - y0) + c * (y - y0) ** 2))
# 定义网格
x = np.linspace(-10, 10, 500)
y = np.linspace(-10, 10, 500)
X, Y = np.meshgrid(x, y)
# 定义三个椭圆形高斯分布
params = [
    (0, 0, 0.8, 0.5, np.pi / 4),  # 中心在(0,0)，σx=3，σy=1，旋转45度
    (-5, -5, 0.9, 0.6, np.pi / 6),  # 中心在(-5,-5)，σx=2，σy=4，旋转30度
    (5, 5, 0.5, 0.3, np.pi / 3)  # 中心在(5,5)，σx=1，σy=3，旋转60度
]
# 绘制每个高斯分布的热力图并保存为单独的图片
for i, (x0, y0, sigma_x, sigma_y, theta) in enumerate(params):
    heatmap = gaussian_ellipse(X, Y, x0, y0, sigma_x, sigma_y, theta)
    plt.figure(figsize=(8, 6))
    plt.contourf(X, Y, heatmap, levels=50, cmap='viridis')
    plt.colorbar()
    plt.title(f'Heatmap of Gaussian Ellipse {i + 1}')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.axis('equal')
    # 保存热力图为图片文件
    plt.savefig(f'gaussian_ellipse_heatmap_{i + 1}.png')
    plt.close()
print("Images saved successfully.")
```





绘制Openpose的

```Python
import cv2
path =  'data/Opencap/squats1_videoAndMocap.trc'
trc = trc_2_dict(path)
def trc_2_dict(pathFile, rotation=None):
    # rotation is a dict, eg. {'y':90} with axis, angle for rotation
    trc_dict = {}
    trc_file = TRCFile(pathFile)
    trc_dict['time'] = trc_file.time
    trc_dict['marker_names'] = trc_file.marker_names
    trc_dict['markers'] = {}
    if rotation != None:
        for axis, angle in rotation.items():
            trc_file.rotate(axis, angle)
    for count, marker in enumerate(trc_dict['marker_names']):
        trc_dict['markers'][marker] = trc_file.marker(marker)
    return trc_dict
feature_markers = [
    "Neck", "RShoulder", "LShoulder", "RHip", "LHip", "RKnee", "LKnee",
    "RAnkle", "LAnkle", "RHeel", "LHeel", "RSmallToe", "LSmallToe",
    "RBigToe", "LBigToe", "RElbow", "LElbow", "RWrist", "LWrist"]
array = [trc['markers'][maker] for maker in feature_markers]   
kpt_all  = np.stack(array, axis=1)
pose_pairs = [
    [0, 1], [0, 15], [0, 16],
    [15, 17],
    [16, 18],
    [1, 2], [1, 5], [1, 8],
    [2, 3],
    [3, 4],
    [5, 6],
    [6, 7],
    [8, 9], [8, 12],
    [9, 10],
    [10, 11],
    [12, 13],
    [13, 14]
]

# 绘制用到的颜色
# 绘制用的颜色
pose_colors = [
    (255., 0., 85.), (255., 0., 0.), (255., 85., 0.), (255., 170., 0.),
    (255., 255., 0.), (170., 255., 0.), (85., 255., 0.), (0., 255., 0.),
    (255., 0., 0.), (0., 255., 85.), (0., 255., 170.), (0., 255., 255.),
    (0., 170., 255.), (0., 85., 255.), (0., 0., 255.), (255., 0., 170.),
    (170., 0., 255.), (255., 0., 255.), (85., 0., 255.), (0., 0., 255.),
    (0., 0., 255.), (0., 0., 255.), (0., 255.,
                                     255.), (0., 255., 255.), (0., 255., 255.)
]
# 创建一个黑色背景
img=np.zeros((512,512,3),np.uint8)

kpt = kpt_all[0]
for p in pose_pairs:
    pt1 = tuple(list(map(int, kpt[p[0], 0:2])))
    c1 = kpt[p[0], 2]
    pt2 = tuple(list(map(int, kpt[p[1], 0:2])))
    c2 = kpt[p[1], 2]

    print('== {}, {}, {}, {} =='.format(pt1, c1, pt2, c2))

    if c1 == 0.0 or c2 == 0.0:
        continue

    color = tuple(list(map(int, pose_colors[p[0]])))
    img = cv2.line(img, pt1, pt2, color, thickness=4)
    img = cv2.circle(img, pt1, 4, color, thickness=-
                         1, lineType=8, shift=0)
    img = cv2.circle(img, pt2, 4, color, thickness=-
                         1, lineType=8, shift=0)

```

使用Alpapose和motionBERT画出三维人体关键点，画出的Openpose的关键点有点奇怪。

阅读博客记录：

标题：人体骨骼关键点检测中的heatmap 与vectormap 生成

https://blog.csdn.net/qq_21033779/article/details/84840307?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task

# 2024.6.5继续撰写小论文

写相关工作部分

```
参考文献
引言




（）Uhlrich S D, Falisse A, Kidziński Ł, et al. OpenCap: Human movement dynamics from smartphone videos[J]. PLoS computational biology, 2023, 19(10): e1011462.
（）Sengupta A, Budvytis I, Cipolla R. Synthetic training for accurate 3d human pose and shape estimation in the wild[J]. arXiv preprint arXiv:2009.10013, 2020.
2D
1.Toshev A, Szegedy C. Deeppose: Human pose estimation via deep neural networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 1653-1660.
10
2.Wei S E, Ramakrishna V, Kanade T, et al. Convolutional pose machines[C]//Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2016: 4724-4732.
3.Newell A, Yang K, Deng J. Stacked hourglass networks for human pose estimation[C]//Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14. Springer International Publishing, 2016: 483-499.
4.Sun K, Xiao B, Liu D, et al. Deep high-resolution representation learning for human pose estimation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 5693-5703.
3D
1. Li S, Chan A B. 3d human pose estimation from monocular images with deep convolutional neural network[C]//Computer Vision--ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part II 12. Springer International Publishing, 2015: 332-347.
2.Martinez J, Hossain R, Romero J, et al. A simple yet effective baseline for 3d human pose estimation[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2640-2649.
3.Pavlakos G, Zhou X, Derpanis K G, et al. Coarse-to-fine volumetric prediction for single-image 3D human pose[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7025-7034.
4.Iskakov K, Burkov E, Lempitsky V, et al. Learnable triangulation of human pose[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 7718-7727.
5.Pavllo D, Feichtenhofer C, Grangier D, et al. 3d human pose estimation in video with temporal convolutions and semi-supervised training[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 7753-7762.
无标记运动捕捉
1.Song K, Hullfish T J, Silva R S, et al. Markerless motion capture estimates of lower extremity kinematics and kinetics are comparable to marker-based across 8 movements[J]. Journal of Biomechanics, 2023, 157: 111751.
2.pose2sim
3.Uhlrich S D, Falisse A, Kidziński Ł, et al. OpenCap: Human movement dynamics from smartphone videos[J]. PLoS computational biology, 2023, 19(10): e1011462.
4.

实验，数据集
1.
2.Uhlrich S D, Falisse A, Kidziński Ł, et al. OpenCap: Human movement dynamics from smartphone videos[J]. PLoS computational biology, 2023, 19(10): e1011462.
```

# 2024.6.6画图，画表

## 3D最后结果的关节图和那个帧数有关。

```python
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import pickle
with open('data/h36m/results.pkl','rb') as f:
    data = pickle.load(f)


def motion2video_3d(motion,f):
    joint_pairs = [(0, 1), (1, 2), (2, 6), (5, 4), (4, 3), (3, 6), (6, 7), (7, 8), (8, 16), (9, 16), (8, 12), (11, 12), (10, 11), (8, 13), (13, 14), (14, 15)]
    joint_pairs_left = [[8, 11], [11, 12], [12, 13], [0, 4], [4, 5], [5, 6]]
    joint_pairs_right = [[8, 14], [14, 15], [15, 16], [0, 1], [1, 2], [2, 3]]
    color_mid = "#00457E"
    color_left = "#02315E"
    color_right = "#2F70AF"
    j3d = motion[f,:,:]
    fig = plt.figure(figsize=(20, 10))
    ax = fig.add_subplot(111, projection='3d')
    # max = np.max(j3d,axis=0)
    # min = np.min(j3d,axis=0)
    # ax.set_xlim(min[0], max[0])
    # ax.set_zlim(min[1], max[1])
    # ax.set_ylim(min[2], max[2])
    ax.set_xlim(-1000, 2000)
    ax.set_zlim(-1000, 2000)
    ax.set_ylim(-1000, 2000)
    # for i in range(len(joint_pairs)):
    #     limb = joint_pairs[i]
    #     xs, ys, zs = [np.array([j3d[limb[0], j], j3d[limb[1], j]]) for j in range(3)]
    #     if joint_pairs[i] in joint_pairs_left:
    #         ax.plot(-xs, -zs, -ys, color=color_left, lw=3, marker='o', markerfacecolor='w', markersize=3,
    #                 markeredgewidth=2)  # axis transformation for visualization
    #     elif joint_pairs[i] in joint_pairs_right:
    #         ax.plot(-xs, -zs, -ys, color=color_right, lw=3, marker='o', markerfacecolor='w', markersize=3,
    #                 markeredgewidth=2)  # axis transformation for visualization
    #     else:
    #         ax.plot(-xs, -zs, -ys, color=color_mid, lw=3, marker='o', markerfacecolor='w', markersize=3,
    #                     markeredgewidth=2)  # axis transformation for visualization

    # xs, ys, zs
    for i in range(len(joint_pairs)):
        limb = joint_pairs[i]
        xs, ys, zs = [np.array([j3d[limb[0], j], j3d[limb[1], j]]) for j in range(3)]
        if joint_pairs[i] in joint_pairs_left:
            ax.plot(xs, ys, zs, color=color_left, lw=3, marker='o', markerfacecolor='w', markersize=3,
                    markeredgewidth=2)  # axis transformation for visualization
        elif joint_pairs[i] in joint_pairs_right:
            ax.plot(xs, ys, zs, color=color_right, lw=3, marker='o', markerfacecolor='w', markersize=3,
                    markeredgewidth=2)  # axis transformation for visualization
        else:
            ax.plot(xs, ys, zs, color=color_mid, lw=3, marker='o', markerfacecolor='w', markersize=3,
                        markeredgewidth=2)  # axis transformation for visualization

    # frame_vis = get_img_from_fig(fig)
    plt.savefig('D:\\周报\\图片练习\\pic_small_paper\\context_model\\preimage_{}.png'.format(f), dpi=300)
    # plt.show()
if __name__ == '__main__':
    # keypoints_3d = np.load('human3.6m_gt.npy',allow_pickle=True)
    keypoints_3d = np.load('human3.6m_pre.npy', allow_pickle=True)
    for i in range(65):
        motion2video_3d(keypoints_3d, i)
    # keypoints_3d = np.load('human3.6m_pre.npy', allow_pickle=True)
    # motion2video_3d(keypoints_3d)
```

读取openpose的3D的



2024.6.8继续绘制小论文的图

最重要的是添加数据，z

![image-20240608151956588](D:\Work_APP\Typora\assets\image-20240608151956588.png)

要选择independent调整线的颜色、并且Binned

![image-20240609001052620](D:\Work_APP\Typora\assets\image-20240609001052620.png)

# 2024.6.9画图，画肩关节，膝关节的误差图

![image-20240609102034725](D:\Work_APP\Typora\assets\image-20240609102034725.png)

<img src="D:\Work_APP\Typora\assets\image-20240609211604717.png" alt="image-20240609211604717" style="zoom:50%;" />

```python
# 计算两条曲线的相关性
import matplotlib.pyplot as plt
from scipy.stats import pearsonr
from scipy.interpolate import CubicSpline

plt.figure(figsize=(10, 6))
plt.plot(time, y_pre_new, label='sin(x)', linestyle='-', linewidth=2)
plt.plot(time, y_gt, label='cos(x)', linestyle='--', linewidth=2)
plt.show()



# y_pre_new 对应的是result.xlsx的sheet3的第5列，并且进行样条插值之后的值，time表示时间
cs = CubicSpline(x, y)
#x_new = time
y_new = cs(x_new)

# 平移函数。
# step --平移步数
# y --要平移的函数
def transforms_function(shift, y):
    y_shifted = np.zeros_like(y)
    shifted_indices = np.arange(len(y)) + shift
    # 可用范围
    valid_indices = shifted_indices < len(y)
    # 超出范围用-1表示检索
    shifted_indices = np.where(valid_indices, shifted_indices, -1)
    for original_index, shifted_index in enumerate(shifted_indices):
        #移动经过的初始部分都会变成0，剩下的就是保留
        if shifted_index != -1:
            y_shifted[shifted_index] = y[original_index]
    return y_shifted

# 计算两条曲线的相关性（在平移过程中一条移动另一条保持不变）
correlation_coefficient_list = []
p_value_list = []
for i in range(20): 
	y_shifted = transforms_function(i, y_pre_new)
	correlation_coefficient, p_value = pearsonr(y_shifted, y_gt)
    correlation_coefficient_list.append(correlation_coefficient)
    p_value_list.append(p_value)

# 找出其中满足下面要求的点，找出皮尔逊相关系数最接近1的点就是所求
correlation_coefficient_max = max(correlation_coefficient_list)
shift_finally = correlation_coefficient_list.index(max(correlation_coefficient_list))
# 与gt曲线最相近的曲线
y_shifted = transforms_function(shift_finally, y_pre_new)
```



![image-20240609215457914](D:\Work_APP\Typora\assets\image-20240609215457914.png)

![image-20240609215512752](D:\Work_APP\Typora\assets\image-20240609215512752.png)

```python
# 输出为Excel好复制
import numpy as np
import pandas as pd

data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])

df = pd.DataFrame(data, columns=['Column1', 'Column2', 'Column3'])
file_path = 'output.xlsx'
df.to_excel(file_path, index=False, sheet_name='Sheet1')
print(f"数据已保存到 {file_path}")


```

# 2024.6.11 撰写小论文

参考文献

```
Rpan: An end-to -end recurrent pose-attention network for action recognition in videos.
The progress of human pose estimation:a survey and taxonomy of models applied in 2d human pose estimation.
Motionbert: Unified pretraining forhuman motion analysis. arXiv preprint arXiv:2210.06551,2022.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13232–13242, 2022.
Epipolar transformer for multi-view human pose estimation.
 Direct multi-view multi-person 3d pose estimation
 Development of an optical motion-capture system for 3d gait analysis
 Benchmarking of a full-body inertial motion capture system for clinical gait analysis
 Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image
Martinez J, Hossain R, Romero J, et al. A simple yet effective baseline for 3d human pose estimation[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2640-2649
Human pose as calibration pattern; 3d huan pose estimation with multiple unsynchronized and uncalibrated cameras.
Extrinsic camera calibration from a moving person
Triangulation. Computer Vision and Image Understanding
An end-to-end workflow for 3D markerless sports kinematics
Learnable triangulation of human pose.
OpenCap: Human movement dynamics from smartphone 
OpenSim: Open-source software to create and analyze dynamic simulations of movement
Context Modeling in 3D Human Pose Estimation: A Unified Perspective
Probabilistic Triangulation for Uncalibrated Multi-View 3D Human Pose Estimation
Movi: A large multi-purpose human motion and video dataset.
Pose2Sim: An open-source Python package for multiview markerless kinematics
Li S, Chan A B. 3d human pose estimation from monocular images with deep convolutional neural network[C]//Computer Vision--ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part II 12. Springer International Publishing, 2015: 332-347.
Martinez J, Hossain R, Romero J, et al. A simple yet effective baseline for 3d human pose estimation[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2640-2649.
Pavlakos G, Zhou X, Derpanis K G, et al. Coarse-to-fine volumetric prediction for single-image 3D human pose[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7025-7034.
Pavllo D, Feichtenhofer C, Grangier D, et al. 3d human pose estimation in video with temporal convolutions and semi-supervised training[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 7753-7762.
Context Modeling in 3D Human Pose Estimation: A Unified Perspective
Song K, Hullfish T J, Silva R S, et al. Markerless motion capture estimates of lower extremity kinematics and kinetics are comparable to marker-based across 8 movements[J]. Journal of Biomechanics, 2023, 157: 111751.
Uhlrich S D, Falisse A, Kidziński Ł, et al. OpenCap: Human movement dynamics from smartphone videos[J]. PLoS computational biology, 2023, 19(10): e1011462.
3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data
Jin S, Xu L, Xu J, et al. Whole-body human pose estimation in the wild[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16. Springer International Publishing, 2020: 196-214.
Ionescu C, Papava D, Olaru V, et al. Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments[J]. IEEE transactions on pattern analysis and machine intelligence, 2013, 36(7): 1325-1339.
Nguyen K X, Zheng L, Hawke A L, et al. Deep learning-based estimation of whole-body kinematics from multi-view images[J]. Computer Vision and Image Understanding, 2023, 235: 103780.

```



```
Rpan: An end-to -end recurrent pose-attention network for action recognition in videos.
The progress of human pose estimation:a survey and taxonomy of models applied in 2d human pose estimation.
Motionbert: Unified pretraining forhuman motion analysis. arXiv preprint arXiv:2210.06551,2022.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13232–13242, 2022.
Epipolar transformer for multi-view human pose estimation.
 Direct multi-view multi-person 3d pose estimation
 Development of an optical motion-capture system for 3d gait analysis
 Benchmarking of a full-body inertial motion capture system for clinical gait analysis
 Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image
Martinez J, Hossain R, Romero J, et al. A simple yet effective baseline for 3d human pose estimation[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2640-2649
Human pose as calibration pattern; 3d huan pose estimation with multiple unsynchronized and uncalibrated cameras.
Extrinsic camera calibration from a moving person
Triangulation. Computer Vision and Image Understanding
An end-to-end workflow for 3D markerless sports kinematics
Learnable triangulation of human pose.
OpenCap: Human movement dynamics from smartphone 
OpenSim: Open-source software to create and analyze dynamic simulations of movement
Context Modeling in 3D Human Pose Estimation: A Unified Perspective
Probabilistic Triangulation for Uncalibrated Multi-View 3D Human Pose Estimation
Movi: A large multi-purpose human motion and video dataset.
Pose2Sim: An open-source Python package for multiview markerless kinematics
Pavllo D, Feichtenhofer C, Grangier D, et al. 3d human pose estimation in video with temporal convolutions and semi-supervised training[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 7753-7762.
Context Modeling in 3D Human Pose Estimation: A Unified Perspective
Song K, Hullfish T J, Silva R S, et al. Markerless motion capture estimates of lower extremity kinematics and kinetics are comparable to marker-based across 8 movements[J]. Journal of Biomechanics, 2023, 157: 111751.
Uhlrich S D, Falisse A, Kidziński Ł, et al. OpenCap: Human movement dynamics from smartphone videos[J]. PLoS computational biology, 2023, 19(10): e1011462.
3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data
Jin S, Xu L, Xu J, et al. Whole-body human pose estimation in the wild[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16. Springer International Publishing, 2020: 196-214.
Ionescu C, Papava D, Olaru V, et al. Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments[J]. IEEE transactions on pattern analysis and machine intelligence, 2013, 36(7): 1325-1339.
Nguyen K X, Zheng L, Hawke A L, et al. Deep learning-based estimation of whole-body kinematics from multi-view images[J]. Computer Vision and Image Understanding, 2023, 235: 103780.

```



# 2024.6.21 继续调代码

```python
D:\Work_APP\Anconda\envs\motionbert\python.exe "D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py" --mode=client --host=127.0.0.1 --port=63568 
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['E:\\test\\human-pose-estimation.pytorch-master'])
Python 3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.31.1 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.31.1
Python 3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)] on win32
from PIL import Image, ImageDraw
image = Image.open('E:\\test\human-pose-estimation.pytorch-master\data\\000000000431.jpg')
import json
with open('data/coco/annotations/person_keypoints_train2017.json','r') as f:
    anns = json.load(f)
    
keypoints_gt = anns['annotations'][109135]['keypoints']
import numpy as np
keypoints_gt_np = np.array(keypoints_gt).reshape(17,2)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-6-856c0c07f97c>", line 2, in <module>
    keypoints_gt_np = np.array(keypoints_gt).reshape(17,2)
ValueError: cannot reshape array of size 51 into shape (17,2)
keypoints_gt = anns['annotations'][109135]['keypoints']
import numpy as np
keypoints_gt_np = np.array(keypoints_gt).reshape(17,3)
bbox = anns['annotations'][109135]['bbox']
from torchvision import transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda img: img[:, bbox[1]:bbox[3], bbox[0]:bbox[2]]),
    #img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225]),
    transforms.Resize((384,288)),
    transforms.ToPILImage()
])
D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\requests\__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (3.0.4) doesn't match a supported version!
  RequestsDependencyWarning)
Transforms = transforms.Compose([transforms.ToTensor()])
bbox = [154,67,311,319]
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
    #img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225]),
    transforms.Resize((384,288)),
    transforms.ToPILImage()
])
input = transform(image)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-13-d61d9e795d9f>", line 1, in <module>
    input = transform(image)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 474, in __call__
    return self.lambd(img)
  File "<ipython-input-12-42c597d75a59>", line 3, in <lambda>
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
AttributeError: 'Tensor' object has no attribute 'crop'
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
    transforms.ToPILImage(),
    #img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225]),
    transforms.Resize((384,288)),
    
])
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
    transforms.ToPILImage(),
    #img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225]),
    transforms.Resize((384,288)),
    
])
input = transform(image)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-16-d61d9e795d9f>", line 1, in <module>
    input = transform(image)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 474, in __call__
    return self.lambd(img)
  File "<ipython-input-15-6d897ca86427>", line 3, in <lambda>
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
AttributeError: 'Tensor' object has no attribute 'crop'
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
    transforms.ToTensor(),
    
    
    #img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225]),
    transforms.Resize((384,288)),
    transforms.ToPILImage(),
    
])
input = transform(image)
input.show()
# ------------------------------------------------------------------------------
# Copyright (c) Microsoft
# Licensed under the MIT License.
# Written by Bin Xiao (Bin.Xiao@microsoft.com)
# ------------------------------------------------------------------------------
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import os
import logging
import torch
import torch.nn as nn
from collections import OrderedDict
BN_MOMENTUM = 0.1
logger = logging.getLogger(__name__)
def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)
class BasicBlock(nn.Module):
    expansion = 1
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.downsample = downsample
        self.stride = stride
    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
class Bottleneck(nn.Module):
    expansion = 4
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,
                               bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion,
                                  momentum=BN_MOMENTUM)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride
    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
class Bottleneck_CAFFE(nn.Module):
    expansion = 4
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck_CAFFE, self).__init__()
        # add stride to conv1x1
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,
                               bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion,
                                  momentum=BN_MOMENTUM)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride
    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
class PoseResNet(nn.Module):
    def __init__(self, block, layers, cfg, **kwargs):
        self.inplanes = 64
        extra = cfg.MODEL.EXTRA
        self.deconv_with_bias = extra.DECONV_WITH_BIAS
        super(PoseResNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        # used for deconv layers
        self.deconv_layers = self._make_deconv_layer(
            extra.NUM_DECONV_LAYERS,
            extra.NUM_DECONV_FILTERS,
            extra.NUM_DECONV_KERNELS,
        )
        self.final_layer = nn.Conv2d(
            in_channels=extra.NUM_DECONV_FILTERS[-1],
            out_channels=cfg.MODEL.NUM_JOINTS,
            kernel_size=extra.FINAL_CONV_KERNEL,
            stride=1,
            padding=1 if extra.FINAL_CONV_KERNEL == 3 else 0
        )
    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),
            )
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        return nn.Sequential(*layers)
    def _get_deconv_cfg(self, deconv_kernel, index):
        if deconv_kernel == 4:
            padding = 1
            output_padding = 0
        elif deconv_kernel == 3:
            padding = 1
            output_padding = 1
        elif deconv_kernel == 2:
            padding = 0
            output_padding = 0
        return deconv_kernel, padding, output_padding
    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):
        assert num_layers == len(num_filters), \
            'ERROR: num_deconv_layers is different len(num_deconv_filters)'
        assert num_layers == len(num_kernels), \
            'ERROR: num_deconv_layers is different len(num_deconv_filters)'
        layers = []
        for i in range(num_layers):
            kernel, padding, output_padding = \
                self._get_deconv_cfg(num_kernels[i], i)
            planes = num_filters[i]
            layers.append(
                nn.ConvTranspose2d(
                    in_channels=self.inplanes,
                    out_channels=planes,
                    kernel_size=kernel,
                    stride=2,
                    padding=padding,
                    output_padding=output_padding,
                    bias=self.deconv_with_bias))
            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))
            layers.append(nn.ReLU(inplace=True))
            self.inplanes = planes
        return nn.Sequential(*layers)
    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.deconv_layers(x)
        x = self.final_layer(x)
        return x
    def init_weights(self, pretrained=''):
        if os.path.isfile(pretrained):
            logger.info('=> init deconv weights from normal distribution')
            for name, m in self.deconv_layers.named_modules():
                if isinstance(m, nn.ConvTranspose2d):
                    logger.info('=> init {}.weight as normal(0, 0.001)'.format(name))
                    logger.info('=> init {}.bias as 0'.format(name))
                    nn.init.normal_(m.weight, std=0.001)
                    if self.deconv_with_bias:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    logger.info('=> init {}.weight as 1'.format(name))
                    logger.info('=> init {}.bias as 0'.format(name))
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
            logger.info('=> init final conv weights from normal distribution')
            for m in self.final_layer.modules():
                if isinstance(m, nn.Conv2d):
                    # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    logger.info('=> init {}.weight as normal(0, 0.001)'.format(name))
                    logger.info('=> init {}.bias as 0'.format(name))
                    nn.init.normal_(m.weight, std=0.001)
                    nn.init.constant_(m.bias, 0)
            # pretrained_state_dict = torch.load(pretrained)
            logger.info('=> loading pretrained model {}'.format(pretrained))
            # self.load_state_dict(pretrained_state_dict, strict=False)
            checkpoint = torch.load(pretrained)
            if isinstance(checkpoint, OrderedDict):
                state_dict = checkpoint
            elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                state_dict_old = checkpoint['state_dict']
                state_dict = OrderedDict()
                # delete 'module.' because it is saved from DataParallel module
                for key in state_dict_old.keys():
                    if key.startswith('module.'):
                        # state_dict[key[7:]] = state_dict[key]
                        # state_dict.pop(key)
                        state_dict[key[7:]] = state_dict_old[key]
                    else:
                        state_dict[key] = state_dict_old[key]
            else:
                raise RuntimeError(
                    'No state_dict found in checkpoint file {}'.format(pretrained))
            self.load_state_dict(state_dict, strict=False)
        else:
            logger.error('=> imagenet pretrained model dose not exist')
            logger.error('=> please download it first')
            raise ValueError('imagenet pretrained model does not exist')
resnet_spec = {18: (BasicBlock, [2, 2, 2, 2]),
               34: (BasicBlock, [3, 4, 6, 3]),
               50: (Bottleneck, [3, 4, 6, 3]),
               101: (Bottleneck, [3, 4, 23, 3]),
               152: (Bottleneck, [3, 8, 36, 3])}
def get_pose_net(cfg, is_train, **kwargs):
    num_layers = cfg.MODEL.EXTRA.NUM_LAYERS
    style = cfg.MODEL.STYLE
    block_class, layers = resnet_spec[num_layers]
    if style == 'caffe':
        block_class = Bottleneck_CAFFE
    model = PoseResNet(block_class, layers, cfg, **kwargs)
    if is_train and cfg.MODEL.INIT_WEIGHTS:
        model.init_weights(cfg.MODEL.PRETRAINED)
        if cfg.TRAIN.RESUME:
            last_result_path = ''
            model_state_dict = model.state_dict()
            print("Loading pretrained weights from: {}".format('checkpoint.pth.tar'))
            import os
            check_path = os.path.join(cfg.TRAIN.RESUME_ROOT, 'checkpoint.pth.tar')
            pretrained_state_dict = torch.load(check_path, map_location='cpu')
            if 'state_dict' in pretrained_state_dict:
                pretrained_state_dict = pretrained_state_dict['state_dict']
            prefix = "module."
            new_pretrained_state_dict = {}
            for k, v in pretrained_state_dict.items():
                #
                if k.replace(prefix, "") in model_state_dict and v.shape == model_state_dict[k.replace(prefix, "")].shape:
                    new_pretrained_state_dict[k.replace(prefix, "")] = v
                elif k.replace(prefix, "") == "final_layer.weight":  # TODO
                    print("Reiniting final layer filters:", k)
                    o = torch.zeros_like(model_state_dict[k.replace(prefix, "")][:, :, :, :])
                    nn.init.xavier_uniform_(o)
                    n_filters = min(o.shape[0], v.shape[0])
                    o[:n_filters, :, :, :] = v[:n_filters, :, :, :]
                    new_pretrained_state_dict[k.replace(prefix, "")] = o
                elif k.replace(prefix, "") == "final_layer.bias":
                    print("Reiniting final layer biases:", k)
                    o = torch.zeros_like(model_state_dict[k.replace(prefix, "")][:])
                    nn.init.zeros_(o)
                    n_filters = min(o.shape[0], v.shape[0])
                    o[:n_filters] = v[:n_filters]
                    new_pretrained_state_dict[k.replace(prefix, "")] = o
        model.load_state_dict(new_pretrained_state_dict, strict=True)
        print("Successfully loaded pretrained weights for backbone")
    return model
import torch
from lib.core.config import config
from lib.core.config import update_config
class Args:
    def __init__(self):
        self.cfg = 'experiments/coco/resnet152/384x288_d256x3_adam_lr1e-3.yaml'
        self.gpus = '0'
        self.workers = 1
        
args = Args()
update_config(args.cfg)
<_io.TextIOWrapper name='experiments/coco/resnet152/384x288_d256x3_adam_lr1e-3.yaml' mode='r' encoding='cp936'>
model = get_pose_net(config, False)
print(model)

model.init_weights(config.MODEL.PRETRAINED)
weight_1 = torch.load('yunresult/output/coco/pose_resnet_152/384x288_d256x3_adam_lr1e-3/model_best.pth.tar')
new_state_dict = {}
for k, v in weight_1.items():
    new_state_dict[k.replace('module.', '')] = v
    
with torch.no_grad():
    output = model(input)
    input_flipped = np.flip(input.cpu().numpy(), 3).copy()
    input_flipped = torch.from_numpy(input_flipped).cuda()
    output_flipped = model(input_flipped)
    output_flipped = flip_back(output_flipped.cpu().numpy(),
                               val_dataset.flip_pairs)
    output_flipped = torch.from_numpy(output_flipped.copy()).cuda()
    output_flipped[:, :, :, 1:] = \
        output_flipped.clone()[:, :, :, 0:-1]
    output = (output + output_flipped) * 0.5
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-28-c426b56fdd6f>", line 2, in <module>
    output = model(input)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "<ipython-input-20-bf0c6cf33dcf>", line 235, in forward
    x = self.conv1(x)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\conv.py", line 444, in _conv_forward
    self.padding, self.dilation, self.groups)
TypeError: conv2d() received an invalid combination of arguments - got (Image, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:
 * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)
      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)
 * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)
      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)
input = Transforms(input).unsqueeze(0)
with torch.no_grad():
    output = model(input)
    input_flipped = np.flip(input.cpu().numpy(), 3).copy()
    input_flipped = torch.from_numpy(input_flipped).cuda()
    output_flipped = model(input_flipped)
    output_flipped = flip_back(output_flipped.cpu().numpy(),
                               val_dataset.flip_pairs)
    output_flipped = torch.from_numpy(output_flipped.copy()).cuda()
    output_flipped[:, :, :, 1:] = \
        output_flipped.clone()[:, :, :, 0:-1]
    output = (output + output_flipped) * 0.5
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-30-c426b56fdd6f>", line 5, in <module>
    output_flipped = model(input_flipped)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "<ipython-input-20-bf0c6cf33dcf>", line 235, in forward
    x = self.conv1(x)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\conv.py", line 444, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
model.cuda()

with torch.no_grad():
    output = model(input)
    input_flipped = np.flip(input.cpu().numpy(), 3).copy()
    input_flipped = torch.from_numpy(input_flipped).cuda()
    output_flipped = model(input_flipped)
    output_flipped = flip_back(output_flipped.cpu().numpy(),
                               val_dataset.flip_pairs)
    output_flipped = torch.from_numpy(output_flipped.copy()).cuda()
    output_flipped[:, :, :, 1:] = \
        output_flipped.clone()[:, :, :, 0:-1]
    output = (output + output_flipped) * 0.5
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-32-c426b56fdd6f>", line 2, in <module>
    output = model(input)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "<ipython-input-20-bf0c6cf33dcf>", line 235, in forward
    x = self.conv1(x)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\conv.py", line 444, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
input = input.cuda()
with torch.no_grad():
    output = model(input)
    input_flipped = np.flip(input.cpu().numpy(), 3).copy()
    input_flipped = torch.from_numpy(input_flipped).cuda()
    output_flipped = model(input_flipped)
    output_flipped = flip_back(output_flipped.cpu().numpy(),
                               val_dataset.flip_pairs)
    output_flipped = torch.from_numpy(output_flipped.copy()).cuda()
    output_flipped[:, :, :, 1:] = \
        output_flipped.clone()[:, :, :, 0:-1]
    output = (output + output_flipped) * 0.5
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-34-c426b56fdd6f>", line 6, in <module>
    output_flipped = flip_back(output_flipped.cpu().numpy(),
NameError: name 'flip_back' is not defined
# ------------------------------------------------------------------------------
# Copyright (c) Microsoft
# Licensed under the MIT License.
# Written by Bin Xiao (Bin.Xiao@microsoft.com)
# ------------------------------------------------------------------------------
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import cv2
# 进行左右翻转后恢复图像的原始姿态
# 这个函数通常用于姿态估计任务中，网络对图像进行预测后，左右翻转图像再预测，
# 最终将翻转后的预测结果翻转回来并合并到原始预测中，以提高预测的准确性。
def flip_back(output_flipped, matched_parts):
    '''
    ouput_flipped: numpy.ndarray(batch_size, num_joints, height, width)
    '''
    assert output_flipped.ndim == 4,\
        'output_flipped should be [batch_size, num_joints, height, width]'
    output_flipped = output_flipped[:, :, :, ::-1]
    for pair in matched_parts:
        tmp = output_flipped[:, pair[0], :, :].copy()
        output_flipped[:, pair[0], :, :] = output_flipped[:, pair[1], :, :]
        output_flipped[:, pair[1], :, :] = tmp
    return output_flipped
# 用于在图像处理中对关键点（如人体姿态关键点）进行水平翻转，并交换左右对称的关节位置
# 数据增强：在训练姿态估计模型时，通过水平翻转来增加数据的多样性。
#
def fliplr_joints(joints, joints_vis, width, matched_parts):
    """
    flip coords
    """
    # Flip horizontal
    joints[:, 0] = width - joints[:, 0] - 1
    # Change left-right parts
    for pair in matched_parts:
        joints[pair[0], :], joints[pair[1], :] = \
            joints[pair[1], :], joints[pair[0], :].copy()
        joints_vis[pair[0], :], joints_vis[pair[1], :] = \
            joints_vis[pair[1], :], joints_vis[pair[0], :].copy()
    return joints*joints_vis, joints_vis
def transform_preds(coords, center, scale, output_size):
    target_coords = np.zeros(coords.shape)
    trans = get_affine_transform(center, scale, 0, output_size, inv=1)
    for p in range(coords.shape[0]):
        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)
    return target_coords
def get_affine_transform(center,
                         scale,
                         rot,
                         output_size,
                         shift=np.array([0, 0], dtype=np.float32),
                         inv=0):
    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):
        scale = np.array([scale, scale])
    scale_tmp = scale * 200.0
    src_w = scale_tmp[0]
    dst_w = output_size[0]
    dst_h = output_size[1]
    rot_rad = np.pi * rot / 180
    src_dir = get_dir([0, src_w * -0.5], rot_rad)
    dst_dir = np.array([0, dst_w * -0.5], np.float32)
    src = np.zeros((3, 2), dtype=np.float32)
    dst = np.zeros((3, 2), dtype=np.float32)
    src[0, :] = center + scale_tmp * shift
    src[1, :] = center + src_dir + scale_tmp * shift
    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]
    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir
    src[2:, :] = get_3rd_point(src[0, :], src[1, :])
    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])
    if inv:
        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))
    else:
        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))
    return trans
def affine_transform(pt, t):
    new_pt = np.array([pt[0], pt[1], 1.]).T
    new_pt = np.dot(t, new_pt)
    return new_pt[:2]
def get_3rd_point(a, b):
    direct = a - b
    return b + np.array([-direct[1], direct[0]], dtype=np.float32)
def get_dir(src_point, rot_rad):
    sn, cs = np.sin(rot_rad), np.cos(rot_rad)
    src_result = [0, 0]
    src_result[0] = src_point[0] * cs - src_point[1] * sn
    src_result[1] = src_point[0] * sn + src_point[1] * cs
    return src_result
def crop(img, center, scale, output_size, rot=0):
    trans = get_affine_transform(center, scale, rot, output_size)
    dst_img = cv2.warpAffine(img,
                             trans,
                             (int(output_size[0]), int(output_size[1])),
                             flags=cv2.INTER_LINEAR)
    return dst_img
with torch.no_grad():
    output = model(input)
    input_flipped = np.flip(input.cpu().numpy(), 3).copy()
    input_flipped = torch.from_numpy(input_flipped).cuda()
    output_flipped = model(input_flipped)
    output_flipped = flip_back(output_flipped.cpu().numpy(),
                               val_dataset.flip_pairs)
    output_flipped = torch.from_numpy(output_flipped.copy()).cuda()
    output_flipped[:, :, :, 1:] = \
        output_flipped.clone()[:, :, :, 0:-1]
    output = (output + output_flipped) * 0.5
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-36-c426b56fdd6f>", line 7, in <module>
    val_dataset.flip_pairs)
NameError: name 'val_dataset' is not defined
from lib.dataset.coco import *
valid_dataset =COCODataset(   config,
        config.DATASET.ROOT,
        config.DATASET.TEST_SET,
        False,
        transforms.Compose([
            transforms.ToTensor(),
            normalize,
        ])
    )
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-38-793c29e95242>", line 7, in <module>
    normalize,
NameError: name 'normalize' is not defined
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
valid_dataset =COCODataset(   config,
        config.DATASET.ROOT,
        config.DATASET.TEST_SET,
        False,
        transforms.Compose([
            transforms.ToTensor(),
            normalize,
        ])
    )
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
with torch.no_grad():
    output = model(input)
    input_flipped = np.flip(input.cpu().numpy(), 3).copy()
    input_flipped = torch.from_numpy(input_flipped).cuda()
    output_flipped = model(input_flipped)
    output_flipped = flip_back(output_flipped.cpu().numpy(),
                               val_dataset.flip_pairs)
    output_flipped = torch.from_numpy(output_flipped.copy()).cuda()
    output_flipped[:, :, :, 1:] = \
        output_flipped.clone()[:, :, :, 0:-1]
    output = (output + output_flipped) * 0.5
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-41-c426b56fdd6f>", line 7, in <module>
    val_dataset.flip_pairs)
NameError: name 'val_dataset' is not defined
with torch.no_grad():
    output = model(input)
    input_flipped = np.flip(input.cpu().numpy(), 3).copy()
    input_flipped = torch.from_numpy(input_flipped).cuda()
    output_flipped = model(input_flipped)
    output_flipped = flip_back(output_flipped.cpu().numpy(),
                               valid_dataset.flip_pairs)
    output_flipped = torch.from_numpy(output_flipped.copy()).cuda()
    output_flipped[:, :, :, 1:] = \
        output_flipped.clone()[:, :, :, 0:-1]
    output = (output + output_flipped) * 0.5
    
center, scale = valid_dataset._box2cs(bbox)
keypoints_pre = get_final_preds(config, output, center, scale)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-44-36562b40d9f5>", line 1, in <module>
    keypoints_pre = get_final_preds(config, output, center, scale)
NameError: name 'get_final_preds' is not defined
# ------------------------------------------------------------------------------
# Copyright (c) Microsoft
# Licensed under the MIT License.
# Written by Bin Xiao (Bin.Xiao@microsoft.com)
# ------------------------------------------------------------------------------
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import math
import numpy as np
from lib.utils.transforms import transform_preds
# 用于从一批热图（heatmaps）中获取最大值的预测坐标及其对应的分数
def get_max_preds(batch_heatmaps):
    '''
    get predictions from score maps
    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])
    '''
    assert isinstance(batch_heatmaps, np.ndarray), \
        'batch_heatmaps should be numpy.ndarray'
    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'
    batch_size = batch_heatmaps.shape[0]
    num_joints = batch_heatmaps.shape[1]
    width = batch_heatmaps.shape[3]
    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))
    idx = np.argmax(heatmaps_reshaped, 2)
    maxvals = np.amax(heatmaps_reshaped, 2)
    maxvals = maxvals.reshape((batch_size, num_joints, 1))
    idx = idx.reshape((batch_size, num_joints, 1))
    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)
    preds[:, :, 0] = (preds[:, :, 0]) % width
    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)
    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))
    pred_mask = pred_mask.astype(np.float32)
    preds *= pred_mask
    return preds, maxvals
# 这个局部最大插值过程可以细化预测的关键点位置，使其更接近实际位置，提高模型的定位精度。
# 这种方法通过考虑热图局部邻域内的梯度信息，
# 来微调关键点的位置，从而弥补热图分辨率较低带来的精度损失。
def get_final_preds(config, batch_heatmaps, center, scale):
    coords, maxvals = get_max_preds(batch_heatmaps)
    heatmap_height = batch_heatmaps.shape[2]
    heatmap_width = batch_heatmaps.shape[3]
    # post-processing
    if config.TEST.POST_PROCESS:
        for n in range(coords.shape[0]):
            for p in range(coords.shape[1]):
                hm = batch_heatmaps[n][p]
                px = int(math.floor(coords[n][p][0] + 0.5))
                py = int(math.floor(coords[n][p][1] + 0.5))
                if 1 < px < heatmap_width-1 and 1 < py < heatmap_height-1:
                    diff = np.array([hm[py][px+1] - hm[py][px-1],
                                     hm[py+1][px]-hm[py-1][px]])
                    coords[n][p] += np.sign(diff) * .25
    preds = coords.copy()
    # Transform back
    for i in range(coords.shape[0]):
        preds[i] = transform_preds(coords[i], center[i], scale[i],
                                   [heatmap_width, heatmap_height])
    return preds, maxvals
# 获取中心和缩放尺度
def get_center_scale(bbox, image_size):
    """
    根据边界框计算中心点和缩放比例
    :param bbox: [x_min, y_min, width, height]
    :param image_size: 模型输入图像尺寸 [width, height]
    :return: center (图像中心), scale (缩放比例)
    """
    x_min, y_min, width, height = bbox
    center = np.zeros((2), dtype=np.float32)
    center[0] = x_min + width * 0.5
    center[1] = y_min + height * 0.5
    aspect_ratio = image_size[0] / image_size[1]
    pixel_std = 200  # 一个标准值，通常为200
    if width > aspect_ratio * height:
        height = width / aspect_ratio
    else:
        width = height * aspect_ratio
    scale = np.array([width / pixel_std, height / pixel_std], dtype=np.float32)
    return center, scale
keypoints_pre = get_final_preds(config, output, center, scale)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-46-36562b40d9f5>", line 1, in <module>
    keypoints_pre = get_final_preds(config, output, center, scale)
  File "<ipython-input-45-a2196f5524cc>", line 52, in get_final_preds
    coords, maxvals = get_max_preds(batch_heatmaps)
  File "<ipython-input-45-a2196f5524cc>", line 24, in get_max_preds
    'batch_heatmaps should be numpy.ndarray'
AssertionError: batch_heatmaps should be numpy.ndarray
keypoints_pre = get_final_preds(config, output.cpu().detach().numpy(), center, scale)
output_np = np.array(output)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-48-3ac07e63f9c6>", line 1, in <module>
    output_np = np.array(output)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\_tensor.py", line 732, in __array__
    return self.numpy()
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
output_np = output.cpu().detach().numpy()
keypoints_vis = keypoints_gt_np[:,-1]
valid_loader = torch.utils.data.DataLoader(
    valid_dataset,
    batch_size=config.TEST.BATCH_SIZE * len(gpus),
    shuffle=False,
    num_workers=config.WORKERS,
    pin_memory=True
)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-51-19c2ab9a8bc6>", line 3, in <module>
    batch_size=config.TEST.BATCH_SIZE * len(gpus),
NameError: name 'gpus' is not defined
gpus = [int(i) for i in config.GPUS.split(',')]
valid_loader = torch.utils.data.DataLoader(
    valid_dataset,
    batch_size=config.TEST.BATCH_SIZE * len(gpus),
    shuffle=False,
    num_workers=config.WORKERS,
    pin_memory=True
)
for i, (input, target, target_weight, meta) in enumerate(valid_loader):
    print(1)
    
D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\requests\__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (3.0.4) doesn't match a supported version!
  RequestsDependencyWarning)
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
with torch.no_grad():
    output_2 = model(input)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-55-e6d9c976d78b>", line 2, in <module>
    output_2 = model(input)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "<ipython-input-20-bf0c6cf33dcf>", line 235, in forward
    x = self.conv1(x)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\nn\modules\conv.py", line 444, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
input  = input.cuda()
with torch.no_grad():
    output_2 = model(input)
    
kepoints_pred_2 = get_final_preds(config, output_2, meta['center'], meta['scale'])
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-58-d74cbd66b827>", line 1, in <module>
    kepoints_pred_2 = get_final_preds(config, output_2, meta['center'], meta['scale'])
  File "<ipython-input-45-a2196f5524cc>", line 52, in get_final_preds
    coords, maxvals = get_max_preds(batch_heatmaps)
  File "<ipython-input-45-a2196f5524cc>", line 24, in get_max_preds
    'batch_heatmaps should be numpy.ndarray'
AssertionError: batch_heatmaps should be numpy.ndarray
kepoints_pred_2 = get_final_preds(config, output_2.cpu().detach().numpy(), meta['center'], meta['scale'])
E:\test\human-pose-estimation.pytorch-master\lib\utils\transforms.py:71: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  scale = np.array([scale, scale])
E:\test\human-pose-estimation.pytorch-master\lib\utils\transforms.py:71: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  scale = np.array([scale, scale])
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-59-d67ed555e5c7>", line 1, in <module>
    kepoints_pred_2 = get_final_preds(config, output_2.cpu().detach().numpy(), meta['center'], meta['scale'])
  File "<ipython-input-45-a2196f5524cc>", line 74, in get_final_preds
    [heatmap_width, heatmap_height])
  File "E:\test\human-pose-estimation.pytorch-master\lib\utils\transforms.py", line 54, in transform_preds
    trans = get_affine_transform(center, scale, 0, output_size, inv=1)
  File "E:\test\human-pose-estimation.pytorch-master\lib\utils\transforms.py", line 86, in get_affine_transform
    src[0, :] = center + scale_tmp * shift
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\_tensor.py", line 744, in __array_wrap__
    return torch.from_numpy(array)
TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
type(meta['center'])
Out[60]: torch.Tensor
kepoints_pred_2 = get_final_preds(config, output_2.cpu().detach().numpy(), meta['center'].cpu().detach().numpy(), meta['scale'].cpu().detach().numpy())
image_test = Image.open('data/000000516038.jpg')
kepoints_pred_2_0 = kepoints_pred_2[0]
k_p = kepoints_pred_2_0[0]
image_ss = Image.open('data/000000516038.jpg')
draw = ImageDraw.Draw(image_ss)
for keypoints_n in k_p:
    for (x, y) in keypoints_n:
        
        radius = 5
        # draw.ellipse((orig_x - radius, orig_y - radius, orig_x + radius, orig_y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
        
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-67-7f38ed5010d2>", line 2, in <module>
    for (x, y) in keypoints_n:
TypeError: cannot unpack non-iterable numpy.float32 object
for keypoints_n in k_p:
    x, y = keypoints_n
    print(x)
    
143.02226
128.14648
202.52539
167.36446
198.46835
170.06914
207.93477
198.46835
180.8879
210.63945
206.58241
201.17305
157.89804
203.87773
206.58241
163.30742
155.19336
for keypoints_n in k_p:
    x, y = keypoints_n
    radius = 2
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),
                 outline=(255, 0, 0))
    
image_ss.show()
image_ss.show()
kgt = meta['joints'][0]
draw = ImageDraw.Draw(image_ss)
for keypoints_n in k_p[:,:,0:2]:
    x, y = keypoints_n
    radius = 2
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),
                 outline=(255, 0, 0))
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-74-906cc9c0b711>", line 1, in <module>
    for keypoints_n in k_p[:,:,0:2]:
IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed
for keypoints_n in kgt[:,:,0:2]:
    x, y = keypoints_n
    radius = 2
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),
                 outline=(255, 0, 0))
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-75-73ed03f9af11>", line 1, in <module>
    for keypoints_n in kgt[:,:,0:2]:
IndexError: too many indices for tensor of dimension 2
for keypoints_n in kgt:
    x, y = keypoints_n
    radius = 2
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),
                 outline=(255, 0, 0))
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-76-f47c9cfb6854>", line 2, in <module>
    x, y = keypoints_n
ValueError: too many values to unpack (expected 2)
for keypoints_n in kgt:
    print(x)
    x, y = keypoints_n
    radius = 2
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),
                 outline=(255, 0, 0))
    
155.19336
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-77-e316b7a5d078>", line 3, in <module>
    x, y = keypoints_n
ValueError: too many values to unpack (expected 2)
kgt = kgt.cpu().detach().numpy()
for keypoints_n in kgt:
    print(x)
    x, y = keypoints_n
    radius = 2
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),
                 outline=(255, 0, 0))
    
155.19336
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-79-e316b7a5d078>", line 3, in <module>
    x, y = keypoints_n
ValueError: too many values to unpack (expected 2)
for keypoints_n in kgt:
    print(x)
    for (x, y) in keypoints_n:
        radius = 2
        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),
                     outline=(255, 0, 0))
155.19336
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-80-b235a671141e>", line 3, in <module>
    for (x, y) in keypoints_n:
TypeError: cannot unpack non-iterable numpy.float64 object
for keypoints_n in kgt[:,0:2]:
    print(x)
    x, y = keypoints_n
    radius = 2
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),
                 outline=(255, 0, 0))
    
155.19336
238.3916825127978
242.82842455867728
232.47602645162522
0.0
210.292316222228
222.12362834457315
186.62969197753762
222.12362834457315
140.78335750344996
239.87059652809097
164.44598174814035
102.3315931058281
93.45810901406922
91.97919499877607
155.57249765638147
55.00634461644735
174.79837985519242
81.62679689172403
72.75331279996513
40.217204463515856
208.81340220693482
211.77123023752114
image_ss.show()
# ------------------------------------------------------------------------------
# Copyright (c) Microsoft
# Licensed under the MIT License.
# Written by Bin Xiao (Bin.Xiao@microsoft.com)
# ------------------------------------------------------------------------------
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import math
import numpy as np
import torchvision
import cv2
from lib.core.inference import get_max_preds
def save_batch_image_with_joints(batch_image, batch_joints, batch_joints_vis,
                                 file_name, nrow=8, padding=2):
    '''
    batch_image: [batch_size, channel, height, width]
    batch_joints: [batch_size, num_joints, 3],
    batch_joints_vis: [batch_size, num_joints, 1],
    }
    '''
    grid = torchvision.utils.make_grid(batch_image, nrow, padding, True)
    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()
    ndarr = ndarr.copy()
    nmaps = batch_image.size(0)
    xmaps = min(nrow, nmaps)
    ymaps = int(math.ceil(float(nmaps) / xmaps))
    height = int(batch_image.size(2) + padding)
    width = int(batch_image.size(3) + padding)
    k = 0
    for y in range(ymaps):
        for x in range(xmaps):
            if k >= nmaps:
                break
            joints = batch_joints[k]
            joints_vis = batch_joints_vis[k]
            for joint, joint_vis in zip(joints, joints_vis):
                joint[0] = x * width + padding + joint[0]
                joint[1] = y * height + padding + joint[1]
                if joint_vis[0]:
                    # 绘制一个圆
                    cv2.circle(ndarr, (int(joint[0]), int(joint[1])), 2, [255, 0, 0], 2)
            k = k + 1
    cv2.imwrite(file_name, ndarr)
def save_batch_heatmaps(batch_image, batch_heatmaps, file_name,
                        normalize=True):
    '''
    batch_image: [batch_size, channel, height, width]
    batch_heatmaps: ['batch_size, num_joints, height, width]
    file_name: saved file name
    '''
    if normalize:
        batch_image = batch_image.clone()
        min = float(batch_image.min())
        max = float(batch_image.max())
        batch_image.add_(-min).div_(max - min + 1e-5)
    batch_size = batch_heatmaps.size(0)
    num_joints = batch_heatmaps.size(1)
    heatmap_height = batch_heatmaps.size(2)
    heatmap_width = batch_heatmaps.size(3)
    grid_image = np.zeros((batch_size*heatmap_height,
                           (num_joints+1)*heatmap_width,
                           3),
                          dtype=np.uint8)
    preds, maxvals = get_max_preds(batch_heatmaps.detach().cpu().numpy())
    for i in range(batch_size):
        image = batch_image[i].mul(255)\
                              .clamp(0, 255)\
                              .byte()\
                              .permute(1, 2, 0)\
                              .cpu().numpy()
        heatmaps = batch_heatmaps[i].mul(255)\
                                    .clamp(0, 255)\
                                    .byte()\
                                    .cpu().numpy()
        resized_image = cv2.resize(image,
                                   (int(heatmap_width), int(heatmap_height)))
        height_begin = heatmap_height * i
        height_end = heatmap_height * (i + 1)
        for j in range(num_joints):
            cv2.circle(resized_image,
                       (int(preds[i][j][0]), int(preds[i][j][1])),
                       1, [0, 0, 255], 1)
            heatmap = heatmaps[j, :, :]
            colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
            masked_image = colored_heatmap*0.7 + resized_image*0.3
            cv2.circle(masked_image,
                       (int(preds[i][j][0]), int(preds[i][j][1])),
                       1, [0, 0, 255], 1)
            width_begin = heatmap_width * (j+1)
            width_end = heatmap_width * (j+2)
            cv2.imwrite('heat_maps_{}.jpg'.format(j), colored_heatmap)
            grid_image[height_begin:height_end, width_begin:width_end, :] = \
                masked_image
            # grid_image[height_begin:height_end, width_begin:width_end, :] = \
            #     colored_heatmap*0.7 + resized_image*0.3
        grid_image[height_begin:height_end, 0:heatmap_width, :] = resized_image
    cv2.imwrite(file_name, grid_image)
# meta 是一个包含元数据的字典，用于存储与每个批次输入相关的额外信息。这些元数据通常包括中心点、缩放因子、得分、图像路径等信息，这些信息对于后续的预测转换、评估和调试非常重要
def save_debug_images(config, input, meta, target, joints_pred, output,
                      prefix):
    if not config.DEBUG.DEBUG:
        return
    if config.DEBUG.SAVE_BATCH_IMAGES_GT:
        save_batch_image_with_joints(
            input, meta['joints'], meta['joints_vis'],
            '{}_gt.jpg'.format(prefix)
        )
    if config.DEBUG.SAVE_BATCH_IMAGES_PRED:
        save_batch_image_with_joints(
            input, joints_pred, meta['joints_vis'],
            '{}_pred.jpg'.format(prefix)
        )
    if config.DEBUG.SAVE_HEATMAPS_GT:
        save_batch_heatmaps(
            input, target, '{}_hm_gt.jpg'.format(prefix)
        )
    if config.DEBUG.SAVE_HEATMAPS_PRED:
        save_batch_heatmaps(
            input, output, '{}_hm_pred.jpg'.format(prefix)
        )
        
batch_image = input
batch_joints = meta['joints']
batch_joints_vis = meta['joints_vis']
save_batch_image_with_joints(batch_image, batch_joints, batch_joints_vis,
                                 'ag.jpg', nrow=8, padding=2)
joint_2d = batch_joints[0]
joint_2d[:,0] *= (480/384)
joint_2d[:,1] *= (640/288)
joint_vis = meta['joints_vis']
joint_vis = joint_vis[0]
image_tensor = Transforms(image)
image_tensor = image_tensor.unsqueeze(0)
image_tensor = np.array(image_tensor)
joint_2d = joint_2d.unsqueeze(0)
joint_vis = joint_vis.unsqueeze(0)
image_tensor = Transforms(image_test)
image_tensor = image_tensor.unsqueeze(0)
save_batch_image_with_joints(image_tensor, joint_2d, joint_vis,
                                 'aaag.jpg', nrow=8, padding=2)
bbox_2 = vaild_dataset.coco
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-100-5ef1b0e62837>", line 1, in <module>
    bbox_2 = vaild_dataset.coco
NameError: name 'vaild_dataset' is not defined
for i in range(575):
    if valid_dataset.coco.anns[i]['image_id']:
        print(i)
        
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-101-a47e43dc5372>", line 2, in <module>
    if valid_dataset.coco.anns[i]['image_id']:
KeyError: 0
for i in range(575):
    if valid_dataset.coco.anns[i]['image_id']== 516038:
        print(i)
        
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-102-ce2500f07ea6>", line 2, in <module>
    if valid_dataset.coco.anns[i]['image_id']== 516038:
KeyError: 0
valid_dataset.coco.anns[0]['image_id']
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-103-bbe775b6ecf9>", line 1, in <module>
    valid_dataset.coco.anns[0]['image_id']
KeyError: 0
valid_dataset.coco.anns[1]['image_id']
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-104-80dc21d7105a>", line 1, in <module>
    valid_dataset.coco.anns[1]['image_id']
KeyError: 1
valid_dataset.coco.anns[i]
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-105-7dfef5f85104>", line 1, in <module>
    valid_dataset.coco.anns[i]
KeyError: 0
437577 in valid_dataset.coco.anns
Out[106]: True
for num in valid_dataset.coco.anns:
    if valid_dataset.coco.anns[num]['image_id'] == 516038:
        print(num)
        
455226
1212505.0
1741076.0
image_test.show()
bbox_test = valid_dataset.coco.anns[455226]['bbox']
bbox_test = [54.65,226.62,206.7,435.34]
joint_2d = batch_joints[0]
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda img: img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]))),
    #img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToPILImage()
])
image_tr = transform(image_test)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-113-5a36e90e5a32>", line 1, in <module>
    image_tr = transform(image_test)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 474, in __call__
    return self.lambd(img)
  File "<ipython-input-112-d43de6cadff5>", line 3, in <lambda>
    transforms.Lambda(lambda img: img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]))),
AttributeError: 'Tensor' object has no attribute 'crop'
transform = transforms.Compose([
    
    transforms.Lambda(lambda img: img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]))),
    transforms.ToTensor(),
    #img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToPILImage()
])
image_tr = transform(image_test)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-115-5a36e90e5a32>", line 1, in <module>
    image_tr = transform(image_test)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 474, in __call__
    return self.lambd(img)
  File "<ipython-input-114-4045dce83fb9>", line 3, in <lambda>
    transforms.Lambda(lambda img: img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]))),
NameError: name 'bboxes_test' is not defined
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox_test[0], bbox_test[1], bbox_test[2], bbox_test[3]))),
    transforms.ToTensor(),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToPILImage()
])
image_tr = transform(image_test)
image_tr.show()
image_tr.show()
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox_test[0], bbox_test[1], bbox_test[2], bbox_test[3]))),
    transforms.ToTensor(),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Resize((384,288)),
    transforms.ToPILImage()
])
image_tr = transform(image_test)
image_tr.show()
for keypoints_n in joint_2d:
    x,y = keypoints_n
    print(x)
        
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-123-d638048495d8>", line 2, in <module>
    x,y = keypoints_n
ValueError: too many values to unpack (expected 2)
for keypoints_n in joint_2d[:,0:2]:
    x,y = keypoints_n
    print(x)
    
tensor(302.4896, dtype=torch.float64)
tensor(308.0355, dtype=torch.float64)
tensor(295.0950, dtype=torch.float64)
tensor(4.5000, dtype=torch.float64)
tensor(267.3654, dtype=torch.float64)
tensor(282.1545, dtype=torch.float64)
tensor(237.7871, dtype=torch.float64)
tensor(282.1545, dtype=torch.float64)
tensor(180.4792, dtype=torch.float64)
tensor(304.3382, dtype=torch.float64)
tensor(210.0575, dtype=torch.float64)
tensor(132.4145, dtype=torch.float64)
tensor(121.3226, dtype=torch.float64)
tensor(119.4740, dtype=torch.float64)
tensor(198.9656, dtype=torch.float64)
tensor(73.2579, dtype=torch.float64)
tensor(222.9980, dtype=torch.float64)
tensor(106.5335, dtype=torch.float64)
tensor(95.4416, dtype=torch.float64)
tensor(54.7715, dtype=torch.float64)
tensor(265.5168, dtype=torch.float64)
tensor(269.2140, dtype=torch.float64)
tensor(213.7548, dtype=torch.float64)
joint_2d_np = np.array(joint_2d[:,0:2])
draw = ImageDraw.Draw(image_test)
for keypoints_n in joint_2d[:,0:2]:
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
image_test.show()
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d_np[:,0:2]:
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
image_tr.show()
image_tr.show()
torch.save(batch_joints, 'batch_joints.pth')
torch.save(batch_image, 'batch_image.pth')
torch.save(batch_joints_vis, 'batch_joints_vis.pth')
image_tr.show()
image_test = Image.open('data/000000516038.jpg')
image_test.show()
image_tr = transform(image_test)
image_tr.show()
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox_test[0], bbox_test[1], bbox_test[2], bbox_test[3]))),
    transforms.ToTensor(),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Resize((386,290)),
    transforms.ToPILImage()
])
image_tr = transform(image_test)
image_tr.show()
draw = ImageDraw.Draw(image_tr)
for keypoints_n in batch_joints[0]:
    # 将热力图中的坐标转换为原图中的坐标
    orig_x = int(x * scale_x)
    orig_y = int(y * scale_y)
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-140-53ac09c7124a>", line 4, in <module>
    orig_x = int(x * scale_x)
NameError: name 'scale_x' is not defined
draw = ImageDraw.Draw(image_tr)
for keypoints_n in batch_joints[0]:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n[:,0:2]
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-141-0b609d753e77>", line 4, in <module>
    x,y = keypoints_n[:,0:2]
IndexError: too many indices for tensor of dimension 1
draw = ImageDraw.Draw(image_tr)
joint_2d
for keypoints_n in joint_2d:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-142-7773cc431cab>", line 5, in <module>
    x,y = keypoints_n
ValueError: too many values to unpack (expected 2)
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-143-d210a02e0a2c>", line 4, in <module>
    x,y = keypoints_n
ValueError: too many values to unpack (expected 2)
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d:
    # 将热力图中的坐标转换为原图中的坐标
    for (x, y) in keypoints_n:
        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-144-601544daaf46>", line 4, in <module>
    for (x, y) in keypoints_n:
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\_tensor.py", line 698, in __iter__
    raise TypeError('iteration over a 0-d tensor')
TypeError: iteration over a 0-d tensor
for keypoints_n in joint_2d:
    x,y = keypoints_n
    print(x)
    
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-145-dc37a4e7044d>", line 2, in <module>
    x,y = keypoints_n
ValueError: too many values to unpack (expected 2)
for keypoints_n in joint_2d:
    print(keypoints_n)
    
tensor([302.4896, 181.7614,   0.0000], dtype=torch.float64)
tensor([308.0355, 162.0426,   0.0000], dtype=torch.float64)
tensor([295.0950, 158.7561,   0.0000], dtype=torch.float64)
tensor([4.5000, 6.4444, 0.0000], dtype=torch.float64)
tensor([267.3654, 158.7561,   0.0000], dtype=torch.float64)
tensor([282.1545, 231.0586,   0.0000], dtype=torch.float64)
tensor([237.7871, 201.4803,   0.0000], dtype=torch.float64)
tensor([282.1545, 365.8041,   0.0000], dtype=torch.float64)
tensor([180.4792, 342.7987,   0.0000], dtype=torch.float64)
tensor([304.3382, 474.2577,   0.0000], dtype=torch.float64)
tensor([210.0575, 441.3930,   0.0000], dtype=torch.float64)
tensor([132.4145, 296.7881,   0.0000], dtype=torch.float64)
tensor([121.3226, 296.7881,   0.0000], dtype=torch.float64)
tensor([119.4740, 493.9766,   0.0000], dtype=torch.float64)
tensor([198.9656, 490.6901,   0.0000], dtype=torch.float64)
tensor([ 73.2579, 658.3004,   0.0000], dtype=torch.float64)
tensor([222.9980, 697.7381,   0.0000], dtype=torch.float64)
tensor([106.5335, 697.7381,   0.0000], dtype=torch.float64)
tensor([ 95.4416, 691.1651,   0.0000], dtype=torch.float64)
tensor([ 54.7715, 691.1651,   0.0000], dtype=torch.float64)
tensor([265.5168, 740.4623,   0.0000], dtype=torch.float64)
tensor([269.2140, 747.0352,   0.0000], dtype=torch.float64)
tensor([213.7548, 747.0352,   0.0000], dtype=torch.float64)
for keypoints_n in joint_2d_np:
    print(keypoints_n)
    
[302.48960314 181.76142192]
[308.0355307  162.04256838]
[295.09503306 158.75609279]
[4.5        6.44444444]
[267.36539528 158.75609279]
[282.15453543 231.05855576]
[237.78711497 201.48027545]
[282.15453543 365.80405493]
[180.47919688 342.7987258 ]
[304.33824566 474.25774939]
[210.05747719 441.39299349]
[132.41449138 296.78806755]
[121.32263627 296.78806755]
[119.47399375 493.97660292]
[198.96562207 490.69012733]
[ 73.25793077 658.3003824 ]
[222.99797482 697.73808947]
[106.53349611 697.73808947]
[ 95.441641  691.1651383]
[ 54.77150558 691.1651383 ]
[265.51675276 740.46227214]
[269.2140378  747.03522332]
[213.75476222 747.03522332]
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d_np:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
image_single = np.array(batch_image[0].unsqueeze(0))
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-149-6eb7bef21e45>", line 1, in <module>
    image_single = np.array(batch_image[0].unsqueeze(0))
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\_tensor.py", line 732, in __array__
    return self.numpy()
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
image_single = batch_image[0].unsqueeze(0)
joints_single = batch_joints[0].unsqueeze(0)
joints_vis_single = batch_joints_vis[0].unsqueeze(0)
save_batch_image_with_joints(image_single, joints_single, joints_vis_single,
                                 'file_name.jpg', nrow=8, padding=2)
save_batch_image_with_joints(image_single, joints_single, joints_vis_single,
                                 'file_name.jpg', nrow=1, padding=2)
save_batch_image_with_joints(image_single, joints_single, joints_vis_single,
                                 'file_name.jpg', nrow=1, padding=1)
save_batch_image_with_joints(image_single, joints_single, joints_vis_single,
                                 'file_name.jpg', nrow=1, padding=0)
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox_test[0], bbox_test[1], bbox_test[2], bbox_test[3]))),
    transforms.ToTensor(),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Resize((288,384)),
    transforms.ToPILImage()
])
image_tr = transform(image_test)
image_tr.show()
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d_np:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
np.max(joint_2d_np,axis=0)
Out[161]: array([308.0355307 , 747.03522332])
np.max(joint_2d_np,axis=1)
Out[162]: 
array([302.48960314, 308.0355307 , 295.09503306,   6.44444444,
       267.36539528, 282.15453543, 237.78711497, 365.80405493,
       342.7987258 , 474.25774939, 441.39299349, 296.78806755,
       296.78806755, 493.97660292, 490.69012733, 658.3003824 ,
       697.73808947, 697.73808947, 691.1651383 , 691.1651383 ,
       740.46227214, 747.03522332, 747.03522332])
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox_test[0], bbox_test[1], bbox_test[2], bbox_test[3]))),
    transforms.ToTensor(),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.Resize((384,288)),
    transforms.ToPILImage()
])
image_tr = transform(image_test)
image_tr.size
Out[165]: (288, 384)
image_tr.show()
joint_2d_np = np.ones(23,2)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-167-70b5f04d64e7>", line 1, in <module>
    joint_2d_np = np.ones(23,2)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\numpy\core\numeric.py", line 204, in ones
    a = empty(shape, dtype, order)
TypeError: Cannot interpret '2' as a data type
joint_2d_np_t= np.ones([23,2])
joint_2d_np_t[:,0] = joint_2d_np[:,0]*(2332/384)
joint_2d_np_t[:,1] = joint_2d_np[:,1]*(774/288)
joint_2d_np_t[:,0] = joint_2d_np[:,0]*(384/2322)
joint_2d_np_t[:,1] = joint_2d_np[:,1]*(288/774)
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d_np_t:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
joint_2d_np_t = joint_2d_np
joint_2d_np_t[:,0] += 1161
joint_2d_np_t[:,1] -= 387
joint_2dd = meta['joints'][0]
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d_np_t:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
joint_2d_np_t = joint_2d_np
joint_2d_np_t[:,0] += 61.825
joint_2d_np_t[:,1] -= 186
joint_2d_np = np.array(joint_2d[:,0:2])
joint_2d_np_t = joint_2d_np
joint_2d_np_t[:,0] += 61.825
joint_2d_np_t[:,1] -= 186
joint_2d_np = np.array(joint_2d[:,0:2])
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d_np_t:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
image_tr = transform(image_test)
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d_np_t:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
joint_2dd = meta['joints'][8]
joint_2dd_np = np.array(joint_2dd_np[:,0:2])
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-187-8706cad8b2b9>", line 1, in <module>
    joint_2dd_np = np.array(joint_2dd_np[:,0:2])
NameError: name 'joint_2dd_np' is not defined
joint_2dd_np = np.array(joint_2dd[:,0:2])
joint_2dd_np_t = np.ones([23,2])
joint_2d_np_t[:,0] = joint_2dd[:,0] + 170
joint_2d_np_t[:,1] = joint_2dd[:,1] - 153
image_tr = transform(image_test)
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2dd_np_t:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
draw = ImageDraw.Draw(image_tr)
for keypoints_n in joint_2d_np_t:
    # 将热力图中的坐标转换为原图中的坐标
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0),outline=(255, 0, 0))
image_tr.show()
image_tr.size
Out[194]: (288, 384)
joint_2dd_np_t[:,0] = joint_2dd[:,0] + 170
joint_2dd_np_t[:,1] = joint_2dd[:,1] - 153
def transform_preds(coords, center, scale, output_size):
    target_coords = np.zeros(coords.shape)
    trans = get_affine_transform(center, scale, 0, output_size, inv=1)
    for p in range(coords.shape[0]):
        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)
    return target_coords
joint_c = transform_preds(joint_2dd,meta['center'][8],meta['scale'][8], image_tr.size)
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:71: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  if AsyncioInteractiveConsole is not None:
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:71: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  if AsyncioInteractiveConsole is not None:
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-197-4daf2849b1d1>", line 1, in <module>
    joint_c = transform_preds(joint_2dd,meta['center'][8],meta['scale'][8], image_tr.size)
  File "<ipython-input-196-0f5bcf89a0d8>", line 3, in transform_preds
    trans = get_affine_transform(center, scale, 0, output_size, inv=1)
  File "<ipython-input-35-2d53ddd0cfe2>", line 86, in get_affine_transform
    src[0, :] = center + scale_tmp * shift
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torch\_tensor.py", line 744, in __array_wrap__
    return torch.from_numpy(array)
TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
type(meta['center'][8])
Out[198]: torch.Tensor
joint_c = transform_preds(joint_2dd,np.array(meta['center'][8]),np.array(meta['scale'][8]), image_tr.size)
joint_c = transform_preds(joint_2dd,np.array([145,580.5]),np.array(meta['scale'][8]), image_tr.size)
image_1 = Image.open('ag1.jpg')
origin = (0, 0)
center = (width // 2, height // 2)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-202-ba0de3c0448e>", line 2, in <module>
    center = (width // 2, height // 2)
NameError: name 'width' is not defined
origin = (0, 0)
center = (image_1.size(0) // 2, image_1.size(1)  // 2)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-203-eaae7e4b72ec>", line 2, in <module>
    center = (image_1.size(0) // 2, image_1.size(1)  // 2)
TypeError: 'tuple' object is not callable
origin = (0, 0)
center = (image_1.size[0]// 2, image_1.size[1]  // 2)
draw = ImageDraw.Draw(image_1)
radius = 5
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
draw = ImageDraw.Draw(image_1)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
center = meta['center'][0]
draw = ImageDraw.Draw(image_1)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
image_00 = Image.open('data/000000516038.jpg')
draw = ImageDraw.Draw(image_00)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_00.show()
image_00 = Image.open('data/000000516038.jpg')
draw = ImageDraw.Draw(image_00)
radius = 5
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_00.show()
draw = ImageDraw.Draw(image_1)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
center = meta['center'][0]
draw = ImageDraw.Draw(image_1)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
center = meta['center'][1]
draw = ImageDraw.Draw(image_1)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
center = meta['center'][2]
draw = ImageDraw.Draw(image_1)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
center = meta['center'][3]
draw = ImageDraw.Draw(image_1)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
center = meta['center'][3]
draw = ImageDraw.Draw(image_1)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
center = meta['center'][4]
draw = ImageDraw.Draw(image_1)
radius = 20
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_1.show()
image_00 = Image.open('data/000000516038.jpg')
draw = ImageDraw.Draw(image_00)
radius = 5
# 绘制原点（左上角）
draw.ellipse((origin[0] - radius, origin[1] - radius, origin[0] + radius, origin[1] + radius), fill=(255, 0, 0), outline=(255, 0, 0))
# 绘制中心点
draw.ellipse((center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius), fill=(0, 255, 0), outline=(0, 255, 0))
# 显示图像
image_00.show()
center = meta['center'][2]
image_00.show()
image_1.show()
s=input[0]
tr = transforms.ToPILImage()
image_test = tr(s)
image_test.show()
joints_vis_test = batch_joints_vis[0]
joint_2 = meta['joints'][1]
joint_2_v = batch_joints_vis[1]
s=input[1]
s=input[1]
image_test = tr(s)
image_test.show()
s=input[2]
image_test = tr(s)
image_test.show()
s=input[3]
image_test = tr(s)
image_test.show()
s=input[4]
image_test = tr(s)
image_test.show()
jj = meta['joints'][0]
width, height = 2500,800
b_image = Image.new('RGB',(width,height),(0,0,0))
b_image.save("black_image.jpg")
b_image.show()
draw = ImageDraw.Draw(image)
for keypoints_n in joint_2d_np:
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
    
    
draw = ImageDraw.Draw(b_image)
for keypoints_n in joint_2d_np:
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
b_image.show()
draw = ImageDraw.Draw(b_image)
for keypoints_n in joint_2dd_np:
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
b_image.show()
b_image = Image.new('RGB',(width,height),(0,0,0))
b_image.show()
draw = ImageDraw.Draw(b_image)
for keypoints_n in joint_2d_np:
    x,y = keypoints_n
    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
a,b = meta['center'][0]
draw.ellipse((a - radius, b - radius, a + radius, b + radius), fill=(0, 255, 0), outline=(0, 255, 0))
b_image.show()
image.show()
image = Image.open('data/000000516038.jpg')
image.show()
draw = ImageDraw.Draw(image)
draw.ellipse((a - radius, b - radius, a + radius, b + radius), fill=(0, 255, 0), outline=(0, 255, 0))
image.show()
for n in range(443):
    if valid_dataset.coco.dataset['image'][n]['file_name'] == '000000516038.jpg':
        print(n)
        
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-252-3720761cb874>", line 2, in <module>
    if valid_dataset.coco.dataset['image'][n]['file_name'] == '000000516038.jpg':
KeyError: 'image'
for n in range(443):
    if valid_dataset.coco.dataset['images'][n]['file_name'] == '000000516038.jpg':
        print(n)
        
432
bbox = valid_dataset.coco.dataset['annotations'][432]['bbox']
bbox = [182.29,90.68,420.67, 415.35]
transform = transforms.Compose([
    
    transforms.Lambda(lambda img: img[:, bbox[1]:bbox[3], bbox[0]:bbox[2]]),
    #img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225]),
    transforms.Resize((384,288)),
    transforms.ToPILImage()
])
image_tr = transform(image)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-257-5ff22829a1d6>", line 1, in <module>
    image_tr = transform(image)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 474, in __call__
    return self.lambd(img)
  File "<ipython-input-256-cf5721798072>", line 3, in <lambda>
    transforms.Lambda(lambda img: img[:, bbox[1]:bbox[3], bbox[0]:bbox[2]]),
TypeError: 'JpegImageFile' object is not subscriptable
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]))),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
    transforms.Resize((384, 288)),
    transforms.ToPILImage()
])
image_tr = transform(image)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-259-5ff22829a1d6>", line 1, in <module>
    image_tr = transform(image)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 474, in __call__
    return self.lambd(img)
  File "<ipython-input-258-60108505a1a8>", line 3, in <lambda>
    transforms.Lambda(lambda img: img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]))),
NameError: name 'bboxes_test' is not defined
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bboxes[0], bboxes[1], bboxes[2], bboxes[3]))),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
    transforms.Resize((384, 288)),
    transforms.ToPILImage()
])
image_tr = transform(image)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-261-5ff22829a1d6>", line 1, in <module>
    image_tr = transform(image)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\torchvision\transforms\transforms.py", line 474, in __call__
    return self.lambd(img)
  File "<ipython-input-260-c0225c4f30c9>", line 2, in <lambda>
    transforms.Lambda(lambda img: img.crop((bboxes[0], bboxes[1], bboxes[2], bboxes[3]))),
NameError: name 'bboxes' is not defined
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
    transforms.Resize((384, 288)),
    transforms.ToPILImage()
])
image_tr = transform(image)
image_tr.show()
image.show()
image.show()
image.show()
image_tr.show()
for n in range(575):
    if valid_dataset.coco.dataset['annotations'][n]['image_id'] == 516038:
        print(n)
        
200
249
260
bbox = valid_dataset.coco.dataset['annotations'][200]['bbox'] 
bbox = [54.65,226.62,206.70,435.34]
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
    transforms.Resize((384, 288)),
    transforms.ToPILImage()
])
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.ToPILImage()
])
image_tr = transform(image)
image_tr.show()
image_tr.show()
image_tr.show()
joint_2d_np_t[:,0] = joint_2d_np[:,0]
joint_2d_np_t[:,1] = joint_2d_np[:,1]-387
image.show()
image_tr.show()
image_tr_1 = image_tr.copy()
image_tr_1.show()
joint_v = meta['joints_vis'][0]
for joint, joint_vis in zip (joint_2d_np, np.array(joint_v)):
    print(joint)
    
[307.48960314 186.76142192]
[313.0355307  167.04256838]
[300.09503306 163.75609279]
[ 9.5        11.44444444]
[272.36539528 163.75609279]
[287.15453543 236.05855576]
[242.78711497 206.48027545]
[287.15453543 370.80405493]
[185.47919688 347.7987258 ]
[309.33824566 479.25774939]
[215.05747719 446.39299349]
[137.41449138 301.78806755]
[126.32263627 301.78806755]
[124.47399375 498.97660292]
[203.96562207 495.69012733]
[ 78.25793077 663.3003824 ]
[227.99797482 702.73808947]
[111.53349611 702.73808947]
[100.441641  696.1651383]
[ 59.77150558 696.1651383 ]
[270.51675276 745.46227214]
[274.2140378  752.03522332]
[218.75476222 752.03522332]
for joint, joint_vis in zip (joint_2d_np, np.array(joint_v)):
    print(joint_vis)
    
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[0. 0. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
[1. 1. 0.]
shape_1 = (254,589)
image_tr.show()
joint_2d_np_t[:,0] = joint_2d_np[:,0] * (152/254)
joint_2d_np_t[:,1] = joint_2d_np[:,1] * (208/590)
draw = ImageDraw.Draw(image_tr_1)
for joint, joint_vis in zip(joint_2d_np_t, np.array(joint_v)):
    x,y = joint
    if joint_vis[0]:
        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
image_tr_1.show()
    
image_tr_1.copy()
Out[291]: <PIL.Image.Image image mode=RGB size=152x208>
image_tr_1 = image_tr.copy()
bbox 
Out[293]: [54.65, 226.62, 206.7, 435.34]
bbox_tr = [54.65,203.62,206.7,457.34]
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox_tr[0], bbox_tr[1], bbox_tr[2], bbox_tr[3]))),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.ToPILImage()
])
image_tr_2 = transform(image)
image_tr_2.show()
bbox_tr = [54.65, 155.62, 206.7, 507.34]
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox_tr[0], bbox_tr[1], bbox_tr[2], bbox_tr[3]))),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.ToPILImage()
])
image_tr_2 = transform(image)
image_tr_2.show()
bbox_tr = [54.65, 153.62, 206.7, 508.34]
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.crop((bbox_tr[0], bbox_tr[1], bbox_tr[2], bbox_tr[3]))),
    # img.crop((bboxes_test[0], bboxes_test[1], bboxes_test[2], bboxes_test[3]
    transforms.ToTensor(),
    transforms.ToPILImage()
])
image_tr_2 = transform(image)
image_tr_2.show()
joint_2d_np_t[:,0] = joint_2d_np[:,0] * (152/254)
joint_2d_np_t[:,1] = joint_2d_np[:,1] * (353/590)
draw = ImageDraw.Draw(image_tr_1)
for joint, joint_vis in zip(joint_2d_np_t, np.array(joint_v)):
    x,y = joint
    if joint_vis[0]:
        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
image_tr_1.show()
image_tr_1 = image_tr_2.copy()
draw = ImageDraw.Draw(image_tr_1)
for joint, joint_vis in zip(joint_2d_np_t, np.array(joint_v)):
    x,y = joint
    if joint_vis[0]:
        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=(255, 0, 0), outline=(255, 0, 0))
image_tr_1.show()
int(batch_image.size(2) + padding)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-307-3b8a14b4fa11>", line 1, in <module>
    int(batch_image.size(2) + padding)
NameError: name 'padding' is not defined
save_batch_image_with_joints(batch_image, batch_joints, batch_joints_vis,
                                 file_name, nrow=8, padding=5)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-308-b22957aac2e7>", line 2, in <module>
    file_name, nrow=8, padding=5)
NameError: name 'file_name' is not defined
save_batch_image_with_joints(batch_image, batch_joints, batch_joints_vis,
                                 test_image, nrow=8, padding=5)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-309-1edf0aec567f>", line 2, in <module>
    test_image, nrow=8, padding=5)
NameError: name 'test_image' is not defined
save_batch_image_with_joints(batch_image, batch_joints, batch_joints_vis,
                                 'test_image.jpg', nrow=8, padding=5)
save_batch_image_with_joints(batch_image, batch_joints, batch_joints_vis,
                                 'test_image.jpg', nrow=8, padding=2)
save_batch_image_with_joints(batch_image, batch_joints, batch_joints_vis,
                                 'test_image.jpg', nrow=8, padding=0)
batch_joints = meta['joints']
batch_image = meta['input']
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-314-fe2909487217>", line 1, in <module>
    batch_image = meta['input']
KeyError: 'input'
batch_image = input
save_batch_image_with_joints(batch_image, batch_joints, batch_joints_vis,
                                 'test_image.jpg', nrow=8, padding=2)

```

## 知识点学习，相似变换、仿射变换、投影变换

![image-20240621105912519](D:\Work_APP\Typora\assets\image-20240621105912519.png)

相似

![image-20240621110007830](D:\Work_APP\Typora\assets\image-20240621110007830.png)

![image-20240621105957784](D:\Work_APP\Typora\assets\image-20240621105957784.png)

仿射变换

角度关系可能会变

![image-20240621105939957](D:\Work_APP\Typora\assets\image-20240621105939957.png)

![image-20240621110108424](D:\Work_APP\Typora\assets\image-20240621110108424.png)

## 单张图片关键点显示

![image-20240621154110615](D:\Work_APP\Typora\assets\image-20240621154110615.png)

预测的还可以17

![image-20240621232633841](D:\Work_APP\Typora\assets\image-20240621232633841.png)

## 手动加载部分权重

```Python
# 手动加载部分权重
state_dict = model.state_dict()

# 仅加载前17个权重
state_dict['final_layer.weight'][:17] = checkpoint['final_layer.weight'][:17]
state_dict['final_layer.bias'][:17] = checkpoint['final_layer.bias'][:17]
model.load_state_dict(state_dict)
```

<img src="D:\Work_APP\Typora\assets\image-20240622152542433.png" alt="image-20240622152542433" style="zoom: 50%;" />

这个效果不是很好，我的做法是加载一下别人训练好的部分部分权重。

<img src="D:\Work_APP\Typora\assets\image-20240622153210873.png" alt="image-20240622153210873" style="zoom:50%;" />



很勾八奇怪

![image-20240622154521944](D:\Work_APP\Typora\assets\image-20240622154521944.png)

```
'center'[466.1563  465.17047]
'scale'[2.5152328 3.353644 ]
'bbox'[264.9376922607422, 263.9518524169922, 403.4372680664062, 403.4372680664062]


'joints_3d'[[473.68356323 444.94241333   0.        ], [500.99609375 448.02987671   0.        ], [479.83926392 530.78564453   0.        ], [506.21838379 622.56884766   0.        ], [493.66082764 621.99542236   0.        ], [488.23513794 616.77313232   0.        ], [445.90008545 441.81585693   0.        ], [456.1890564  537.15808105   0.        ], [467.30923462 633.76934814   0.        ], [452.6399231  627.30395508   0.        ], [445.83035278 621.79571533   0.        ], [488.18673706 397.43405151   0.        ], [481.0284729  340.39694214   0.        ], [478.51754761 318.80801392   0.        ], [485.76895142 297.57162476   0.        ], [454.01608276 359.759552     0.        ], [430.05877686 415.7348938    0.        ]]
'joints_3d_vis'[[1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.], [1. 1. 0.]]
'F:\\\\data\\\\processed_h36m\\\\processed\\images\\S1/S1_Directions_1.54138969/S1_Directions_1.54138969_000001.jpg'

```

<img src="D:\Work_APP\Typora\assets\image-20240626151436984.png" alt="image-20240626151436984" style="zoom: 33%;" />![image-20240626151447709](D:\Work_APP\Typora\assets\image-20240626151447709.png)

<img src="D:\Work_APP\Typora\assets\image-20240626151506963.png" alt="image-20240626151506963" style="zoom: 67%;" />

# 2024.6.29好好干活

final_layer.weight

final_layer.bias

# 2024.7.4重新写OpenCap的dataset用来训练contextpose



学习xml

因为仿着h36m，它里面有个metadata.xml用来存放h36m的标签，

![image-20240704093801218](D:\Work_APP\Typora\assets\image-20240704093801218.png)

XML是可扩展性标记语言，传输存储数据，不是为了展示数据。XML标签必须自定义，写标签名的时候有含义。



如何写一段XML，必须得有根节点，可以在浏览器上展示

![image-20240704095610746](D:\Work_APP\Typora\assets\image-20240704095610746.png![image-20240704095637896](D:\Work_APP\Typora\assets\image-20240704095637896.png)

![image-20240704095755724](D:\Work_APP\Typora\assets\image-20240704095755724.png)

![image-20240704095947775](D:\Work_APP\Typora\assets\image-20240704095947775.png)

![image-20240704100135582](D:\Work_APP\Typora\assets\image-20240704100135582.png)

![image-20240704100211825](D:\Work_APP\Typora\assets\image-20240704100211825.png)

![image-20240704100238632](D:\Work_APP\Typora\assets\image-20240704100238632.png)

头声明

![image-20240704100411075](D:\Work_APP\Typora\assets\image-20240704100411075.png)

![image-20240704100521725](D:\Work_APP\Typora\assets\image-20240704100521725.png)

大小写注意

![image-20240704100613475](D:\Work_APP\Typora\assets\image-20240704100613475.png)

![image-20240704100655405](D:\Work_APP\Typora\assets\image-20240704100655405.png)

注释

![image-20240704100721423](D:\Work_APP\Typora\assets\image-20240704100721423.png)

特殊字符使用实体

![image-20240704100835497](D:\Work_APP\Typora\assets\image-20240704100835497.png)

需要转义的字符

![image-20240704100918294](D:\Work_APP\Typora\assets\image-20240704100918294.png)





![image-20240704101008137](D:\Work_APP\Typora\assets\image-20240704101008137.png)

属性规则。

一个标签可以多个属性，属性的值必须使用引号标签。，

命名规则：数字字母下划线,数字不能开头。
属性就是表示标签自身的一些额外信息。

还有，在解析XML数据时，属性会带来额外的解析代码（多了一步，比较麻烦）



![image-20240704101943474](D:\Work_APP\Typora\assets\image-20240704101943474.png)

需要大段转移字符进行替换的时候。

什么时候用实体替换，什么时候cdata，CDATA必须大写



## Opencap的标签制作

![image-20240705111241992](D:\Work_APP\Typora\assets\image-20240705111241992.png)



# 2024.7.8重写opencap的dataset（二）

仿着human3.6m的来，重写受试者-动作-子动作的映射关系。
![image-20240708153923039](D:\Work_APP\Typora\assets\image-20240708153923039.png)

![image-20240708154013865](D:\Work_APP\Typora\assets\image-20240708154013865.png)

方法是做一个excel，这个Excel包含了受试者，动作名，动作和新动作的编号，如下图所示。

![image-20240708154146596](D:\Work_APP\Typora\assets\image-20240708154146596.png)

然后实现的代码是

其中subactions_l的内容是

![image-20240708154316121](D:\Work_APP\Typora\assets\image-20240708154316121.png)

```
transformed_list = [[(item[1], item[2])] for item in subactions_l]
# actions可以用读取excel的方式获取。
for k,v in nested_dict.items():
    for index, action in  enumerate(actions):
        nested_dict[k][transformed_list[index][0]] = action
```

这样就能获得最终的nested_dict



# 2023.7.13使用mmdection进行批量人体目标检测

遇到问题1，model.show_result，model没有这个属性就报错了

```
#使用csdn上找的代码
def show_result_pyplot(model, img, result, score_thr=0.3, fig_size=(15, 10)):
    """Visualize the detection results on the image.
    Args:
        model (nn.Module): The loaded detector.
        img (str or np.ndarray): Image filename or loaded image.
        result (tuple[list] or list): The detection result, can be either
            (bbox, segm) or just bbox.
        score_thr (float): The threshold to visualize the bboxes and masks.
        fig_size (tuple): Figure size of the pyplot figure.
    """
    if hasattr(model, 'module'):
        model = model.module
    img = model.show_result(img, result, score_thr=score_thr, show=False)
    return img
    # plt.figure(figsize=fig_size)
    # plt.imshow(mmcv.bgr2rgb(img))
    # plt.show()

```

解决方法：自定义绘制边界框函数，从result中获取



# 2024.7.15移除前缀，让权重文件可加载

```
# 假设权重文件是一个字典，格式如下
weights = {
    'backbone.conv1.weight': [1, 2, 3],
    'backbone.conv1.bias': [0.1, 0.2],
    'keypoint_head.fc1.weight': [4, 5, 6],
    'keypoint_head.fc1.bias': [0.3, 0.4],
    'other_layer.weight': [7, 8, 9]
}

# 要去除的前缀列表
prefixes_to_remove = ['backbone.', 'keypoint_head.']

# 创建一个新的字典，用于存储去除前缀后的键值对
new_weights = OrderedDict()

# 遍历原始权重字典
for key, value in weights.items():
    new_key = key
    # 检查每个前缀
    for prefix in prefixes_to_remove:
        if key.startswith(prefix):
            new_key = key[len(prefix):]
            break
    new_weights[new_key] = value

print(new_weights)

```

# 2024.7.16将OpenCap的marker点绘制到图片上

```
extra_rotation = np.array([[ 0,  0, -1],
                            [ 1,  0,  0],
                            [ 0, -1,  0]])

translation_vector = np.array([-0.9697277235218427, 
                                0.3914217038805379, 
                               -0.2622398277673762])

adjusted_rotation_matrix = extra_rotation @ rotation_matrix


```



**其实我论文里还需要验证的是，足部关键点的信息好坏，以及骨长约束是不是更好了？**



# 2024.7.25解决将Opencap视频提取成帧时候的重复帧问题，以及帧命名不符合实际的问题。

问题描述，这边采用的是human3.6m对视频帧采样的方式，即让每一帧的三维点距离阈值去区分动作变化的幅度。

为啥有的视频明明只有200帧却会出现800的情况？

为啥分离出来的帧是重复的？

方向，把那些出现帧异常的动作单独debug试试？



# 2024.7.26解决帧不同步问题，在squats.trc这个文件中点的个数是801个，但是视频的帧数只有480个，因此要想办法同步

```Python
# 轨迹绘制代码
from mvn.datasets.utilsTRC import *
poses_3d = []
Markers_data = trc_2_dict('data/OpenCap/squats1.trc')
for mark in Markers_data['marker_names']:
    poses_3d.append(Markers_data['markers'][mark])
R_HJC = poses_3d[47]
L_HJC = poses_3d[48]
M_HJC = (L_HJC + R_HJC)/2
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
positions  = M_HJC
# 创建一个图形对象
fig = plt.figure()
ax = fig.add_subplot(111,projection='3d')
# 提取X,Y,Z坐标
x = positions[:, 0]
y = positions[:, 2]
z = positions[:, 1]
# 绘制运动轨迹
ax.plot(x,y,z,label='Trajectory',color='b')
ax.scatter(x[0],y[0],z[0],color='r',label='Start', s=100)#起点
ax.scatter(x[-1],y[-1],z[-1], color='g',label='End', s=100)#终点
#设置坐标轴标签
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlable('Z')
#设置图例
ax.legend()
# 显示图像
plt.show()

```



![image-20240726144223401](D:\Work_APP\Typora\assets\image-20240726144223401.png)

![image-20240726144345781](D:\Work_APP\Typora\assets\image-20240726144345781.png)

![image-20240726144622195](D:\Work_APP\Typora\assets\image-20240726144622195.png)

1,2图显示，在Y方向上变化最明显，因此在绘图3过程中将原先的(x,y,z)→(x,z,y)

```python
import numpy as np
import matplotlib.pyplot as plt
positions = M_HJC
t = np.arange(positions.shape[0])
x = positions[:, 0]
y = positions[:, 1]
z = positions[:, 2]

#创建图像对象
fig, axs = plt.subplots(3,1,figsize=(10,8))
# 绘制X随时间变化的图像
axs[0].plot(t,x,label='X',color='r')
axs[0].set_xlabel('Time')
axs[0].set_ylabel('X')
axs[0].legend()#添加图例，右上角那个
axs[0].grid(True)
# 绘制 Y 随时间变化的图像
axs[1].plot(t, y, label='Y', color='g')
axs[1].set_xlabel('Time')
axs[1].set_ylabel('Y')
axs[1].legend()
axs[1].grid(True)
# 绘制 Z 随时间变化的图像
axs[2].plot(t, z, label='Z', color='b')
axs[2].set_xlabel('Time')
axs[2].set_ylabel('Z')
axs[2].legend()
axs[2].grid(True)

#调整子图布局
plt.tight_layout()#使得子图元素（如标题、标签、图例等）之间不重叠，从而使得整个图形布局更加紧凑、美观。
#显示图像
plt.show()



```

这是光学动捕直接获得的

![image-20240726144747178](D:\Work_APP\Typora\assets\image-20240726144747178.png)

经过平移变化过后

![image-20240731105336581](D:\Work_APP\Typora\assets\image-20240731105336581.png)

这是HRnet获取的

![image-20240727090537309](D:\Work_APP\Typora\assets\image-20240727090537309.png)

这是openpose获取的

![image-20240727085506201](D:\Work_APP\Typora\assets\image-20240727085506201.png)

这是用motionBERT预测的

![image-20240726163824271](D:\Work_APP\Typora\assets\image-20240726163824271.png)

先使用光流法，然后再使用双视角

![image-20240731084738022](D:\Work_APP\Typora\assets\image-20240731084738022.png)

经卡尔曼滤波之后的双视图

![image-20240731091156681](D:\Work_APP\Typora\assets\image-20240731091156681.png)

经卡尔曼滤波之后（二）

![image-20240731092116934](D:\Work_APP\Typora\assets\image-20240731092116934.png)

human3.6M,S1Directions 1.55011271

![image-20240726161117177](D:\Work_APP\Typora\assets\image-20240726161117177.png)

gt

![image-20240726161940417](D:\Work_APP\Typora\assets\image-20240726161940417.png)



![image-20240726162023291](D:\Work_APP\Typora\assets\image-20240726162023291.png)

# 2024.7.27，利用alphapose获取二维点，并结合相机参数进行三角测量，还是为了确定怎么在480个帧中获取801个关键点

1.根据光流法进行视频帧的扩展。

2.利用Alphapose推理出带有自信度的二维关键点，要注意点的可见性

3.利用

2024.7.30遇到的一些代码上的疑问

![image-20240730101223201](D:\Work_APP\Typora\assets\image-20240730101223201.png)

目标是将json文件转化成pkl文件

![image-20240730101254685](D:\Work_APP\Typora\assets\image-20240730101254685.png)

![image-20240730101335060](D:\Work_APP\Typora\assets\image-20240730101335060.png)

如果A处没有被注释，代码进入循环之前没有使得keypoint_single初始化导致squats1中的所有元素都是一样的，这是为啥？



![image-20240730101529902](D:\Work_APP\Typora\assets\image-20240730101529902.png)

如果A处注释使用B处，则以下变量元素正常，这是为啥？

![image-20240730101720867](D:\Work_APP\Typora\assets\image-20240730101720867.png)

这边解释一下，理解的正确性还有待考证：

首先，使用A处的代码相当于只创建了一个keypoint_single一个变量，这个变量会随着循环的进行获取json_data中的数据而进行更改，所以第n次进入循环相当于把当前次的数据复制了n次装进squats1变量中



# 7.31尝试解决squats视频同步问题

```

from utilsTRC import *
 aa = trc_2_dict('Data/subject2_test/MarkerData/OpenPose_default/PreAugmentation/squats1.trc', rotation={'y': 90, 'z': 180})
RHip = aa['markers']['RHip']
LHip = aa['markers']['LHip']
MHip = (RHip + LHip)/2

mocap_1 = trc_2_dict('Data/subject2_test/MarkerData/Mocap/squats1.trc')
R_HJC = mocap_1['marker_names']['R_HJC']
L_HJC = mocap_1['marker_names']['L_HJC']
import yaml
with open('Data/subject2_test/mocapToVideoTransform.yaml', 'r') as file:
    TF = yaml.safe_load(file)
# 假设已知的光学动捕数据点
M_HJC = (R_HJC + L_HJC) / 2

# 提取旋转矩阵和平移向量
R_fromMocap_toVideo = np.array(TF['R_fromMocap_toVideo'])
position_fromMocapOrigin_toVideoOrigin = np.array(TF['position_fromMocapOrigin_toVideoOrigin'])

# 定义转换函数
def transform_mocap_to_video(mocap_points, R, T):
    return np.dot(mocap_points, R.T) + T


# 转换到视频坐标系
M_HJC_TF = transform_mocap_to_video(M_HJC, R_fromMocap_toVideo, position_fromMocapOrigin_toVideoOrigin)

print("Transformed motion capture points in video coordinate system:")
print(M_HJC_TF)
```





# 2024.8.3使用wholebady去炼丹，先学习human3.6m的数据集的读取方式

```
In [7]: path = ['E:/test/wholebody3d-main/datasets/json/2Dto3D_test_2d.json', 'E:/test/wholebody3d-main/datasets/json/2Dto3D_train.json','E:/test/wholebody3d-main/datasets/json/2Dto3D_train_part1.json','E:/test/wholebody3d-main/datasets/json/2Dto3D_train_part2.json',]
In [8]: path = ['E:/test/wholebody3d-main/datasets/json/2Dto3D_test_2d.json', 'E:/test/wholebody3d-main/datasets/json/2Dto3D_train.json','E:/test/wholebody3d-main/datasets/json/2Dto3D_train_part1.json','E:/test/wholebody3d-main/datasets/json/2Dto3D_train_part2.json',
'E:/test/wholebody3d-main/datasets/json/2Dto3D_train_part3.json','E:/test/wholebody3d-main/datasets/json/2Dto3D_train_part4.json',
'E:/test/wholebody3d-main/datasets/json/I2Dto3D_test_2d.json','E:/test/wholebody3d-main/datasets/json/RGBto3D_test_img.json','E:/test/wholebody3d-main/datasets/json/RGBto3D_train.json']
   ...: 
   ...: 
   ...: 
   ...: 
In [9]: import numpy as np
In [10]: np.save('H3WB_datasets.npy',path)
path_np = "F:/data/H3WB/h3wb_train.npz"
check_data = np.load(path_np, allow_pickle=True)
metadata = check_data['metadata'].item()
train_data = check_data['train_data'].item()
predict_data = json.load(open(path[0]))
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-15-5e40c4422b5d>", line 1, in <module>
    predict_data = json.load(open(path[0]))
NameError: name 'json' is not defined
import json
predict_data = json.load(open(path[0]))
predict_data = json.load(open(path[1]))
predict_data = json.load(open(path[2]))
predict_data = json.load(open(path[7]))
predict_data = json.load(open(path[0]))
predict_data = json.load(open(path[1]))
predict_data = json.load(open(path[2]))
predict_data = json.load(open(path[3]))
predict_data = json.load(open(path[6]))
predict_data = json.load(open(path[7]))
predict_data = json.load(open(path[8]))
Process finished with exit code 0

```

# 2024.8.29对H3WB的边界框进行处理，让它与contextpose处理出来的bboxes-Human36M-GT.npy里面的格式差不多

问题一



2024.8.30

```
from collections import Counter
target_elements = ['S1', 'S5', 'S6', 'S7', 'S8']
element_count = Counter()
for dd in images_part:
    element = dd[0]
    if element in target_elements:
        element_count[element] += 1
```



# 2024.10.17 无人扶我青云志，我自踏雪至山巅

![image-20241017154431777](D:\Work_APP\Typora\assets\image-20241017154431777.png)

可视化S1,Direction1，80





学习一下MPII关键点处理的代码

目的是理解一下边界框裁剪过程，以及整个二维关键点处理流程，比如说

```python
import json
file_name = 'F:\data\MPII\data\mpii_test.json'
with open(file_name) as anno_file:
    anno = json.load(anno_file)
gt_db = []
for a in anno:
        image_name = a['image']
		# mpii标注中的center和scale是指：
		# H * W的原图像中，bbox的框原来应该是四个坐标确定，这里是用center和scale两个值来表示
		# bbox的center即为center， 而bbox在mpii中默认是正方形，边长（宽） = scale * 200，这个200是官方定的
        
        c = np.array(a['center'], dtype=np.float32)
        s = np.array([a['scale'], a['scale']], dtype=np.float32)

        # Adjust center/scale slightly to avoid cropping limbs
        # 因为mpii直接默认bbox为正方形，因此可能真正的bbox是矩形，调成正方形后可能会把人体某些部分给裁掉，所以直接把正方形扩大
        if c[0] != -1:
            c[1] = c[1] + 15 * s[1]
            s = s * 1.25

        # MPII uses matlab format, index is based 1,
        # we should first convert to 0-based index
        c = c - 1
		
		# 用到的都只有前两维
        joints_3d = np.zeros((16, 3), dtype=np.float32)
        joints_3d_vis = np.zeros((16,  3), dtype=np.float32)
        
        joints = np.array(a['joints'])
        joints[:, 0:2] = joints[:, 0:2] - 1
        joints_vis = np.array(a['joints_vis'])
        assert len(joints) == 16, \
        'joint num diff: {} vs {}'.format(len(joints),
                                          16)

        joints_3d[:, 0:2] = joints[:, 0:2]
        joints_3d_vis[:, 0] = joints_vis[:]
        joints_3d_vis[:, 1] = joints_vis[:]
upper_body_ids = [7,8,9,10,11,12,13,14,15]
lower_joints = []
upper_joints = []
for joint_id in range(16):
    if joints_3d_vis[joint_id][0] > 0:  # 这些关节必须都是可见的
        if joint_id in upper_body_ids:
            upper_joints.append(joints[joint_id])
        else:
            lower_joints.append(joints[joint_id])
if np.random.randn() < 0.5 and len(upper_joints) > 2:
    selected_joints = upper_joints
else:
    selected_joints = lower_joints \
        if len(lower_joints) > 2 else upper_joints
        
# 这一部分对上半身或者下半身的边界进行随机裁剪
selected_joints = np.array(selected_joints, dtype=np.float32)


left_top = np.amin(selected_joints, axis=0)
right_bottom = np.amax(selected_joints, axis=0)
w = right_bottom[0] - left_top[0]
h = right_bottom[1] - left_top[1]

# 保证是正方形
if w > 1 * h:
    h = w * 1.0 / 1
elif w < 1 * h:
    w = h * 1
scale = np.array(
    [
        w * 1.0 / 200,
        h * 1.0 / 200
    ],
    dtype=np.float32
)
# 适当放大，避免裁剪到人
scale = scale * 1.5
```

# 绘制人体边界框

```
import matplotlib.pyplot as plt
import matplotlib.patches as patches#绘制集合图形的
import numpy as np
from PIL import Image

# 假设你有一个图片
image_path = 'E:\\test\main_project\ContextPose-PyTorch-release-master\data\MPII\\039361034.jpg'
img = Image.open(image_path)

# 目标检测框的坐标，格式为 [x, y, width, height],x,y 左下角顶点坐标！！！！
# 表示方法 2  [x1, y1, x2, y2] 左上角的坐标（x1,y1）以及右下角的坐标（x2,y2）
# 例如，这里有两个框的坐标：
boxes = np.array([
    [50, 30, 100, 150],   # 第一个框 (x, y, width, height)
    [200, 100, 80, 120],  # 第二个框 (x, y, width, height)
])
bbox = [center[0], center[1], scale[0]*200,scale[0]*200]

# 绘制图像 有带坐标走的
fig, ax = plt.subplots(1)
ax.imshow(img)

# 绘制检测框(多个边界框)
for box in boxes:
    x, y, width, height = box
    rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')
    ax.add_patch(rect)

# 显示结果
plt.show()



# [426.1111,273.    ]
```

![image-20241018151448891](D:\Work_APP\Typora\assets\image-20241018151448891.png)

调整之后的

![image-20241018154917645](D:\Work_APP\Typora\assets\image-20241018154917645.png)

rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')！！！！这个x,y坐标是左下角顶点坐标，修改之后最终的上半身人体的裁剪结果如下图所示。

![image-20241018162441733](D:\Work_APP\Typora\assets\image-20241018162441733.png)

把图片标签中的center和scale转化成边界框的表示形式。

```
#center，scale→(x,y,w,h)
H = scale * 200.0
x = center[1] - H/2
y = center[0] - H/2
```

getitem函数的学习

```
import cv2
image_file = image_path # 'E:\\test\\main_project\\ContextPose-PyTorch-release-master\\data\\MPII\\039361034.jpg'
data_numpy = cv2.imread(
    image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION
)
# Cv2。IMREAD_IGNORE_ORIENTATION是OpenCV中cv2.imread ()函数的可选标志。这个标志告诉OpenCV忽略JPEG图像中的EXIF方向标签，该标签存储拍摄照片时相机的方向（例如，肖像或风景)

if self.color_rgb:
        data_numpy = cv2.cvtColor(data_numpy, cv2.COLOR_BGR2RGB)



cv2.imshow("image_w",data_numpy)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

![image-20241018163842984](D:\Work_APP\Typora\assets\image-20241018163842984.png)



使用Opencv读取的数据（没有经过BGR到RGB转化）直接采用PIL显示的效果

![image-20241018164559804](D:\Work_APP\Typora\assets\image-20241018164559804.png)

经过转化之后

![image-20241018164659342](D:\Work_APP\Typora\assets\image-20241018164659342.png)



```
D:\Work_APP\Anconda\envs\motionbert\python.exe "D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py" --mode=client --host=127.0.0.1 --port=61999 
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['E:\\test\\main_project\\ContextPose-PyTorch-release-master'])
Python 3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.31.1 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.31.1
Python 3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)] on win32
import json
import numpy as np
file_name = 'F:\data\MPII\data\mpii_train.json'
with open(file_name) as anno_file:
    anno = json.load(anno_file)
gt_db = []
for a in anno:
    image_name = a['image']
    # mpii标注中的center和scale是指：
    # H * W的原图像中，bbox的框原来应该是四个坐标确定，这里是用center和scale两个值来表示
    # bbox的center即为center， 而bbox在mpii中默认是正方形，边长（宽） = scale * 200，这个200是官方定的
    c = np.array(a['center'], dtype=np.float)
    s = np.array([a['scale'], a['scale']], dtype=np.float)
    # Adjust center/scale slightly to avoid cropping limbs
    # 因为mpii直接默认bbox为正方形，因此可能真正的bbox是矩形，调成正方形后可能会把人体某些部分给裁掉，所以直接把正方形扩大
    if c[0] != -1:
        c[1] = c[1] + 15 * s[1]
        s = s * 1.25
    # MPII uses matlab format, index is based 1,
    # we should first convert to 0-based index
    c = c - 1
    # 用到的都只有前两维
    joints_3d = np.zeros((16, 3), dtype=np.float)
    joints_3d_vis = np.zeros((16, 3), dtype=np.float)
    if self.image_set != 'test':
        joints = np.array(a['joints'])
        joints[:, 0:2] = joints[:, 0:2] - 1
        joints_vis = np.array(a['joints_vis'])
        assert len(joints) == 16, \
            'joint num diff: {} vs {}'.format(len(joints),
                                              16)
        joints_3d[:, 0:2] = joints[:, 0:2]
        joints_3d_vis[:, 0] = joints_vis[:]
        joints_3d_vis[:, 1] = joints_vis[:]
        
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  start_new_thread = thread.start_new_thread
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:22: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from _pydev_bundle import fix_getpass
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-4-d2e4db631b14>", line 23, in <module>
    if self.image_set != 'test':
NameError: name 'self' is not defined
for a in anno:
    image_name = a['image']
    c = np.array(a['center'], dtype=np.float)
    s = np.array([a['scale'], a['scale']], dtype=np.float)
    if c[0] != -1:
        c[1] = c[1] + 15 * s[1]
        s = s * 1.25
    c = c - 1
    joints_3d = np.zeros((16, 3), dtype=np.float)
    joints_3d_vis = np.zeros((16, 3), dtype=np.float)
    joints = np.array(a['joints'])
    joints[:, 0:2] = joints[:, 0:2] - 1
    joints_vis = np.array(a['joints_vis'])
    assert len(joints) == 16, \
        'joint num diff: {} vs {}'.format(len(joints),
                                          16)
    joints_3d[:, 0:2] = joints[:, 0:2]
    joints_3d_vis[:, 0] = joints_vis[:]
    joints_3d_vis[:, 1] = joints_vis[:]
    
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  '''
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from _pydev_bundle._pydev_getopt import gnu_getopt
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  
D:/Work_APP/PyCharm/Pycharm2023/PyCharm Community Edition 2023.1.4/plugins/python-ce/helpers/pydev/pydevconsole.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  try:
for a in anno:
    image_name = a['image']
    c = np.array(a['center'], dtype=np.float32)
    s = np.array([a['scale'], a['scale']], dtype=np.float32)
    if c[0] != -1:
        c[1] = c[1] + 15 * s[1]
        s = s * 1.25
    c = c - 1
    joints_3d = np.zeros((16, 3), dtype=np.float32)
    joints_3d_vis = np.zeros((16, 3), dtype=np.float32)
    joints = np.array(a['joints'])
    joints[:, 0:2] = joints[:, 0:2] - 1
    joints_vis = np.array(a['joints_vis'])
    assert len(joints) == 16, \
        'joint num diff: {} vs {}'.format(len(joints),
                                          16)
    joints_3d[:, 0:2] = joints[:, 0:2]
    joints_3d_vis[:, 0] = joints_vis[:]
    joints_3d_vis[:, 1] = joints_vis[:]
    
for joint_id in range(16):
    if joints_3d_vis[joint_id][0] > 0:  # 这些关节必须都是可见的
        if joint_id in upper_body_ids:
            upper_joints.append(joints[joint_id])
        else:
            lower_joints.append(joints[joint_id])
            
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-7-82902e0e5219>", line 3, in <module>
    if joint_id in upper_body_ids:
NameError: name 'upper_body_ids' is not defined
upper_body_ids = [7,8,9,10,11,12,13,14,15]
for joint_id in range(16):
    if joints_3d_vis[joint_id][0] > 0:  # 这些关节必须都是可见的
        if joint_id in upper_body_ids:
            upper_joints.append(joints[joint_id])
        else:
            lower_joints.append(joints[joint_id])
            
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-9-82902e0e5219>", line 6, in <module>
    lower_joints.append(joints[joint_id])
NameError: name 'lower_joints' is not defined
lower_joints = []
upper_joints = []
for joint_id in range(16):
    if joints_3d_vis[joint_id][0] > 0:  # 这些关节必须都是可见的
        if joint_id in upper_body_ids:
            upper_joints.append(joints[joint_id])
        else:
            lower_joints.append(joints[joint_id])
            
if np.random.randn() < 0.5 and len(upper_joints) > 2:
    selected_joints = upper_joints
else:
    selected_joints = lower_joints \
        if len(lower_joints) > 2 else upper_joints
    
selected_joints = np.array(selected_joints, dtype=np.float32)
center = selected_joints.mean(axis=0)[:2]
left_top = np.amin(selected_joints, axis=0)
right_bottom = np.amax(selected_joints, axis=0)
joints = np.array(a['joints'])
joints[:, 0:2] = joints[:, 0:2] - 1
joints_vis = np.array(a['joints_vis'])
joints_3d[:, 0:2] = joints[:, 0:2]
joints_3d_vis[:, 0] = joints_vis[:]
joints_3d_vis[:, 1] = joints_vis[:]
w = right_bottom[0] - left_top[0]
h = right_bottom[1] - left_top[1]
# 保证是正方形
if w > self.aspect_ratio * h:
    h = w * 1.0 / self.aspect_ratio
elif w < self.aspect_ratio * h:
    w = h * self.aspect_ratio
scale = np.array(
    [
        w * 1.0 / self.pixel_std,
        h * 1.0 / self.pixel_std
    ],
    dtype=np.float32
)
# 适当放大，避免裁剪到人
scale = scale * 1.5
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-14-37ee7bdb0ebc>", line 4, in <module>
    if w > self.aspect_ratio * h:
NameError: name 'self' is not defined
w = right_bottom[0] - left_top[0]
h = right_bottom[1] - left_top[1]
# 保证是正方形
if w > 1 * h:
    h = w * 1.0 / 1
elif w < 1 * h:
    w = h * 1
scale = np.array(
    [
        w * 1.0 / 200,
        h * 1.0 / 200
    ],
    dtype=np.float32
)
# 适当放大，避免裁剪到人
scale = scale * 1.5
bbox = [center[0], center[1], scale[0]*200,scale[0]*200]
import matplotlib.pyplot as plt
import matplotlib.patches as patches#绘制集合图形的
import numpy as np
from PIL import Image
# 假设你有一个图片
image_path = 'E:\\test\main_project\ContextPose-PyTorch-release-master\data\MPII\\039361034.jpg'
img = Image.open(image_path)
fig, ax = plt.subplots(1)
ax.imshow(img)
x, y, width, height = box
rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')
ax.add_patch(rect)
Backend MacOSX is interactive backend. Turning interactive mode on.
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-19-06ed1cec5f53>", line 4, in <module>
    x, y, width, height = box
NameError: name 'box' is not defined
fig, ax = plt.subplots(1)
ax.imshow(img)
x, y, width, height = bbox
rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')
ax.add_patch(rect)
Out[20]: <matplotlib.patches.Rectangle at 0x1276ec742c8>
fig, ax = plt.subplots(1)
ax.imshow(img)
x, y, width, height = bbox
rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')
ax.add_patch(rect)
Out[21]: <matplotlib.patches.Rectangle at 0x12777ac2408>
fig, ax = plt.subplots(1)
ax.imshow(img)
x, y, width, height = bbox
rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')
ax.add_patch(rect)
Out[22]: <matplotlib.patches.Rectangle at 0x12777b9d188>
draw = ImageDraw.Draw(image)
draw.ellipse((center[0] - 3, center[1] - 3, center[0] + 3, center[1] + 3), fill="red")
image.show()
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-23-17e117d01bcf>", line 1, in <module>
    draw = ImageDraw.Draw(image)
NameError: name 'ImageDraw' is not defined
from PIL import Image, ImageDraw
draw = ImageDraw.Draw(image)
draw.ellipse((center[0] - 3, center[1] - 3, center[0] + 3, center[1] + 3), fill="red")
image.show()
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-25-17e117d01bcf>", line 1, in <module>
    draw = ImageDraw.Draw(image)
NameError: name 'image' is not defined
draw = ImageDraw.Draw(img)
draw.ellipse((center[0] - 3, center[1] - 3, center[0] + 3, center[1] + 3), fill="red")
img.show()
fig, ax = plt.subplots(1)
ax.imshow(img)
x, y, width, height = bbox
x = x - height/2
y = y - height/2
rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')
ax.add_patch(rect)
Out[27]: <matplotlib.patches.Rectangle at 0x12700040ec8>
import cv2
image_file = image_path
data_numpy = cv2.imread(
    image_file
)
cv2.imshow(data_numpy)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-31-1c4452da1bbf>", line 1, in <module>
    cv2.imshow(data_numpy)
cv2.error: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'imshow'
> Overload resolution failed:
>  - imshow() missing required argument 'mat' (pos 2)
>  - imshow() missing required argument 'mat' (pos 2)
>  - imshow() missing required argument 'mat' (pos 2)
cv2.imshow("image_w",data_numpy)
cv2.waitKey(0)
cv2.destroyAllWindows()
cv2.imshow("image_w",data_numpy)
cv2.waitKey(0)
cv2.destroyAllWindows()
data_numpy = cv2.imread(
    image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION
)
cv2.imshow("image_w",data_numpy)
cv2.waitKey(0)
cv2.destroyAllWindows()
fig, ax = plt.subplots(2)
ax.imshow(data_numpy)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-36-d254173e52c7>", line 2, in <module>
    ax.imshow(data_numpy)
AttributeError: 'numpy.ndarray' object has no attribute 'imshow'
fig, ax = plt.subplots(1)
ax.imshow(data_numpy)
Out[37]: <matplotlib.image.AxesImage at 0x12700647508>
data_numpy_2= cv2.cvtColor(data_numpy, cv2.COLOR_BGR2RGB)
fig, ax = plt.subplots(1)
ax.imshow(data_numpy_2)
Out[39]: <matplotlib.image.AxesImage at 0x127006c90c8>
d = None
logger.error('=> fail to read {}'.format(image_file))
raise ValueError('Fail to read {}'.format(image_file))
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-41-5bc5aa8f3719>", line 1, in <module>
    logger.error('=> fail to read {}'.format(image_file))
NameError: name 'logger' is not defined
import logging
logger.error('=> fail to read {}'.format(image_file))
raise ValueError('Fail to read {}'.format(image_file))
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-43-5bc5aa8f3719>", line 1, in <module>
    logger.error('=> fail to read {}'.format(image_file))
NameError: name 'logger' is not defined
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)
logger.error('=> fail to read {}'.format(image_file))
raise ValueError('Fail to read {}'.format(image_file))
ERROR:__main__:=> fail to read E:\test\main_project\ContextPose-PyTorch-release-master\data\MPII\039361034.jpg
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-45-5bc5aa8f3719>", line 2, in <module>
    raise ValueError('Fail to read {}'.format(image_file))
ValueError: Fail to read E:\test\main_project\ContextPose-PyTorch-release-master\data\MPII\039361034.jpg
np.random.randn()
Out[46]: 0.45505891379492613
np.random.randn()
Out[47]: 0.04702918117029883
np.random.randn()
Out[48]: -0.61953925626949
a = np.arange(10)
a
Out[50]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
np.clip(a, 1, 8)
Out[51]: array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])
a
Out[52]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
np.clip(a, 3, 6, out=a) 
Out[53]: array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])
a
Out[54]: array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])
sf = 0.25
s = s * np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)
s
Out[57]: array([2.2804186, 2.2804186], dtype=float32)
s = s * np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)
s
Out[59]: array([2.8505232, 2.8505232], dtype=float32)
random.random()
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-60-1e16b7152bea>", line 1, in <module>
    random.random()
NameError: name 'random' is not defined
import random
random.random()
Out[62]: 0.338901017195686
data_numpy
Out[63]: 
array([[[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       ...,
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]]], dtype=uint8)
data_numpy_t = data_numpy[:, ::-1, :]
data_numpy_t
Out[65]: 
array([[[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       ...,
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]],
       [[16, 16, 16],
        [16, 16, 16],
        [16, 16, 16],
        ...,
        [16, 16, 16],
        [16, 16, 16],
        [16, 16, 16]]], dtype=uint8)
data_numpy_t.shape
Out[66]: (480, 720, 3)
data_numpy.shape
Out[67]: (480, 720, 3)
cv2.imshow("image_w",data_numpy_t)
cv2.waitKey(0)
cv2.destroyAllWindows()
jj[:,0] = w - joints[:,0] -1
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-69-ffbf8eb61cdb>", line 1, in <module>
    jj[:,0] = w - joints[:,0] -1
NameError: name 'jj' is not defined
jj = []
jj[:,0] = w - joints[:,0] -1
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-71-ffbf8eb61cdb>", line 1, in <module>
    jj[:,0] = w - joints[:,0] -1
TypeError: list indices must be integers or slices, not tuple
jj = np.arange(16,2)
jj
Out[73]: array([], dtype=int32)
jj[:,0] = w - joints[:,0] -1
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-74-ffbf8eb61cdb>", line 1, in <module>
    jj[:,0] = w - joints[:,0] -1
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
jj =  joints
jj[:,0] = w - jj[:,0] -1
affine_Marix = np.array([[-1,0,0],[0,1,0],[0,0,1]])
input = cv2.warpAffine(
    data_numpy,
    affine_Marix,
    (int(256), int(256)),
    flags=cv2.INTER_LINEAR)
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-78-0e27eb95b6b7>", line 5, in <module>
    flags=cv2.INTER_LINEAR)
cv2.error: OpenCV(4.9.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\imgwarp.cpp:2757: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 2 && M0.cols == 3 in function 'cv::warpAffine'
M = np.array([[-1, 0, 720], [0, 1, 0]], dtype=np.float32)
reflected_img = cv2.warpAffine(data_numpy, M, (720, 480))
cv2.imshow("image_w",reflected_img)
cv2.waitKey(0)
cv2.destroyAllWindows()
fig, ax = plt.subplots(1)
ax.imshow(reflected_img)
Out[81]: <matplotlib.image.AxesImage at 0x12701b9e808>
M_translation = np.array([[1, 0, 100], [0, 1, 0]])
reflected_img = cv2.warpAffine(data_numpy, M_translation, (720, 480))
Traceback (most recent call last):
  File "D:\Work_APP\Anconda\envs\motionbert\lib\site-packages\IPython\core\interactiveshell.py", line 3457, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-82-24f638e37fa1>", line 2, in <module>
    reflected_img = cv2.warpAffine(data_numpy, M_translation, (720, 480))
cv2.error: OpenCV(4.9.0) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\imgwarp.cpp:2757: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 2 && M0.cols == 3 in function 'cv::warpAffine'
M_translation = np.array([[1, 0, 100], [0, 1, 0]], dtype=np.float32)
reflected_img = cv2.warpAffine(data_numpy, M_translation, (720, 480))
fig, ax = plt.subplots(1)
ax.imshow(reflected_img)
Out[84]: <matplotlib.image.AxesImage at 0x1270227c308>
float(88)
Out[85]: 88.0
np.eye(3)
Out[86]: 
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
t_mat = np.eye(3)
t_mat[0, 2] = -256 / 2
t_mat[1, 2] = -256 / 2
t_mat
Out[89]: 
array([[   1.,    0., -128.],
       [   0.,    1., -128.],
       [   0.,    0.,    1.]])
t_inv = t_mat.copy()
t_inv[:2, 2] *= -1
t_inv
Out[92]: 
array([[  1.,   0., 128.],
       [  0.,   1., 128.],
       [  0.,   0.,   1.]])

```





![image-20241021162122165](D:\Work_APP\Typora\assets\image-20241021162122165.png)

相机参数没有很大误差（正确），将23个3维点进行投影之后的

![image-20241021223159446](D:\Work_APP\Typora\assets\image-20241021223159446.png)

这个是GT值投影出来的，也不是说非常完美吧？

![image-20241021223830809](D:\Work_APP\Typora\assets\image-20241021223830809.png)





# 2024.10.21 应用相机参数进行二维投影。

详细请参考context改版的vis.py

```
label_data = np.load('E:\\test\main_project\ContextPose-PyTorch-release-master\data\H3WB_data_process\H3WB-multiview-labels-GTbboxes.npy',allow_pickle=True).item()
keypoints_data = label_data['table'][10][3]
cameras = label_data['cameras']
for keypoint in keypoints_data:
```

# 2024.10.23将数据集加载的数据放入模型看输出

遇到两个问题：
1.原来只有17个点，有16个身体段，需要计算15个身体段的平均值和标准差，这个计算过程是怎么样的？我能利用的





COCO 转H36m

https://blog.csdn.net/jacke121/article/details/135833703?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-135833703-blog-103265652.235^v43^pc_blog_bottom_relevance_base7&spm=1001.2101.3001.4242.1&utm_relevant_index=4





# 学习多视图三维重建的一个博客

https://medium.com/@satya15july_11937/3d-image-reconstruction-from-multi-view-stereo-782e6912435b

NeRF(神经辐射)

局限性：适用于静态场景

计算复杂度：NeRF需要大量计算资源

推广到未见过的场景

<img src="D:\Work_APP\Typora\assets\image-20240727102223900.png" alt="image-20240727102223900" style="zoom:50%;" />





## 三维重建需要解决的问题？

令Xi为空间中的3D点集，P，P‘为将Xi投影到图像xi和xi’的相机对。点Xi和相机对根据图像对应关系（x,<->xi'）构成场景重建。

x：已知图像点

-通过视差匹配（多视角立体情况）

-通过使用特征匹配（运动结构的情况）

需要找到

1.投影矩阵（P和P‘）

2.3D点（X）

P和P’可从基本矩阵或基本矩阵方程中找到。X（3D点）可使用三角测量法找到

处理三个几何图形：

对应几何：给定第一个图像视图中的图像点x，这如何约束第二个视图中对应点x‘的位置。

相机几何：给定一组相应的图像{xi→xi’}，其中i=1...n，2个视频的相机P和P‘是什么。

场景几何：给定对应的图像点x<-->x'和相机p和p，X在空间中的位置

### 1.对应几何

这里的第一个任务是使用一下方法找到左图像和右图像之间的对应关系（xi ←→xi’）

- 视差图
- 运动结构，特征匹配
  

**多视图几何**
多视图几何是计算机视觉和数学的一个分支，研究场景中多个视图之间的几何关系。它涉及分析和处理来自多个摄像机或视点的图像，以提取有关场景的 3D 信息。

数学工具:对极几何、单应性。

1.对极几何：用于关联两个或多个图像的对应点

2.单应性：用于关联从不同角度捕获的同一场景中的点。

#### 对极几何

![image-20240727104511201](D:\Work_APP\Typora\assets\image-20240727104511201.png)

对极几何研究的是场景中两盒视图的几何关系，这两个视图由位于不同位置或方向的两个摄像头或传感器拍摄。堆积几何的基本概念是堆积约束，该概念规定3D点在两个图像上的投影位于一条称为对极线的线上。

#### 基本矩阵

![image-20240727104801799](D:\Work_APP\Typora\assets\image-20240727104801799.png)

- 基本矩阵和本质矩阵是对极几何的代数表示。
- 当相机已校准（例如，立体相机）时使用基本矩阵，当相机未校准（即运动结构）时使用本质矩阵（上右图）。
- 它告诉你如何利用公式从 xi 移动到 xi'
  ![image-20240727104857670](D:\Work_APP\Typora\assets\image-20240727104857670.png)

![image-20240727105909654](D:\Work_APP\Typora\assets\image-20240727105909654.png)

### 2.相机几何

基本矩阵在对应几何中找到找到的，现在的问题是如何从基本矩阵中找到P和P‘。有很多方法，查找P和P’的一种方法

![image-20240727110829641](D:\Work_APP\Typora\assets\image-20240727110829641.png)

### 3.场景几何

场景几何就是利用三角测量法找到X

#### 3.1三角剖分

![image-20240727110940987](D:\Work_APP\Typora\assets\image-20240727110940987.png)

三角测量是从多幅图像进行三维重建的关键步骤，因为它允许我们根据场景中的二维投影重建场景中三维形状。

3D重建中的三维测量是利用空间中的某个点在两幅或多幅图像上的投影来确定该点3D位置的过程。三角测量的基本原理是找到相机中心的视线与相应图像的交点。

三角测量可能无法提供空间的精确3D位置，取决于：

- 对应匹配：3D重建过程从对应匹配开始。如果对应匹配出现噪声，则整个过程将失败。
- 使用三角测量方法的类型：
  线性、非线性、重点

![image-20240727111313707](D:\Work_APP\Typora\assets\image-20240727111313707.png)











# 进行Opensim的学习（一）

## 1.逆运动学

逆向运动学（IK）工具逐步遍历实验谁的每个时间范围，并将模型定位在最匹配实验标记和该书剑步的坐标数据的姿态。该“最佳匹配”是使标记和/或坐标的加权平方误差纸盒最小化的姿态。从IK工具获得准确的结果对于使用静态优化、残差减少算法和计算肌肉控制等后续工具只管重要。

![image-20240430152852720](D:\Work_APP\Typora\assets\image-20240430152852720.png)

IK工具的输入和输出。实验数据以绿色显示，Opensim文件(.osim)显示未红色。设置文件未蓝色

## 输入：

IK的主要输入文件如下：

​	（1）subject01_simbody.osim:通过使用缩放工具或其他方式缩放通用模型而生成的特定于主题的Opensim模型，以及包含调整的虚拟标记的关联标记集。

​	 （2）subject01_walk.trc:从运动捕捉系统获得的实验的实验标记轨迹，以及感兴趣时间范围。

​      （3）gait2354_setuo_IK.xml:包含IK工具的所有设置信息的文件，包括标记权重（IK）人物。与在缩放工具中一样，标记权重是相对的，决定了虚拟标记跟踪实验标记的好坏程度（给定标记的权重越大，误差越小，即标记的虚拟表示和实验表示之间的距离）



**学习TF**

学习，假设输入x1，x2，通过一个Embedding层得到a1,a2。Wq，Wk，Wv是共享的。



<img src="D:\Work_APP\Typora\assets\image-20240420220216380.png" alt="image-20240420220216380" style="zoom:50%;" />

![image-20240420220417320](D:\Work_APP\Typora\assets\image-20240420220417320.png)

并行化处理，q全放一起，k全放一起，v全放一起

<img src="D:\Work_APP\Typora\assets\image-20240420220528121.png" alt="image-20240420220528121" style="zoom: 25%;" />



<img src="D:\Work_APP\Typora\assets\image-20240420220608770.png" alt="image-20240420220608770" style="zoom:50%;" />

将求出来的每一个Q和每一个K做一个match，d对应的

![image-20240420220645532](D:\Work_APP\Typora\assets\image-20240420220645532.png)

根据公式计算

![image-20240420221104163](D:\Work_APP\Typora\assets\image-20240420221104163.png)



![image-20240420221131515](D:\Work_APP\Typora\assets\image-20240420221131515.png)

矩阵乘法处理，并行化处理

<img src="D:\Work_APP\Typora\assets\image-20240420221145344.png" alt="image-20240420221145344" style="zoom:50%;" />

αhat是每一个v的权重大小

<img src="D:\Work_APP\Typora\assets\image-20240420221323861.png" alt="image-20240420221323861" style="zoom:50%;" />

同理能得到b2

<img src="D:\Work_APP\Typora\assets\image-20240420221416803.png" alt="image-20240420221416803" style="zoom:50%;" />

qkv进行操作

<img src="D:\Work_APP\Typora\assets\image-20240420221451679.png" alt="image-20240420221451679" style="zoom:33%;" />

抽象成一个模块

<img src="D:\Work_APP\Typora\assets\image-20240420221525794.png" alt="image-20240420221525794" style="zoom:50%;" />



multi-head

<img src="D:\Work_APP\Typora\assets\image-20240420221550698.png" alt="image-20240420221550698" style="zoom:50%;" />

2个head处理，将q1均分给每一个head，按照head个数均分

![image-20240420221619337](D:\Work_APP\Typora\assets\image-20240420221619337.png)



![image-20240420221702179](D:\Work_APP\Typora\assets\image-20240420221702179.png)

然后将第二个数字为1的数字全部归为head1

![image-20240420221749611](D:\Work_APP\Typora\assets\image-20240420221749611.png)

均分操作

![image-20240420221829078](D:\Work_APP\Typora\assets\image-20240420221829078.png)

同理

![image-20240420221856032](D:\Work_APP\Typora\assets\image-20240420221856032.png)

head2也是同理

![image-20240420221911653](D:\Work_APP\Typora\assets\image-20240420221911653.png)

对每个head执行selfattention操作

<img src="D:\Work_APP\Typora\assets\image-20240420221948208.png" alt="image-20240420221948208" style="zoom: 33%;" />

进行拼接

![image-20240420222033987](D:\Work_APP\Typora\assets\image-20240420222033987.png)



<img src="D:\Work_APP\Typora\assets\image-20240420222012905.png" alt="image-20240420222012905" style="zoom:50%;" />

WO进一步融合

![image-20240420222106403](D:\Work_APP\Typora\assets\image-20240420222106403.png)

![image-20240420222110666](D:\Work_APP\Typora\assets\image-20240420222110666.png)

![image-20240420222115667](D:\Work_APP\Typora\assets\image-20240420222115667.png)

![image-20240420222131141](D:\Work_APP\Typora\assets\image-20240420222131141.png)



和组卷积很像





位置编码，对a3和a2变化，不影响b1，说明有问题

![image-20240420222255148](D:\Work_APP\Typora\assets\image-20240420222255148.png)

![image-20240420222347333](D:\Work_APP\Typora\assets\image-20240420222347333.png)

结果都没太大差别，可训练的位置编码

# 学习VT（视觉transformer

1.Patch的嵌入，传入图片的大小，patch的大小，输入通道3，embed_dim=768（根据large模型会变化）

patch的数目14×14

<img src="D:\Work_APP\Typora\assets\image-20240420213514177.png" alt="image-20240420213514177" style="zoom:50%;" />



<img src="D:\Work_APP\Typora\assets\image-20240420213428816.png" alt="image-20240420213428816" style="zoom:50%;" />

qkv， 

![image-20240422143449862](D:\Work_APP\Typora\assets\image-20240422143449862.png)

VIT模型不能更改输入图片大小

![image-20240420213954763](D:\Work_APP\Typora\assets\image-20240420213954763.png)

2.多头注意力机制

<img src="D:\Work_APP\Typora\assets\image-20240420213354976.png" alt="image-20240420213354976" style="zoom:50%;" />

head_dim = dim//num_heads

<img src="D:\Work_APP\Typora\assets\image-20240420214348989.png" alt="image-20240420214348989" style="zoom:50%;" />

self.scale = qk_scale or head_dim ** -0.5,对应的理论就是一下

![image-20240420214447532](D:\Work_APP\Typora\assets\image-20240420214447532.png)



# Datesets 和 DataLoaders获取

Datesets：单个训练样本。读取训练数据，特征和标签。
DataLoaders: 多个样本minbatch形式，对每个周期进行数据集打乱，固定保存等 。

![image-20240409162016746](D:\Work_APP\Typora\assets\image-20240409162016746.png)

getitem就是根据指定索引返回当前索引对应的训练样本和label

紧接着是DataLoaders，希望训练的是一个minibatches，打乱，multiprocessing：数据读取不会影响GPU的训练的延迟。

%%%%test通常不需要shuffle，因为test过程中只进行一个前向计算，不参与梯度更新

![image-20240409163253858](D:\Work_APP\Typora\assets\image-20240409163253858.png)

num_workers--取决于cpu数

![image-20240409163520378](D:\Work_APP\Typora\assets\image-20240409163520378.png)

!!!重点 collate_fn，对Sampler采样的一个批次进行再处理，比如对一个batch中的几个样本进行padding，输入输出都是一个batch，对应于dataset中的transform--对单个样本进行预处理，这边是**对多个样本（一个batch）进行预处理**，比如进行归一化。

一旦把数据放进Dataloader中就可以对数据进行遍历，每次遍历，都会得到batch_size个的特征和标签，

**第一行使用iter将train_dataloader变成一个迭代器，然后通过next从迭代器中去生成一个一个批次。**



![image-20240409164244063](D:\Work_APP\Typora\assets\image-20240409164244063.png)

或者跟项目中的方法一样通过enumerate生成迭代器

![image-20240409164935122](D:\Work_APP\Typora\assets\image-20240409164935122.png)



下面是输出

![image-20240409164712879](D:\Work_APP\Typora\assets\image-20240409164712879.png)

Sampler的高级用法：在训练过程中对样本进行一个排序，对相同长度的样本放在同一个minbatch中，这样不会导致，有的样本为1有的样本长度为100，这样就不会导致一个浪费。

![image-20240409165514590](D:\Work_APP\Typora\assets\image-20240409165514590.png)

**下面是流式的data（不常用），上面的常用**

1.初始化函数

![image-20240409171156274](D:\Work_APP\Typora\assets\image-20240409171156274.png)

Sampler(样本级别)、batch_sampler（batch）是可以自定义的一种采样方法

2.判断是否设置了shuffle

![image-20240409171410427](D:\Work_APP\Typora\assets\image-20240409171410427.png)

互斥的，shuffle和sampler

3.bacth_sa

![image-20240409171513296](D:\Work_APP\Typora\assets\image-20240409171513296.png)

batch_sampler已经告诉了pytorch你想用多大的batch_size，想啥方式变成minbatch



![image-20240409171714368](D:\Work_APP\Typora\assets\image-20240409171714368.png)

返回一个有序的索引

![image-20240413093650049](D:\Work_APP\Typora\assets\image-20240413093650049.png)

sampler以某种顺序从dataset中取元素。

4.batchsampler

![image-20240413093859158](D:\Work_APP\Typora\assets\image-20240413093859158.png)

yield→return

![image-20240413093940115](D:\Work_APP\Typora\assets\image-20240413093940115.png)

​																									这里的意思是 not drop last，最后一个不扔掉

batch_sampler的作用就是把dataset中以sampler的方式取出来的元素把它拼起来，拼成一个batch返回，返回它的索引index

![image-20240413094435416](D:\Work_APP\Typora\assets\image-20240413094435416.png)

​																									**_代表内部变量 相当于java里的private**

![image-20240413094443930](D:\Work_APP\Typora\assets\image-20240413094443930.png)

根据batch_sampler是否有判定是否auto_collation, default_collate.

默认collect啥也没干，自定义的collate_fn，要以batch作为输入，把feature，label取出来，先把然后把一个batch中所有feature中的最大长度算出来，然后把其他feature都填充到最大长度，最后再组合成一个batch返回。

主要做了三件事：一、构建sampler

​							二、构建batch_sampler

​							三、构建collect

get_iterator函数

两个逻辑，传递的self，而这边的self传递的就是dataloader的，得到一个dataloader iter类的一个对象

![image-20240413095310420](D:\Work_APP\Typora\assets\image-20240413095310420.png)

创建一个fetcher，可以从dataset去取，然后获取一个batch索引，得到数据，然后返回数据

![image-20240413095655300](D:\Work_APP\Typora\assets\image-20240413095655300.png)

![image-20240413095357147](D:\Work_APP\Typora\assets\image-20240413095357147.png)



这个函数一般在啥时候用？如果实现了上面的第一行这个函数的话，这个类的实例化对象，把它变成迭代器

![image-20240413095520938](D:\Work_APP\Typora\assets\image-20240413095520938.png)

index sampler

返回一个batchsampler采样器，而dataloader的长度就是基于批样本的长度算出来的

![image-20240413100057889](D:\Work_APP\Typora\assets\image-20240413100057889.png)

在哪里调用的？

![image-20240413100239475](D:\Work_APP\Typora\assets\image-20240413100239475.png)

![image-20240413100548712](D:\Work_APP\Typora\assets\image-20240413100548712.png)

为什么会有这个类，单线程处理数据继承了这个类的，而单线程处理数据中有个next_data是咋调用的还不清楚，next调用了基类data中的__next 的方法，next能调用多少次就是根据dataloader的长度来决定的

![image-20240413100621860](D:\Work_APP\Typora\assets\image-20240413100621860.png)

# One-hot类型

对分类任务而言：

分类标签一个重要的作用，就是要计算预测标签与真实标签之间的相似性，从而计算 loss 值。loss值越小，说明预测标签与真实标签之间越接近。
相似性其实就是两个标签之间的距离。如果按照 0 代表猫， 1 代表狗， 2 代表人这种表示方法，那么猫和狗之间距离为 1， 狗和人之间距离为 1， 而猫和人之间距离为 2。

这在参与损失计算的时候是完全不能接受的：互相独立的标签之间，竟然出现了不对等的情况。

独热编码：

<img src="D:\Work_APP\Typora\assets\image-20240413101857246.png" alt="image-20240413101857246" style="zoom:50%;" />

![image-20240413102507689](D:\Work_APP\Typora\assets\image-20240413102507689.png)

通过几何学可知，在三维坐标系下，[1, 0, 0]、[0, 1, 0]和[0, 0, 1]这三个向量是互相垂直的，也就是互相正交独立。他们之间距离相等，这就解决了上面说的独立的标签之间，表示方法不对等的情况。softmax 激活函数输出一系列概率值（**会把一系列得分转化成概率分布**），来说明本轮推理属于某一分类的概率是多少。假设某一轮推理得到的softmax得分如下，有70%的概率是猫，20%的概率是狗，10%的概率是人。

<img src="D:\Work_APP\Typora\assets\image-20240413102216141.png" alt="image-20240413102216141" style="zoom: 50%;" />





# 调参心得

https://blog.csdn.net/u014157632/article/details/99683301?spm=1001.2101.3001.6650.5&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-5-99683301-blog-126092992.235%5Ev43%5Epc_blog_bottom_relevance_base7&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-5-99683301-blog-126092992.235%5Ev43%5Epc_blog_bottom_relevance_base7&utm_relevant_index=8