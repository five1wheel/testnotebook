

# 课题总体思路

**一、输入选择：权衡用RGB和深度的优缺点？**
使用RGB相机和深度相机进行人体姿态估计都有各自的优缺点。下面是一些使用深度图进行人体姿态估计的优势：

1. **距离信息更准确**：深度相机提供了每个像素的距离信息，这可以帮助更准确地确定人体的位置和姿态。相比之下，RGB图像只提供颜色信息，缺乏深度信息。
2. **对遮挡更鲁棒**：由于深度图提供了距离信息，所以在人体部分被遮挡时，可以更容易地识别出人体的形状和姿势。在RGB图像中，由于缺乏深度信息，遮挡会导致对人体姿势的识别更加困难。
3. **减少背景干扰**：深度图通常会将背景与前景（如人体）分离得更清晰，这有助于减少背景干扰，使得人体姿态估计更加准确。
4. **更容易进行3D姿态估计**：深度图提供了三维空间中的信息，因此更容易进行三维人体姿态估计。相比之下，RGB图像中的姿势信息是二维的，需要额外的技术或假设来进行三维估计。

虽然使用深度图进行人体姿态估计具有上述优势，但也存在一些限制：

1. **设备成本**：深度相机通常比RGB相机更昂贵，因此在某些情况下，成本可能会成为限制因素。
2. **能见度限制**：深度相机对于透明或反射的表面的性能可能不佳。这可能会导致深度图的质量受到影响，从而影响姿态估计的准确性。
3. **分辨率限制**：某些深度相机的分辨率可能相对较低，这可能会限制其在一些情况下的准确性和精度。

综上所述，尽管使用深度图进行人体姿态估计具有一些优势，但在实际应用中需要权衡其优缺点，并根据特定情况选择最合适的解决方案。

**二、深度图为啥是2.5D?**

RGB+D 图像是一种结合了 RGB (红绿蓝) 彩色信息和 D (深度) 信息的图像。这种图像通过融合常规的三通道颜色图像和一通道深度信息，为物体的每个像素提供了颜色以及距离观察点的深度信息。

为什么说它是2.5D，而不是3D呢？这是因为尽管深度信息给出了每个像素点关于观察方向的距离信息，但这种信息只是从单一视角捕获的。在3D数据中，你可以获取一个对象的完整三维形状，可以从任意角度观察该对象而不丢失信息。但在2.5D图像中，深度信息仅限于从特定的视角看到的，它不包含物体背面或被遮挡部分的信息。

简单地说，2.5D 提供的是一个表面的深度地图，而不是一个完整的三维结构。它比2D图像多了深度信息，但并不等同于3D，因为它没有提供关于物体完整几何形状的信息，只是从一个视角提供了额外的深度维度。因此，我们称之为2.5D，意味着它比2D图像多了一些信息，但没有达到完全的3D描述。



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200307225832807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L015QXJyb3c=,size_16,color_FFFFFF,t_70)

（1）实时出姿态，看看点云的运算会不会影响到实时性

（2）和海康相机结合，粗糙一点没关系，先出个结果

（3）数据集用点云数据集+深度加RGB数据+h36m



**问题分析**

1.得到人体各个关键点位置

2.将他们按照顺序进行拼接

3.难点？

遮挡不容易检测到，匹配

4.Openpose的做法

COCO中的关键点

<img src="D:\Work_APP\Typora\assets\image-20240220085750524.png" alt="image-20240220085750524" style="zoom:50%;" />

**应用领域**

<img src="D:\Work_APP\Typora\assets\image-20240220085913679.png" alt="image-20240220085913679" style="zoom:50%;" />

<img src="D:\Work_APP\Typora\assets\image-20240220090207691.png" alt="image-20240220090207691" style="zoom:33%;" />



RGB+D，估计人体姿态，做一些下游任务，比如疾病检测，康复训练指导

**topdown方法**

<img src="D:\Work_APP\Typora\assets\image-20240220090555497.png" alt="image-20240220090555497" style="zoom:33%;" />

利用相对值进行预测，相对框的相对值哈。拼接的时候是按照一定的规则连接。

**存在的问题的：**

1.姿态估计做成啥样主要由人体检测所决定，能检测到效果估计也没有问题。（目标检测器需要比较好）

2.如果两人重叠，只检测到一个人，那肯定会丢失一个目标，（小目标）

3.计算效率有点低，如果一张图片中存在多个人，姿态估计就很慢

4.设计一种不依赖于人体框而是直接进行预测的。

先进行目标检测，检测到每一个人所在的位置，所以关键点连接不会出错

**举个栗子**

如何得到姿态估计结果？分几步走？

1.首先得到所有关键点位置

2.图中有多少个人，我们需要把属于同一个人的拼接在一起





<img src="D:\Work_APP\Typora\assets\image-20240220091714326.png" alt="image-20240220091714326" style="zoom:25%;" />

**挑战任务**

<img src="D:\Work_APP\Typora\assets\image-20240220092017668.png" alt="image-20240220092017668" style="zoom:25%;" />

已知关键点，怎么把这些关键点拼接起来。如上图拼接成两个人

总结：1.找到关键点（先利用那个pointTF的方式去进行分割，大概知道哪个部分是身体的哪个部分，可以利用已有的2D的去检测）2.将

一、human3.6m数据集中的TOF数据处理、RGB图像处理导出2D骨架模型。





首先将Directions 1.cdf的数据进行可视化，结果如下：

<img src="D:\Work_APP\Typora\assets\image-20240108202449854.png" alt="image-20240108202449854" style="zoom: 33%;" />

文件中的四个变量

![image-20240108202557628](D:\Work_APP\Typora\assets\image-20240108202557628.png)

存在的疑问是x3,x4（1,144,176,680）每个维度

问题解答：144*176是深度图片的大小，一共是1383帧，680代表不同的深度图，x3是保留背景的深度图，x4是去除背景的深度图。



随后，通过Alphapose、HRnet等导出2D关节点

<img src="D:\Work_APP\Typora\assets\image-20240109164801113.png" alt="image-20240109164801113" style="zoom:33%;" />

<img src="https://www.stubbornhuang.com/wp-content/uploads/2021/06/wp_editor_md_8b60abe0636eee8925dfa737317b832b.jpg" alt="姿态估计 – Halpe Full-Body136数据集骨骼关节keypoint标注对应-StubbornHuang Blog" style="zoom:33%;" />



在Alphapose的使用过程中发现coco 17关键点,Halpe *26关键点*和Halpe 136关键点，为了方便起见选择coco去进行训练。

将导出的17个关键点





二、进行RGB的投影（这个方法得出来的三维坐标有点奇怪）另寻他法

以下是 人体姿态估计及在康复训练情景交互中的应用[1]中的深度图生成点云阵后，与openpose结合生成三维关节点的过程

![image-20240109143835680](D:\Work_APP\Typora\assets\image-20240109143835680.png)



1)首先根据文献,对Kinect进行深度相机和彩色相机的内参标定。得到彩色相机的焦距为(fx_RGB,fy_RCB),中心点坐标为(Cx_RGB,Cy_RGB),内参矩阵为KRGB。深
度相机的焦距为(fsD,f,D),中心点坐标为(Cx_D,Cy_D),内
参矩阵为KD。
2)然后标定彩色相机和深度相机的变换关系,得到深度相机坐标系到彩色相机坐标系的旋转矩阵和平移向量分别为RD-RGB和tD-RGB。
3)根据深度相机的内参矩阵,可以将深度图像的二维坐标映射为深度相机坐标系下的三维坐标。设深度图像上的一点为(xp,yn),该点的深度值为depth(xp,yD),
则该点在深度相机坐标下的三维坐标(Xp,Yp,Zp)为:

![image-20240109145301406](D:\Work_APP\Typora\assets\image-20240109145301406.png)

4)结合式(1)将深度相机坐标下的三维坐标(XD,YD,ZD)转换成彩色相机坐标下的三维坐标(XRGB,YRGB,ZRGB)为:

![image-20240109145429863](D:\Work_APP\Typora\assets\image-20240109145429863.png)

5)进一步可得(XRGB,RGB,YRGB,ZRGB)在彩色图像坐标系下的坐标(xRGB,YRGB)为:

![image-20240109145809562](D:\Work_APP\Typora\assets\image-20240109145809562.png)

上述方案由于数据集通常缺少相机的信息。







### 模型改进

第一个用于baseline改进的模块看看这些模块怎么插入的

uniformer：

```python
class Uniformer(nn.Module):
    def __init__(self, cfg):
        depth = cfg.UNIFORMER.DEPTH
		num_classes = cfg.MOEL.NUM_CLASSES
        img_size = cfg.DATA.TRAIN_CROP_SIZE
        in_chans = cfg.DATA.INPUT_CHANNEL_NUM[0]
        embed_dim = cfg.UNIFORMER.EMBED_DIM
		head_dim = cfg.UNIFORMER.HEAD_DIM
        mlp_ratio = cfg.UNIFORMER.MLP_RATIO
        qkv_bias = cfg.UNIFORMER.QKV_BIAS
        qk_scale = cfg.UNIFORMER.QKV_SCALE
        representation_size = cfg.UNIFORMER.REPRESENTATION_SIZE
        drop_rate = cfg.UNIFORMER.DROPOUT_RATE
        attn_drop_rate = cfg.UNIFORMER.ATTENTION_DROPOUT_RATE
        drop_rate = cfg.UNIFORMER.ATTENTION_DROPOUT_RATE
        split = cfg.UNIFORMER.SPLIT
        std = cfg.UNIFORMER.STD
        self.use_checkpoint = cfg.MODEL.USE_CHECKPOINT
        self.checkpoint_num = cfg.MODEL.CHECKPOINT_NUM
        logger.info(f'Use checkpoint:{self.use_checkpoint}')
        logger.info(f'Checkpoint number:{self.checkpoint_num}')
        self.num_classes = num_classes
        self.num_features = self.embed_dim = emded_dim #num_feature for consistency with other models
		norm_layer = partial(nn.LayerNorm, eps=1e-6)
        self.patch_embed1 = SpeicalPatchEmbed(
            img_size=img_siz, patch_size = 4, in_chans=in_chans, embed_dim=embed_sim[1], std=std)
        self.patch_embed2 = PatchEmbed(
            img_size=img_size // 4,patch_size = 2, in_chans=embed_dim[0], embed_dim=embed_dim[1], std=std)
        self.patch_embed3 = PatchEmbed(
        img_size=img_size // 8, patch_size=2, in_chans=embed_dim[1], embed_dim=embed_dim[2], std=std)
        self.patch_embed4 = PatchEmbed(
            img_size=img_size // 16, patch_size=2, in_chans=embed_dim[2], embed_dim=embed_dim[3], std=std)
        self.pos_drop = nn.Dropout(p=drop_rate)
        
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depth))]#随深度衰减
        num_heads = [dim // head_dim for dim in embed_dim]
		self.blocks1 = nn.ModuleList([
            CBlock(
            		dim=embed_dim[0], num_heads=num_heads[0], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, 
        			drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
            for i in range(depth[0])])
        self.blocks2 = nn.ModuleList([
            CBlock(
                dim=embed_dim[1], num_heads=num_heads[1], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+depth[0]], norm_layer=norm_layer)
            for i in range(depth[1])])
		if split:
            self.blocks3 = nn.ModuleList([
                SplitSABlock(
                	dim=embed_dim[2], num_heads=num_heads[2], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate,attn_drop=attn_drop_rate, drop_path=dpr[i+depth[0]+depth[1]],norm_layer=norm_layer)
            for i in range(depth[2])])
         else:
            self.blocks3 = nn.ModuleList([
                SABlock(
                    dim=embed_dim[2], num_heads=num_heads[2], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+depth[0]+depth[1]], norm_layer=norm_layer)
                for i in range(depth[2])])
            self.blocks4 = nn.ModuleList([
                SABlock(
                    dim=embed_dim[3], num_heads=num_heads[3], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+depth[0]+depth[1]+depth[2]], 										norm_layer=norm_layer)
                for i in range(depth[3])])
         self.norm = bn_3d(embed_dim[-1])

```





## 研究现状

引自----基于RGB-D视频的三维人体姿态估计
（1）随着深度学习技术的发展，基于深度学习的算法也被广泛应用到人体姿态估计领域。然而诸如遮挡、训练数据不足、**深度歧义**仍然是该领域中的共性问题。

|  分类标准  |         类别1          |             类别2              |
| :--------: | :--------------------: | :----------------------------: |
| 关键点维度 | 二维（关键点像素坐标） | 三维（相机坐标系下的三维坐标） |

分类标准1中的困难：**训练数据样本不足的问题。单目最大的挑战深度歧义，多目摄像头可以有效解决深度歧义，成本昂贵，特殊的硬件要求和环境。近年随着硬件成本下降，深度相机越来越普及，一些方法通过同时利用RGB相机和深度，通过融合RGB和深度信息，可以有效地缓解单目三维姿态估计下的深度歧义问题。**

**二维现状：**

- 基于回归的方法：直接从图像或视频帧中估计对应的人体关键点坐标。DeepPose使用AlexNet通过级联的方式学习从图像到人体

  | DeepPose 深度学习，图→关键点                                 |
  | ------------------------------------------------------------ |
  | 1.Sun 等人[4]在 ResNet-50基础上引入结构感知，基于骨骼的姿态表示 |
  | 2.Luvizon 等人，端对端姿态回归，使用 soft-argmax将特征图转换为全微分框架下的**关节坐标** |
  | 3.Li 等人[7]提出了一个多任务框架，一、回归器预测 二、滑窗从图像中检测身体部位 |
  | 4.Fan 等人[8]提出了一个双流，第一个任务确定一个图像块是否包含身体关节，第二个任务估计图像中关节的坐标 |

- 基于人体部位检测的方法 





## 复现3d_Pose_baseline_pytorch

第一次跑还是误差比较大的，下面第一幅图是GT，第二幅图是预测的，我只能说是依托答辩，

<img src="D:\Work_APP\Typora\assets\image-20240328100837756.png" alt="image-20240328100837756" style="zoom: 80%;" />

![image-20240328101103935](D:\Work_APP\Typora\assets\image-20240328101103935.png)



初步分析可能是训练的问题 1.metadata.xml没用codev1.2版本的

2.没有按readme重新命名，应该不是主要影响。

3.再切片的时候那些相机矩阵是不是搞混了？有没有对应上

```python
    # cam_path = "E:\\test\\3d_pose_baseline_pytorch-master\data\h36m\cameras.h5"
    # data_path = "E:\\test\\3d_pose_baseline_pytorch-master\data"
    # model = LinearModel()
    # model = pickle.load(file=open("E:\\test\\3d_pose_baseline_pytorch-master\checkpoint\example\\archive\data.pkl", 'rb'))
    # rcams = load_cameras(cam_path)
    # actions = ["Directions",
    #            "Discussion",
    #            "Eating",
    #            "Greeting",
    #            "Phoning",
    #            "Photo",
    #            "Posing",
    #            "Purchases",
    #            "Sitting",
    #            "SittingDown",
    #            "Smoking",
    #            "Waiting",
    #            "Walking"]
    # test_loader = DataLoader(
    #     dataset=Human36M(actions=actions, data_path=data_path, use_hg=False, is_train=False),
    #     batch_size=64,
    #     shuffle=False,
    #     num_workers=0,
    #     pin_memory=True)
    # inpss = []
    # tarss = []
    # num = []
    # for i, (inps, tars) in enumerate(test_loader):
    #     inpss.append(inps)
    #     tarss.append(tars)
    #     num.append(i)
    # inps = inpss[0].cuda()
    # tars = tarss[0].cuda()
    # outputs_0 = model(inps)
    #
    # criterion = nn.MSELoss(size_average=True).cuda()
    # loss = criterion(outputs_0.cuda(), tars.cuda())
    # test_3d = torch.load(os.path.join(data_path, 'test_3d.pth.tar'))
    # test_2d = torch.load(os.path.join(data_path, 'test_2d.pth.tar'))
    # #load mean/std file
    # stat_3d = torch.load(os.path.join(data_path, 'stat_3d.pth.tar'))
    # stat_2d = torch.load(os.path.join(data_path, 'stat_2d.pth.tar'))
    # p2d = data_process.unNormalizeData(inps, stat_2d['data_mean_2d'], stat_2d['data_std_2d'], stat_2d['dim_to_use_2d'])
    # p3d = data_process.unNormalizeData(outputs_0.cpu().detach().numpy(), stat_3d['mean'], stat_3d['std'], stat_3d['dim_use'])
    # gt = data_process.unNormalizeData(tars.cpu().detach().numpy(), stat_3d['mean'], stat_3d['std'], stat_3d['dim_use'])
    # # load_model("E:\\test\\3d_pose_baseline_pytorch-master\checkpoint\example\\archive\data.pkl")
    #
    # R, T, f, c, k, p, name = rcams[(1,1)]
    # gt = gt.reshape(-1, 3)
    # p3d = p3d.reshape(-1, 3)
    #
    # #要进行坐标系变化，不然整个人都是躺着的
    # p3d = data_process.camera_to_world_frame(p3d, R, T)
    # gt = data_process.camera_to_world_frame(gt, R, T)
    # #为了能画
    # gt = gt.reshape(-1, 96)
    # p3d = p3d.reshape(-1, 96)
    #
    # import matplotlib.gridspec as gridspec
    #
    # gs1 = gridspec.GridSpec(1, 3)  # 5 rows, 9 columns
    # fig = plt.figure(figsize=(6.4, 3.2))
    # gs1 = gridspec.GridSpec(1, 3)  # 5 rows, 9 columns
    # gs1.update(wspace=-0.00, hspace=0.05)  # set the spacing between axes.
    # plt.axis('off')
    #
    # ax2 = plt.subplot(gs1[1], projection='3d')
    # gt = gt[0, :]
    # show3Dpose(gt, ax2)
    #
    # ax3 = plt.subplot(gs1[2], projection='3d')
    # p3d = p3d[0, :]
    # show3Dpose(p3d, ax3, lcolor="#9b59b6", rcolor="#2ecc71")
```

![image-20240401202440706](D:\Work_APP\Typora\assets\image-20240401202440706.png)