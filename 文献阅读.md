基于两流2D CNN的方法

顾名思义，双流2DCNN框架通常包含了两个2D CNN分支，它们采用了从RGB视频中提取的不同输入特征用于HAR，通常通过融合策略得到最终结果，如图1(a)所示。在本节中，我们将回顾经典的两流方法，以及它们的扩展。

作为一个景点的双流框架，S和Z提出了一个由空间网络和时间网络组成的双流CNN模型。更具体的说，给定一个视频，每个单独的RGB帧和基于多帧的光流分别被输入到空间流和时间流中，因此这两个流在HAR中学习了外观特征和运动特征。最后，将这两个流的分数进行融合，生成最终分类结果，在另一项经典工作中K等人将低分辨率RGB帧和高分辨率中心裁剪两个独立的溜来加快计算速度，研究了不同的融合策略来模拟视频中的时间动态。一些研究视图扩展和改进这些经典的双流CNN。

为了为HAR生成更好的视频表示，W将多尺度视频帧和光流输入双流CNN提取卷积特征图，然后在以提取轨迹中心的时空管道上采样。最后，使用Fisher向量表示和支持向量机对得到的特征进行聚合。C等人利用人体关节位置，从RGB图像和光流图像裁剪出多个人体部位，通过双流网络进行特征提取，然后使用SVM分类器进行HAR。

还有其他几个作品，扩展了双流框架，提取了HAR的长期视频级信息，W将每个视频分成三个片段，并使用双流网络对每个片段进行处理。然后利用平均赤化方法将三个片段分类分数进行融合，生成视频级预测。D不想那样融合片段分数，而是通过元素惩罚来聚合片段特诊，G基于双流框架从采样的外观和运动帧中提取特征，并使用动作词将特征聚合成单个视频级表示进行分类。F等人将外观残差特征门控调制特征级运动信息相乘，Z等人通过加入运动显著流，将53中的双流CNN扩展成三流CNN，以更好地捕捉显著运动信息，B等人从RGB序列和光流序列构建动态图像，通过55秩赤化来总结长期全局外观和运动，然后将动态图像与原始

 

 

单深度图像中局部人体姿态的实时识别

 

鲁棒性的交互式人体跟踪应用包括游戏，人机交互、安全、远程呈现，甚至医疗保健。最近实时深度相机引入大大简化了这项任务。然而，即使最好的现有系统，也存在局限性。尤其值得一提的是，在Kinect发布之前，没有任何一款游戏能够在消费者硬件以交互速率运行，同时处理各种形状、尺寸的各种身体动作，有些系统通过逐帧跟踪来实现高速运行，但很难快速重新初始化，因此不够鲁邦。在本文中，我们专注于部分的姿态识别，从单个深度图中检测出每个骨骼关节的一个小3D位置候选集合。我们对每帧初始化和回复关注。旨在补充任何适当的跟踪算法。这些算法可能进一步纳入时间运动学一致性，本文提出算法构成了Kinect游戏平台的核心组件。

如图1所示最近期物体是被

 

 

 

基于自适应点云采样策略的连续三维人体姿态估计

带着问题去读，

\1. 自适应体现在哪

\2. 点云

摘要：基于点云的人体姿态估计任然存在噪声和估计抖动伪影，这主要是由于手工采集点云和采用单帧估计策略所致。本文提出了一种基于点云序列的三维人体姿态估计方法。为了对输入有效点云进行采样，我们设计了一种基于密度引导注意力机制的可微分点云采样，为了避免遗忘三维人体姿态估计问题引起的抖动，我们采用时间信息来获得更稳定的结果。在ITOP数据集和NTURGBD数据集上的实验表明，我们所提供的所有组件都是有效的，我们的方法可以达到最先进性能。

对于三维姿态估计输入，深度图或点云通常更可取。首先，点云包含了人体的三维空间信息，可以正确估计人体姿态尺度。其次，点云在光强变化的环境下质量一般不变，这导致了更多潜在应用场景，如室内增强现实。最后，深度传感器广泛应用于手机平板电脑，这需要强大算法来利用这些深度传感器。

挑战：1.噪声点云2.遮挡、自遮挡造成模糊性3.单帧姿态估计上，缺乏时间平滑增强，目前的方法可能在连续点云序列上产生晃动伪影。

本文提出了一种基于点云序列的三维人体姿态估计方法。受单帧基于点云的框架启发，我们使用点云序列设计了一种新的两阶段人体姿态估计通道。点云采集模块用于自适应地旋转有效点云，可以为人体姿态估计提供信息。为解决噪声点云问题，我们观察到点云采样测量可以提高输入点云质量。因此，我们首先基于密度引导注意李机制估计点云采样中心，并利用这些中心点对姿态感知点云进行采样。为了解决伪影和遮挡问题，我们使用时间一致性来约束结果，从而生成准确的人体姿态结果

\1. 介绍，基于点云的三维人体姿态是今年来的一个脊柱研究领域，他可以应用于人机交互、运动重定向和虚拟化身控制等多个领域。对于三维人体姿态估计的输入，深度图或点云通常更可取。首先，点云包含了人体的三维空间信息，可以是估计的人体姿态尺度正确。其次，点云在环境。

三个创新：

（1）提出了一种基于密度引导和注意的可微点云采样方法。选择人体前景中的点云，为姿态估计提供更有效的点云输入。

（2）提出一种基于时间序列的电源三维人体姿态估计方法，比现在的获得更好更平滑的人体姿态估计结果。

（3）构建一个试试的人体捕获系统，实现了人体运动馆的平稳捕捉

\2. 相关工作

2.1人体姿态估计

回顾使用单一图像作为输入的相关最新方法，以及利用时间信息的集中代表性的3D人体姿态估计方法。

对于深度图估计的3D，res2net

（1）利用统计模型从深度图估计人体姿态

（2）将深度图视为点云，并将其转换为3D体素网格;然后使用3D CNN来估计人体的3D姿势。

（3）首先使用RGB图像对点云进行体素化，并使用类似res2net的网络

![img](D:\Work_APP\Typora\assets\wps1.jpg) 

（4）来估计人体的3D姿势。

（5）[Zhang et al.， 2020]使用2D/3D混合表示深度图和类生成方法。

 

视频的三维人体姿态估计方法可分为两大类

第一种是使用之后的时间信息，并使用它来平滑估计结果。

（1）提出了一种多级序列细化网络来估计3D人体姿态序列。

（2）使用全连接网络来细化粗输入位姿。

（3）使用时间一致的2D姿势来估计3D姿势序列

第二种类型使用时间信息并从序列中提取与时间相关的特征。

（1）设计了一个半监督的管道，从视频中学习3D人体动力学。

（2）提出了一种全卷积结构，利用时间卷积来估计视频中的3D人体姿势。

（3）首先估计了每帧的2D关节位置和SMPL模型参数，并使用束调整估计平滑结果

（4）注于从标记帧和未标记帧学习特征，以执行密集的时间位姿传播和估计。

（5）提出了一种针对多帧场景的人体姿态估计方法，他们利用视频帧之间的时间信息来促进关键点检测。

2.2基于点云的深度学习

基于点云的方法主要以点云作为输入，可以从输入点云坐标和其他信息如表面或者其他信息中提取。

（1）点云分割分类

（2）将点云学习方法用于目标检测任务

（3）提出了PointNet的端到端网络，该网络使用点坐标和表面法线作为输入，并使用多层感知器将它们映射到高维空间。

（4）作者进一步使用了一个分区采样模块，并将输出递归地交给该模块。另一方面，他们还提出在混合相机中使用2D信息来加速3D检测

（5）提出了一种新方法，在对基于体素的表示进行卷积时，使用点来表示点云

（6）提出了一种可微分点云采样方法，可以预测采样中心，并实现了最先进的性能

区别：（1）我们提出了第一个基于点云序列的三维人体姿态估计框架，并提出了姿态一致性损失来平滑姿态估计结果

（2）在采样阶段引入注意机制，提高了三维姿态估计任务的性能。

3.方法

姿估计方法大致可以分为两部分：基于密度的点云采样模块和顺序的三维人体位姿估计模块。

（1）提出一种利用密度引导注意机制的可微点云采样

（2）三维人体姿态估计模块中，我们从采样的点云序列中估计出三维人体姿态，对点采样网络和三维人体姿态估计网络进行端到端训练

3.1点云采样模块

原始深度图包含冗余和噪声的像素，增加了计算成本，降低了人体姿态估计的精度。如果将背景像素或噪声像素混合到点云中进行人体姿态估计，那么人体姿态估计往往会出现较大的估计误差。我们通过设计一种带有密度引导注意机制的新型可微采样策略来解决这个问题。

步骤：

\1. 首先，我们的目标是用输入点云P生成一组采样中心R

使原始点云P中R的邻近点在人体姿态估计任务中表现更好

\2. 在采样中心R、预测权值wpred和原始点云的引导下对点云进行采样

3.2采样中心生成

期望输入点云中采样中心的邻近点云位于人体前景中

考虑了采样中心点与其邻域之间的关系

一种密度引导的注意机制，自适应生成采样结果

核心点往往是人的表面内部的点，边界点往往属于人的边界，那些既不是核心点也不是边界点的点通常不那么重要或有噪声，三维空间中是稀疏的，包含的人体姿态感知信息很少。

定义：r（核心点）距离和周围点数量

### **The TF operators are included under  `tf_ops`, you need to compile them first. Update  `nvcc`  and  `python`  path if necessary. The code is tested under TF1.15. If you are using earlier version it's possible that you need to remove the  `-D_GLIBCXX_USE_CXX11_ABI=0`  flag in g++ command(check `tf_xxx_compile.sh` under each ops subfolder) in order to compile correctly.**

 

 

 

# ***\*Wearable Motion Capture: Reconstructingand Predicting 3D Human Poses From\**** ***\*Wearable Sensors\****

可穿戴运动捕捉，从可穿戴传感器重建和预测3D人体姿态

 

摘要：

在无约束测量环境中重建和预测三维人体行走趋势，通过评估治疗后的进展并未辅助设备控制提供信息，有潜力用于运动残疾换则的健康检测系统。最新的姿态估计算法利用运动捕捉系统，从IMU传感器和第三人称视角相机捕捉数据。然而仅对门诊病人而言，第三人称视角并不总是可行的。因此我们提出来可穿戴式运动捕捉，即通过可穿戴IMU和穿戴摄像机重建和预测三维人体姿态，从而帮助临床医生对患者进行诊断。为了解决这一问题，引入新的面向注意力的循环神经网络，包含一个基于传感器的面向注意力的循环编码器，重建随时变化的三维人体姿态，预测一下时间步长的三维人体姿态

评估方法：使用可穿戴imu和可穿戴视频摄像机收集了一个新的可穿戴运动捕捉数据集，以及肌肉骨骼关节高度基准针织。提出A在新的下肢可穿戴运动捕捉数据集上显示出较高精度，而且它在两个公共全身姿态数据集上优于最先进的方法

介绍：运动障碍的人在行走时面临多种不利条件，如下背部的压力增加，代谢成本增加，不太不对称。适当检测行走的进展可以减轻这些缺点，并通过频繁评估即使随访治疗，预防继发问题，如关节炎，跌倒风险和血管疾病。目前的检测程序尽在临床现场可用。然而由于缺乏可行技术，对门诊出院后的治疗过程进行监控具有极大挑战性。因此有必要对门诊外的行走姿势进行评估，这不仅可以防止不必要的就诊，大大节省医疗开支，而且可以使患者在定期就诊之间即使得到适当治疗。

挑战：最常见的是，运动捕捉系统被用于实现对人体姿态的高精确理解，但大量的可穿戴标记和额外的运动捕捉相机在实验室中设置，使这种方法无约束的日常环境中不可行。

一些论文专注于从第三人称视角RGB或RGD-d相机中重建人体姿态。然而，这些基于第三人称视角的方法并不总是适用于门诊患者。因此，仅通过可穿戴传感器就需要对人体姿态进行重建和预测，使出院患者能够在日常生活中自由行走。

一些工作依赖于大量的IMU传感器，来获得精确的人体姿态重建，但佩戴大量传感器在日常生活中使用非常不舒服，和不切实际。最近一些作品使用了一组简化的IMU传感器来进行人体姿态重建。然而，稀疏惯性传感器的动作捕捉具有固有的模糊性和挑战性。

研究问题：上述挑战导致了一个研究问题：当出院患者在日常生活中行走时，如何设计一种可行有效的方法，通过一套小新的可穿戴传感器准确感知他们的姿态。因此，临床医生可以在诊所外访问或者的行走功能，研究人员可以设计智能假肢设备，以帮助门诊患者进行实时最佳控制。 

​	我们的贡献，如图1所示，提出了两个任务来处理可穿戴运动捕捉问题，1.随着时间的推移，重建3D人体互换姿态，用于临床诊断，预测一下时间不长的3D人体姿态，以时间实时辅助设备控制。

​	我们的方法说明，（a）我们的实验设置，（b）我们的目标，（c）提出的注意力党项循环神经网络的输入和输出。需要注意的是，之前的工作需要从第三人称视角获取视觉数据来进行姿态重建，在这，门诊患者并不总是可能时间的，与此相反，我们提出了在日常生活环境中重建和预测新购的随身相机和IMU传感器解决方案。

​	目前还没有包含可穿戴IMU和可穿戴相机数据的公共数据集，以及3D行走姿态基准真值，为了开发可穿戴运动捕捉算法，我们手机了10个不同不行速度的受试者在不同步行条件下的数据集，跑步机上，在地面上再留意上。虽然我们的合作研究项目针对下肢截肢患者，但我们的可穿戴运动捕捉也可以用于重建和预测上肢和全身姿态。因此，我们也比较了我们的可穿戴运动捕捉方法在两个相关的全身姿态数据集上的性能（DIP-IMU）包含10名受试者的IMU数据和TotalCapture包含了5名受试者的IMU数据和来自第三人称视角摄像机的视频。

贡献：

\1. 从可穿戴IMU传感器和可穿戴摄像机重建和预测三维人体姿态，

\2. 提出了一种新颖的注意力导向循环神经网络

\3. 引入了一个新的数据集，包含来自可穿戴IMU和可穿戴相机传感器的数据与3D人体姿态基准真值

基于IMU的人体动作捕捉:可穿戴imu在捕捉人体动作方面表现出显著稳定性和准确性

之前R引入IMU传感器

两个时间

 

![img](D:\Work_APP\Typora\assets\wps2.jpg) 

 

我们使用采集的下肢可穿戴IMU+相机数据集，可穿戴的动捕

 

#  

# ***\*Markerless vs. Marker-Based Gait Analysis: A Proof of Concept Study\****

 

无标记与基于标记的步态分析，一项概念证明研究

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

MotionBERT:学习人体动作表征的统一视角

一个统一视角来处理各种认为中心的视频任务。通过学习人体运动表示从大规模和异构的数据资源具体地说，我们提出了一个预训练阶段，其中一个运动编码器被训练从有噪声的部分2D观测中回复潜在的3D运动。通过这种方式获得的运动表示包含了关于人体运动的集合、运动学和物理知识，可以很容易地转移到多个下游任务。我们用一个双流时空转换器神经网络来是先运动编码器。该算法能够全面、自适应地捕捉骨骼关节之间的长时间空间关系，在从头开始训练时，其三维姿态估计误差最低。此外，我们提出的框架通过简单地微调带有简单回归的与旋律运动编码器，在所有三个下游任务上实现了最先进的性能。这表明了学习的运动表示的多功能性。

感知和理解人类活动一直是机器智能的核心追求。为此，研究人员定义了各种任务来从视频中估计以人为中心的寓意标签，例如骨架关节点、动作类和表面网络。虽然在这些任务重每一项都取得了重大进展，但他们往往是鼓励地建模而不是作为互相关联的问题。例如，时空卷积网络，在三维姿态估计、动作识别中被应用于人体关节时空关系建模，但它们之间的练习尚未得到充分探索。直觉上，这些模型都应该学会识别典型的人体运动模式，尽管他们针对不同的问题设计的。然而，当前的方法未能挖掘出利用这些任务之间的共性。理想情况下，我们可以开发一个统一以人为中心的视频表示，可以在所有相关人物之间共享。

开发这种表示的一个重大挑战是可用数据资源的异构性。动捕系统，提供了通过标记和传感器获得高保真的3D运动数据，但捕获的视频外观通常局限于简单的室内场景。动作识别数据集提供了动作寓意的朱姐，但他们要么不包含人体姿态标签，要么包含日常活动的受限动作。相比之下，在野外拍摄的人类视频提供了丰富多样的外观和动作。然而，获取精确的2D姿态标注需要相当大的工作量，而获取真值3D关节位置几乎是不可能的，因此现有的研究大多集中在使用单一类型的人体运动数据进行特定人物，无法享受其他数据资源的优势。

在这些工作中，我们为学习人体运动标准提供了一个新的视角。其关键思想是我们可以统一地从异构的数据资源中学习一种通用的人体运动表示，并利用该表示统一地处理不同的下游任务。我们提供了一个两阶段的框架，由与训练和微调组成，如图1所示，在训练前阶段，我们从不同的运动数据源中提取二维骨骼序列，并使用随机的遮罩和噪声破坏他们，然后，我们通过训练运动编码器从损坏的2D骨架中回复3D运动。这个挑战性的任务本质要求，从他的时间运动推断出潜在的3D人体结构，回复错误和确实的观测结果。通过这种方式，运动编码器隐含地捕捉人体运动常识，如关节连接，解剖约束和时间动力学。在实践中，我们提出了双流时空转换器作为运动编码器来捕捉骨架关键点之间的远程关系。我们假设，从大规模和多样化的数据资源中学习到的运动表示可以在不同的下游任务之间共享，从而有利于他们的可能性。因此，对于每个下游任务，我们使用任务特定的训练数据和带有简单回归头的监督信号来适应预先训练的运动表征。

综上所述，本研究的贡献在三个方面。1通过一个共享的人体运动表征学习框架，为解决各种以人为中心的视频任务提供一个新的视角。

2.我们提出了一种预训练方法来利用大规模而异质性的人体运动资源，学习可泛华的人体运动表征，我们的方法可以同时利用3D动作捕捉数据的精度和在户外RGB视频的多样性。

3.我们设计了一个具有级联时空自注意快的双流变压器网络可以作为人体运动建模的通用骨架。

实验表明上述设计能够实现多功能的人体运动表示，可传递到多个下游任务，优于任务特定的最先进方法。

基于骨骼的动作识别，

前人的工作只粗了动作识别与人体姿态估计之间的内在联系，对于人体关节时空关系的建模，比以往采用LSTM和GCN。最近，PoseConv3D提出将3D-CNN应用到堆叠的2D接缝热图上，得到改进

人体补网。基于SMPL等参数话人体模型，许多研究工作集中于从单个图像回归人体网络。SPIN额外地结合了适合的主体。

 

![img](D:\Work_APP\Typora\assets\wps3.jpg) 

时空转换器，DSTformer。

 

 

 

方法介绍：正如第一节所讨论的，我们的方法包括两个阶段，即统一的前训练和特定的微调。在第一阶段，我们训练一个运动编码器来完成2D到3D的提升任务，其中我们使用提议的DSTFORMER作为骨架。

第二阶段，我们微调了预训练的运动编码器和一些新的层下游的任务，我们使用2D 骨序列作为预训练和微调输入，因为他们可以可靠地从各种运动源中提取，并且对变化更鲁邦。现有研究表明，在不同下游任务中使用2D骨架序列是有效的。我们将首先介绍DSTformer作为骨干的体系结构，然后详细描述训练方案。

3.2网络架构。

 

 

 

问题：

1.

![img](D:\Work_APP\Typora\assets\wps4.jpg) 

![img](D:\Work_APP\Typora\assets\wps5.jpg) 

这边的特征点是啥？

\3. 这边正则化是啥目的是啥？

![img](D:\Work_APP\Typora\assets\wps6.jpg) 

\4. 什么是余弦相似度？

![img](D:\Work_APP\Typora\assets\wps7.jpg) 

\5. 使用属于相同和不同领域的负对是什么？

![img](D:\Work_APP\Typora\assets\wps8.jpg) 

\6. 泛化能力如何评估？

 

 

 

 

 

 

\7. 什么是Adam优化器？

 

![img](D:\Work_APP\Typora\assets\wps9.jpg) 

 

 

![img](D:\Work_APP\Typora\assets\wps10.jpg) 

 

# ***\*HUMAN3.6M数据集骨骼关节keypoint标注对应\****

（1）Top-Down（自上而下）方法

 

将人体检测和关键点检测分离，在图像上首先进行人体检测，找到所有的人体框，对每个人体框图再使用关键点检测，这类方法往往比较慢，但姿态估计准确度较高。目前的主流是CPN，Hourglass，CPM，Alpha Pose等。

 

（2）Bottom-Up（自下而上）方法

 

先检测图像中人体部件，然后将图像中多人人体的部件分别组合成人体，因此这类方法在测试推断的时候往往更快速，准确度稍低。典型就是COCO2016年人体关键点检测冠军Open 

https://blog.csdn.net/guyuealian/article/details/125502259

![img](D:\Work_APP\Typora\assets\wps11.jpg) 

![img](D:\Work_APP\Typora\assets\wps12.jpg) 

 

 

 

 

 

 

 

 

 

 

 

![img](D:\Work_APP\Typora\assets\wps13.jpg) 

 

 

 

 

![img](D:\Work_APP\Typora\assets\wps14.jpg) 

特征自动学习的功能

![img](D:\Work_APP\Typora\assets\wps15.jpg) 

https://ww.bilibili.com/video/BV1ZZ4y1R7m6

 

![img](D:\Work_APP\Typora\assets\wps16.jpg) 

 

 

 

![img](D:\Work_APP\Typora\assets\wps17.jpg) 

 

![img](D:\Work_APP\Typora\assets\wps18.jpg) 

 

 

 

 

人体姿态估计衍生问题：

单人多人问题，追踪问题，将2D关键点投射到3D表面上

Detectron2(facebook)

DensePose: Dense Human Pose Estimation In The

 

Densepose！！2D-3D

 

 

# ***\*Mediapipe\*******\*代码\****

![img](D:\Work_APP\Typora\assets\wps19.jpg) 

Mask 代表是不是人体

 

 

![img](D:\Work_APP\Typora\assets\wps20.jpg) 

x要乘以原图的宽度，y要乘以原图的高度获取归一化坐标

 

![img](D:\Work_APP\Typora\assets\wps21.jpg) 

对列表中的每个元素逐一用函数来操作

关键点的处理，因为

![img](D:\Work_APP\Typora\assets\wps22.jpg) 

 

对视频进行处理，pip追踪，有人体的，加快速度

![img](D:\Work_APP\Typora\assets\wps23.jpg)![img](D:\Work_APP\Typora\assets\wps24.jpg) 

 

 

 

 

 

 

 

 

# ***\*OpenCap:智能手机视频中的人体运动动力学\****

问题

1.肌肉激活、关节负荷和关节力矩对疾病风险的评估？ 

2.

 

 

关于mediapipe的学习

 

 

引入齐次坐标的意义

在透视空间里面，两条平行线可以相交，***\*平行线在透视空间的无穷远处交于一点，但是在欧氏空间却不能\*******\*。\****

齐次坐标就是用N+1维来代表N维坐标，加上一个额外的变量w来形成2D齐次坐标，一个点(X,Y)在齐次坐标里面变成了。

X = x/w

Y = y/w

(∞,∞)，然后它的齐次坐标表示为（1，2，0），因为(1/0, 2/0) = (∞,∞)，我们可以不用”∞"来表示一个无穷远处的点了

![img](D:\Work_APP\Typora\assets\wps25.jpg) 

![img](D:\Work_APP\Typora\assets\wps26.jpg) 

 

 

 

 

 

# ***\*2021轻量级人体姿态估计模型修炼之路\****

处理好数据、使用合适的Loss，随便用个剪枝后的轻量级模型都可以达到很高的精度.

调整模型输出、准备训练数据、修改DataLoader等,数据就用COCO和MPII就差不多了.

分类Accuracy、F1，检测常用mAP，一般是用PCK（PCKh）和OKS+mAP

 

 

肩关节角度测量的

# ***\*Validity and reliability of Kinect skeleton for measuring\**** ***\*shoulder joint angles: a feasibility study\****

***\*评估手段\****

类内相关系数(ICCs)、测量的标准误差和最小的可检测变化，确定了Kinect对肩部角度测量的绝对和相对测试-重测可靠性

计算了Kinect和测量肩部角度的标准方法之间95%的一致性限制(LOA)，以确定***\*并行\****有效性。

Kinect和两个测量标准之间的95% LOA大于±5◦在所有姿势的两种视图。

 

 

 

 

 

 

 

神奇的问题

互相调用就出问题，

![img](D:\Work_APP\Typora\assets\wps27.jpg) 

检测不到源文件中有这个变量报错

![img](D:\Work_APP\Typora\assets\wps28.jpg) 

 

 

 

 

 

 

# ***\*文献汇报，看的一些比较有趣的文献列表\****

 

1.

KeyTr: Keypoint Transporter for 3DReconstruction of Deformable Objects in Videos

KeyTr:用于视频中可变形对象的3D重构的关键点传输器

文章目的：我们考虑了从视频中重建动态物体深度的问题。动态视频深度预测的最新进展主要集中在通过多视图约束来改善单目深度估计器的输出，同时对场景的动态部分的变形施加很少或没有限制。然而，来自运动的非刚性结构理论规定了对3D重建的变形进行约束。因此，我们提出了一个与之前的工作有很大不同的新模型。其想法是使用Sinkhorn的算法将动态点云拟合到视频数据中，以将3D点与2D像素关联起来，并使用可微点渲染器来确保3D变形与测量光流的兼容性。通过这种方式，我们的算法，称为关键点传输器，在整个视频中对物体的整体变形进行建模，因此它可以相应地约束重建。与较弱的变形模型相比，这大大减少了重建的模糊性，对于动态对象，关键点传输器可以获得质量优于或至少与先前方法相当的重建，同时速度更快，依赖于预训练的单目深度估计器网络。为了评估该方法，我们对新的合成视频数据集进行了评估，这些视频描述了具有地面真实深度的动态人类和动物。我们还展示了对众包的真实世界宠物视频的定性结果。

 

2.

LifelongGlue: Keypoint matching for 3D reconstruction with continual neuralnetworks*

LifelongGlue:连续神经网络三维重建的关键点匹配*

人类通过不断的学习过程获得知识。他们从经验中学习，积累知识，并运用它来完成手头的任务。基于人工智能的系统的主要目标是产生人类大脑持续学习的能力。目前基于人工智能的自主系统在适当调节、良好调整和同质化的数据上表现良好。然而，对于大多数最先进的系统来说，当呈现多个基于任务的增量数据时，性能会受到抑制。在大脑学习的激励下，本文引入了LifelongGlue，这是一种用于3D重建图像之间关键点关联的持续学习神经网络。从视频或序列图像中对场景进行3D重建，在增强现实(AR)应用中起着至关重要的作用。关键点关联对于从多个视角对场景进行准确的姿态估计至关重要。目前开发的方法没有考虑视频连续帧之间的关系，并独立估计每对的关键点。我们提出的网络通过持续的自关注和交叉关注增强了局部特征的表达性，从而利用先前学习的知识实现了序列图像之间的精确点匹配。与传统和先前基于深度学习的方法相比，我们的方法在具有挑战性的室内和室外场景中获得了更高的姿态估计结果。我们的方法的性能在多个数据集上得到了验证。结果表明，所提出的方法优于最先进的匹配方法，同时获得了实质性的改进。

3.

Aperformance evaluation of keypoints detectionmethods SIFT and AKAZE for 3D reconstruction

关键点检测方法SIFT和AKAZE在三维重建中的性能评价

关键词:三维重建，关键点匹配，SIFT, AKAZE，多视点立体

摘要:本文讨论了多视点图像的三维重建。我们特别关注用于三维重建的关键点检测/特征描述符SIFT[1]和AKAZE[2]，并评估它们的性能。3D重建的质量高度依赖于图像之间的keypoint匹配结果，因此评估其性能非常重要。首先给出了SIFT和AKAZE算法的关键点匹配结果。接下来，根据每个匹配结果进行3D重建，并对3D点的精度进行评估。我们还使用了Multi View Stereo (CMVS[41])来获得密集的3D点。此外，在本研究中，我们提出了一种将SIFT和AKAZE相结合的新方法，以获得更详细的重建，并且显示出比单独使用SIFT或AKAZE更好的重建结果。

\4. BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos 

BKinD-3D:多视点视频的自监督3D关键点发现

量化三维运动对于研究人类和其他动物的行为很重要，但手动姿势注释是昂贵和耗时的。自监督关键点发现是一种很有前途的策略，可以在没有注释的情况下估计3D姿势。然而，目前的关键点发现方法通常处理单个2D视图，而不能在3D空间中操作。我们提出了一种新的方法，从行为智能体的多视图视频中执行3D中的自监督关键点发现，而不需要在2D或3D中进行任何关键点或边界框监督。我们的方法BKinD-3D使用带有3D体积热图的编码器-解码器架构，经过训练可以重建多个视图之间的时空差异，此外还可以对已学习的主体3D骨架进行关节长度约束。通过这种方式，我们在人类和大鼠的视频中不需要人工监督就能发现关键点，展示了3D关键点发现在研究行为方面的潜力。

5.HoloPose: Holistic 3D Human Reconstruction In-The-Wild

HoloPose:野外整体3D人体重建

我们介绍了HoloPose，一种整体单眼三维人体重建方法。我们首先引入了一个用于3D模型参数回归的基于部分的模型，该模型允许我们的方法在野外操作，优雅地处理严重闭塞和大姿态变化。我们进一步训练了一个包含2D、3D和Dense Pose估计的多任务网络，以驱动3D重建任务。为此，我们引入了一种迭代改进方法，该方法将基于模型的2D/3D关节位置和DensePose的3D估计与CNN提供的基于图像的对应估计对齐，由于自下而上的CNN处理，实现了基于模型的全局一致性和高空间精度。我们在具有挑战性的基准测试上验证了我们的贡献，表明我们的方法允许我们获得准确的关节和3D表面估计，同时在野外以超过10fps的速度运行。有关我们方法的更多信息，包括视频和演示，请访问

在这项工作中，我们提出了HoloPose，一种使用人体形状的紧密先验模型与多种姿态估计方法相结合的方法，以获得精确的单目3D人体重建。我们已经考虑到人体的铰接性质，表明它在单片基线上大大提高了性能，并引入了一种改进程序，允许迭代地调整单镜头系统的形状预测结果，以满足互补的全卷积网络施加的几何约束。未来，我们打算探索神经网格合成模型;使用分布式表示可以更容易地适应多模态分布，包括男性、女性和儿童表面，这些表面目前由单独的形状模型处理。此外，我们的方法可以从更精确的几何建模中受益，例如通过结合透视投影、表面法线和轮廓信息[4]，而我们预计使用深度数据、多视图[18]或时间信息[53]可以帮助消除3D重建误差。

SMPL是一个真实的人体3D模型，基于皮肤和混合形状，并从数千个3D身体扫描中学习。这个网站提供了学习SMPL的资源，包括带有动画SMPL模型的示例FBX文件，以及在Python、Maya和Unity中使用SMPL的代码。

 

Densepose

人体表面的全方位观察，把每个人变成UV贴图，一片一片一片，一片。

系统可以覆盖浑身上下超过5000个节点，比十几个关节要细致得多。

6.Single-View 3D reconstruction: A Survey of deep learning methods☆

单视图三维重建:深度学习方法综述

在过去的五年中，使用深度学习技术的单视图3D形状重建和生成领域得到了快速发展。随着该领域达到成熟阶段，大量的方法不断被提出，目的是进一步推动研究状态。本文的重点是通过根据它们作为输出使用的形状表示对这些方法进行分类来调查文献。具体来说，它涵盖了每种方法的主要贡献、监督程度、训练范式，以及它与整个文献的关系。此外，本调查还讨论了该领域使用的常见3D数据集、损失函数和评估指标。最后，对目前的研究现状进行了深入的分析和反思，并对尚未解决的问题和可能的未来方向进行了总结。这项工作是向感兴趣的研究人员介绍数据驱动的单视图3D重建领域的努力，同时足够全面，可以作为已经在该领域进行研究的人的参考。

\8. What Do Single-view 3D Reconstruction Networks Learn?

单视图3D重建网络学习什么?

卷积网络在单视图对象重建中的表现令人印象深刻，并已成为一个热门的研究课题。所有现有的技术都统一在一个想法上，即拥有一个编码器-解码器网络，该网络对输出空间的3D结构进行非平凡的推理。在这项工作中，我们建立了两种替代方法，分别执行图像分类和检索。这些简单的基线在定性和定量上都比最先进的方法产生更好的结果。我们表明，编码器-解码器方法在统计上与这些基线无法区分，从而表明单视图对象重建中的当前技术状态实际上并没有进行重建，而是进行图像分类。我们确定了引起这种行为的流行实验程序的各个方面，并讨论了改进当前研究状态的方法。

 

\9. MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video

MixSTE: Seq2seq混合时空编码器，用于视频中3D人体姿态估计

最近提出了一种基于变压器的方法，通过全局考虑所有帧之间的身体关节来学习时空相关性，从2D关键点序列估计3D人体姿态。我们观察到不同关节的运动有显著差异。然而，以前的方法不能有效地模拟每个关节的实体帧间对应关系，导致对时空相关性的学习不足。我们提出了MixSTE (Mixed Spatio-Temporal Encoder)，它有一个时间变压器块来单独建模每个关节的时间运动，还有一个空间变压器块来学习关节间的空间相关性。这两个块交替使用，以获得更好的时空特征编码。此外，将网络输出从输入视频的中心帧扩展到整个帧，从而提高了输入和输出序列之间的一致性。在三个基准(即Human3.6M、MPI-INF-3DHP和HumanEva)上进行了广泛的实验。结果表明，我们的模型比最先进的方法高出10.9%的P-MPJPE和7.6%的MPJPE。代码可在https://github上获得。com/JinluZhang1126/MixSTE。

 

\10. Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation

关键点转换器:解决具有挑战性的手和物体交互中的关节识别，以实现准确的3D姿态估计

 

我们提出了一种鲁棒和精确的方法来估计在一个单一的彩色图像在密切互动的两只手的三维姿态。这是一个非常具有挑战性的问题，因为关节之间可能会发生大的咬合和许多混淆。最先进的方法通过回归每个关节的热图来解决这个问题，这需要同时解决两个问题:定位关节和识别关节。在这项工作中，我们提出通过依赖CNN首先将关节定位为2D关键点，并通过这些关键点上CNN特征之间的自关注将它们与相应的手部关节相关联来分离这些任务。由此产生的架构，我们称之为“关键点转换器”，效率很高，因为它在InterHand2.6M数据集上使用大约一半的模型参数就实现了最先进的性能。我们还表明，它可以很容易地扩展到估计由一只或两只手操纵的物体的3D姿态，并且具有高性能。此外，我们创建了一个新的数据集，其中有超过75000张双手操纵物体的图像，这些图像以3D方式进行了充分的注释，并将公开提供。

 

\11. Learnable Triangulation of Human Pose

人体姿态的可学习三角测量

 

 

# 机器视觉的一些原理性笔记



 坐标系简介：**影像坐标系、相机坐标系、世界坐标系**

##  影像坐标

![image-20240113102724601](D:\Work_APP\Typora\assets\image-20240113102724601.png)

$(u_0, v_0)$这个点是像主点，是投影中心到成像平面的垂点。利用这个点来进行像素点到物理尺寸的一个转化。其公式如下：


$$
u - u _ { 0 } = x / d _ { x }\\
v - v _ { 0 } = y / d _ { y }\\
\left[ \begin{array}  { l  }  { u } \\ { v } \\ { 1 } \end{array} \right] = \left[ \begin{array}  { l  }  { \frac { 1 } { d x } } & {  1  } & { v _ { 0 } } \\ { 0 } & { 0 } & { v _ { 0 } } \\ { 0 } & { 0 } & { 1 } \end{array} \right] \left[ \begin{array}  { l  }  { x } \\ { y } \\ { 1 } \end{array} \right]-(1)
$$

## 相机坐标

相机坐标系的原点在相机中心，$ XY$($X_C-Y_C-Z_C$坐标系)轴和影像$xy$坐标系的$xy$轴平行，$Z$轴垂直于像平面且**朝向像平面**，$Z$轴和像平面的交点正是影像$xy$坐标系的原点（像主点）


![image-20240113104250794](D:\Work_APP\Typora\assets\image-20240113104250794.png)



在此方案下，像平面上的所有像素点在相机坐标系下的$Z$坐标等于焦距$f$，$XY$坐标和影像坐标系下的值相等，即若像素点$p$在影像$XY$坐标系下的坐标为$(x,y)$，则其在相机坐标系下的坐标为$(x,y,f)$。

建立像素点p和空间点P在相机坐标系下的坐标：

![](D:\Work_APP\Typora\assets\image-20240113105108812.png)

在相机坐标系中，空间点P和像素坐标的转换，由相似可知
$$
\left[ \begin{array}  { l  }  { x } \\ { y } \\ { 1 } \end{array} \right] = \left[ \begin{array}  { l  }  { \frac { f } { Z _ { c } } } & { 0 } &{0}\\ { 0 } & { \frac { f _ { c } } { Z _ { c } } } &{0}\\ { 0 } & { 0 } & { \frac { 1 } { z _ { c } } } \end{array} \right] \left[ \begin{array}  { l  }  { X _ { c } } \\ { Y _ { c}}\\{Z _ { c}} \end{array}\right]
$$
紧接着将xy坐标转化成影响坐标uv，结合上面的公式(1)可以得到：

![image-20240113114050717](D:\Work_APP\Typora\assets\image-20240113114050717.png)

通常把$Z_c$称为尺度因子$λ$，把中间的矩阵3*3叫内参矩阵K，显然内参矩阵K描述的是相机坐标到uv坐标之间的转换关系
$$
λ p = K P _ { c }
$$
**内参矩阵**

其中的fx=f/dx同理fy，像元方向上的像素单位值
$$
K = \left[ \begin{array}  { l l l  }  { f _ { x } } & { 0 } & { u _ { 0 } } \\ { 0 } & { f _ { y } } & { v _ { 0 } } \\ { 0 } & { 0 } & { 1 } \end{array} \right]
$$
![image-20240115203516285](D:\Work_APP\Typora\assets\image-20240115203516285.png)

其中s是图像非正交性因子。

## 世界坐标系

世界坐标系是一个固定的三维坐标系，是一个绝对坐标系，它旨在将空间的所有点都统一到同一个坐标系下表达，在不同应用场景中，世界坐标系的定义不一样，在大地测量中，将水准原点当做世界坐标系的原点；在相机标定中，将标定板（棋盘）的某个角点作为世界坐标系。

<img src="D:\Work_APP\Typora\assets\image-20240115202511450.png" alt="image-20240115202511450" style="zoom:50%;" />

世界坐标系和相机坐标系都是三维坐标系，它们之间可以用旋转平移来转换

<img src="D:\Work_APP\Typora\assets\image-20240115202615190.png" alt="image-20240115202615190" style="zoom: 50%;" />

转换公式如下
$$
\left[ \begin{array}  { l  }  { X _ { c } } \\ { Y _ { c } } \\ { Z _ { c } } \end{array} \right] = R _ { 3 \times 3 } \left[ \begin{array}  { l  }  { X _ { w } } \\ { Y _ { w } } \\ { Z _ { w } } \end{array} \right] + t _ { 3 \times 1 }
$$
旋转矩阵R和平移矢量t称为外参数矩阵。

有疑问：经过mmpose等这些算法估计的人体3D关键点坐标是属于哪个坐标系下的？

**外参矩阵**

外参矩阵也是相机的关键参数之一，由一个3x3的单位正交旋转矩阵R和3x1的平移矢量 t组成，它们描述的是***世界坐标系到相机坐标系***之间的转换关系。需要提一点的是，在不同学科中外参矩阵会有一些定义区别，比如在摄影测量学科中，将相机坐标系转换到世界坐标系的旋转矩阵R 以及摄影中心在世界坐标系中的位置C作为外参。它们目的一致，都是为了描述相机和世界坐标系之间的转换关系。
$$
P _ { c } = [ R \quad t ] \left[ \begin{array}  { l  }  { P _ { w } } \\ { 1 } \end{array} \right]
$$


**投影矩阵**

实践中最多的还是影像uv，和世界坐标系，在影像三维重建中，通常前者是输入，后者是输出，所以将世界坐标系转换成uv坐标系很关键！！



![image-20240115204033376](D:\Work_APP\Typora\assets\image-20240115204033376.png)

# SVD计算旋转、平移矩阵

问题引入

![image-20240116091923630](D:\Work_APP\Typora\assets\image-20240116091923630.png)



![image-20240116091850467](D:\Work_APP\Typora\assets\image-20240116091850467.png)

![image-20240116091835642](D:\Work_APP\Typora\assets\image-20240116091835642.png)

![image-20240116091906276](D:\Work_APP\Typora\assets\image-20240116091906276.png)

# 数据集已有的

Human3.6

NTU-RGB-D

ITOP（深度的）



# 阅读文献（二）

Fast and Accurate 3D Hand Pose Estimation via Recurrent Neural Network for Capturing Hand Articulations

## 基于递归神经网络的手部关节三维姿态快速准确估计

​		**阅读目标**：手部关节和全身关节是否有相似的可以共用之处。课题构象是进行关节活动度的评估、康复动作的指导（先实现肩关节活动度的），因此在训练的过程中也最好考虑关节活动的依赖关系。



​		**提出的问题**：1.  文章中是怎么利用手部关节结构的，怎么

​    

### 		**摘要** 

​		之前工作的局限性，依赖于复杂的网络结构，没有充分利用手部关节结构，没有充分考虑手指不同关节运动是相互依赖的。在本文中，我们提出了一个分层结构的卷积递归神经网络，它有六个分支，可以独立地估计手掌和五个手指的3D位置。手掌位置通过全连接层预测。HCRNN直接将深度图作为输入，而无需耗时的数据转换，例如3D体素和点云。

### 		**介绍**

​		先说明目前3D手部识别的主流方法，介绍了传统方法的两种提高精度的手段，1.转成体积信息-体素，从2D输入中提取出更多信息。2.利用手的结构特性的有效网络架构设计，手部姿态划分层子任务（关注特定手指和手部区域）。本文采用递归RNN

​	（1）分解子任务，RNN利用手指序列的空间信息

​	（2）三个公开数据集的精度，以及单个GPU的运行速度

### 	 **估计方法**

#### 		整体网络架构

1. 编码器-ResNet-50

2. 回归子网络（SubNet）

   - 手部关节序列建模
   - 手指分支
   - 手掌分支
   - 整体策略-特征集成（FC（全连接）层）

3. 损失函数，全局关节损失，局部关节损失

4. 操作细节：

   值得学习之处：

首先从深度图像中提取围绕中心点的固定大小的立方体。由于在简单深度阈值化之后通过置信获得的中心店可能不准确，因此一些方法，训练了一个简单的2Dcnn，该CNN使用深度阈值化获取裁剪的深度图像，并生成从当前中心点到基准真值联合为止中心的3D偏移。按照[5]

### 		实验结果

#### 1.数据集和评价标准

用了4个手部的数据集

#### 2.消融实验

图5（1）手掌+手指（自回归网络）

图5（2）五指+手掌，均采用全连接网络

图5（3） 五指(自回归网络)+手掌

#### 3.最先进方法比较

​	比较内容：a)图7（上）不同错误阈值上成功帧的百分比，（下）每个手关节的3D距离误差

三个数据集（左:ICVL数据集，中:NYU数据集，右:MSRA数据集。）

b）图8 MSRA数据集上不同偏航(左)和俯仰(右)视点角度上的平均误差距离比较。----看鲁棒性的

c)三个公共数据集上的定性结果，（基准真值用红、预测结果用绿）

## 3D human pose estimation in video with temporal convolutions and semi-supervised training

### 摘要

这项工作，证明了可以通过一个在2D关键点的基础上基于时间膨胀卷积的全卷积模型有效评估视频的3D姿态。反向投影，简单有效的半监督训练方法，利用了未标记的视频数据。

### 1.总体介绍

3D到2D是可以多对一的（模糊性）,递归神经网络时间信息进行建模解决。

主要方法：

**使用现成的2D关键点检测器预测未标记视频的2D关键点，预测3D姿势，然后将这些映射回2D空间**。

本文的贡献：

1）用三维人体姿态估计视频的基础上的膨胀的时间卷积的二维关键点轨迹

2）提出了一种半监督方法，利用相机内部参数。

### 2.相关工作

在深度学习成功之前，大多数3D姿态估计方法都是基于特征工程以及关于骨骼和关节移动性的假设。使用卷积神经网络的第一种神经方法专注于端对端重建，通过直接从RGB图像估计3D姿态，而无需中间监督。

**1）两步姿态估计**

困难在于预测准确的2D姿态。

**2）视频姿态估计**

实验验证seq2seq模型，输出姿态在长序列上发生漂移。解决方案：没5帧重新初始化编码器（可能牺牲了时间一致性）

**3）半监督训练**

利用未标记的视频和现成的2D关键点检测器结合，扩展具有反向投影的损失函数。解决了自动编码问题，编码器:姿态估计器，解码器：投影层。还有惩罚项，惩罚那些远离原始输入的2D关节坐标。一个epoch中有前是标记数据，后是未标记数据，**基准真值作为目标，训练损失函数**。未标记的数据用于实现编码器的dropout。预测的**3D被投影回2D**检测与输入的一致性。

**4）本文工作**

- 不使用热图，而是使用检测到的关键点坐标来描述姿势。好处是可以用高效的1D卷积。
- 方法特性：计算复杂度与关键点的空间分辨率无关，更少的参数达到高精度。
- 与[16]LSTM相比，我们通过在时间维度上执行1D卷积来利用时间信息，通过提出优化方法降低重建误差。区别于[16]该方法学习的是确定性的映射，而不是seq2seq（序列到序列）模型。
- 区别于文章中提出的两步模型（堆叠沙漏模型），本文证明Mask R-CNN和级联金字塔网络（CPN）检测对3D人体姿态估计更有鲁棒性。



### 3.时间扩张卷积模型

具有残差连接的完全卷积架构，将一些列2D姿态作为输入，并通过时间卷积对其进行转换。

1. 输入层获取级联（x，y）坐标，卷积核大小是W，C个输出通道
2. B 是ResNet 风格的块，首先执行内核W膨胀因子为$D=W^B$的一维卷积。
3. 执行具有内核大小1的卷积
4. 批归一化
5. 校正线性单元
6. dropout。
7. 感受野W以指数方式增加，参数数量线性增加。
8. 设置过滤器超参数W和D→任何输出帧的感受野形成所有输入帧的树。
9. 利用时间信息输出所以帧 3D姿态预测。
10. 评估实时的场景下利用因果卷积。

只执行未填充卷积，同时用左右边界帧的副本填充输入序列，结果会更好。



### **4.半监督方法**

**轨迹模型**：根关节的全局位置（轨迹），所有关节相对于根关节的位置（3D姿态），目的是为了正确执行到2D的反向投影。还引入一个损失函数-WMPJPE为了调节受试者因为距离相机过远而难以回归精确轨迹的问题。2D关键点集中在一小块区域里，不必回归精确轨迹。

**骨骼长度L2损失**：为实现不仅仅是复制输入的合理3D姿态预测，引入软约束，以使**未标记**批次中受试者的**平均骨长**与**标记**批次受试者的**平均骨长**大致匹配。

主要关注[40]提出的一种基于顺序（深度图）注释的弱监督。

**讨论：**该方法只需要相机内部参数。

1. 利用时间扩张卷积将2D姿态映射到3D上。
2. 投影层将3D姿态投影到2D，（焦距、主像点、切向、径向）

### 5.实验设置

1）数据集评估

Human3.6M中，有7个受试者带有3D姿态注释。本文将（S1，S5，S6，S7，S8）进行训练，对（S9和S11）进行测试。为每个动作训练一个模型。

HumanEva-Ⅰ，训练不同的模型来评估三个动作，展示了为所有动作训练一个模型时的结果，采用15关节骨架。

评估策略：MPJPE、P-MPJPE（平移旋转、缩放，对齐误差）、N-MPJPE（半监督实验）

2）2D姿态估计实现细节

- 以前的方法：从基准真值边界框中提取受试者，然后运用堆叠沙漏检测框里的2D关键点位置。

- 本文方法：不依赖于任何特定的2D关键点检测器。

- 研究的几种2D检测器用于户外使用：沙漏检测器、ResNet-101-FPN、Mask R-CNN （Detectron中的参考实现）、FPN扩展的CPN（级联金字塔网络），然后CPN需要提供外部边界框，本文使用Mask-R-CNN的框

  

- Mask R-CNN和CPN，本文从COCO上预训练，在Human3.6M的3D投影上微调检测器。在消融实验中，直接将3D姿态估计器应用到预训练的2D COCO关键点（由于COCO和human3.6M关键点不同）来估计Human3.6m的3D关节。

- 对于R-CNN的具体训练过程，在Human3.6M上微调模型，重新初始化关键点网络的最后一层，以及回归热图一学习一组新关键点检测deconv层。学习率采用逐步下降的方式：（学习率大小-迭代次数）1 e-3--60k，1 e-4--10k，1e-5--10k。推理过程用softmax，提取2D分布的期望（soft-argmax）。

- 对于CPN，使用分辨率为384×288的ResNet-50。微调，初始化GlobalNet和RefineNet的最后一层（卷积权重和批量归一化统计），一个GPU上训练32个图像批次，逐步衰减学习率，5e-5（初始值的1/10）--6k。。。微调时保持**批处理规范化**。使用基准真值边界框进行训练，使用微调的**Mask R-CNN**模型预测的**边界框**进行测试。

3）3D姿态估计的实现细节

- 通过根据相机变换旋转和平移基准真值姿态来训练和评估相机中的3D姿态，不实用全局轨迹（在半监督中才用了）。
- 优化器：Amsgrad，对human3.6m，80个epoch，指数衰减学习率计划，η = 0.001开始，α = 0.95（缩放因子，训练的每个时期）
- 具有感受野大于1的模型，对姿态序列中样本的相关性敏感，致使批量归一化的有偏统计（非独立会有影响）。初步试验，训练时，预测利用相邻帧比不利用时间信息模型（**批次中具有良好随机化的样本**）更差。解决方法：选择训练片段，减少相关性。这个剪辑下来的子片段，设置为架构的感受野宽度，这样方便每个训练剪辑的3D姿态。利于推广。
- 训练阶段：步幅卷积代替膨胀卷积，将膨胀因子设置为步幅大小。---避免计算未用的状态
  推理期间：处理整个序列，重新利用其他3D帧的中间状态---快速推理
  没有池化层，为了避免丢失有效的帧，仅在序列输入边界上进行填充复制
- 批量归一化的默认参数→测试误差波动，推断的估计波动。(为了稳定运行统计)解决方法：设置批量归一化的动量β[0.1,0.001]，针对每个epoch，它的大小以指数衰减。在测试训练时执行水平翻转增强。

### 6.结果

1)时间扩张卷积模型

​		*B = 4个块,243个输入帧,在两种评价标准下，该模型的平均误差低于其他方法。并且不依赖于其他方法的（+）等的额外数据。[27]比俺们好，但是它使用了基准真值框（2D）。将所以卷积核的宽度设为W=1的单帧基线相比，评估指标高出了5mm。差距较大体现在高度动态的动作，走路（6.7mm）。具有因果卷积的模型大约在单帧基线和我们模型之间的一般。因果卷积通过预测最右侧输入帧的3D姿态来实现在线处理。本文，还发现，基准真值框和Mask R-CNN预测边界框具有相似性能。*

​	**MPII→Human3.6M：沙漏网络预测**

**COCO→Human3.3M：Detertron和CPN**

**得出结论：Mask R-CNN和CPN给予比堆叠沙漏网络更好的性能**

**结果的原因：更高的热图分辨率，更强的特征组合（Mask R-CNN--特征金字塔 && CPN的RefineNet），更多样化的数据集**

​		引入MPJVE（即MPJPE的导数，单帧基线的下降值）衡量预测随时间的平滑度。表2

​		表4是说明可以推广到较小的数据集HumanEva-I。

​		表5在复杂度方面比较了卷积模型与[16]的LSTM模型。用模型参数数量和FLOP的估计值去衡量。后者我们考虑矩阵乘法，并报告假设无限长序列的摊销成本。和[16]比，在计算量减半的情况下，我们的模型实现了显著较低的误差。以此说明扩张卷积的优势。

​		

### Ordinal Depth Supervision for 3D Human Pose Estimation

#### 摘要

​       介绍的是一种端对端方法，提出的原因，当时大多数数据集都是在实验室条件下使用光学动捕采集的，并且很能达到2D姿态（如MPII和LSP数据集）检测数据集可变性的要求，使用由人类关节的顺序深度提供的弱监督方式，减轻对3D基准真值的需求。顺序深度信息可以通过人工注释者针对广泛的姿态和图像来获取。顺序注释可以很容易地结合到典型3D人体姿态估计的卷积网络的训练过程中。本文展示了在不同设置中使用顺序关系训练卷积网络。

### 1.方法介绍

3D基准真值不易获取。

提升端对端判别方法：增加训练数据

- 通过合成的方法获取---不能保证来自和自然相同的分布中。
- 准确捕获3D基准真值的多视图系统可以在户外工作----需要校正同步，不实用难扩产。
- [5,25]采用可靠的2D姿态检测器估计来恢复3D姿态----3D姿态可能错误，而这种原因在于关节的二进制顺序深度关系模糊
- 利用基于图像的特征，如遮挡和阴影

​         顺序深度可以解决重建模糊性方面的能力，并且该信息可以由人工注释获取。人工对有序深度优于当时的显式度量深度。

- 使用它们来直接预测关节的深度
- 将它们与2D关键点注释结合联合收割机来预测3D姿势
- 演示了他们如何被纳入3D姿态的体积表示

​        最终结果是，顺序深度监督对人体姿态问题的有效性（相对于采用实际3D基准真值的完全监督方法而言更具有竞争力），为使用可用注释集进行进一步探索提供动力。

### 2.相关工作

**重建方法**

- 采用2D姿态检测器定位2D人体姿态关节，并使用这些位置来估计合理的3D姿态[10,17]
- 使用来自2D姿态的卷积的2D热图来重建视频序列中的3D姿态
- [5]将3D热提行走统计模型拟合到预测的2D关节.
- 2D提升到3D的
- [25]使用一个简单的多层感知器实现了最先进的结果，该感知器在给定2D关键点作为输入的情况下回归3D关节位置
  缺陷是没有使用图像的证据（`image-base evidence`）
  `直接使用的、与图像内容相关的信息，如纹理、光照、阴影、轮廓以及物体间的相对位置和大小等`

**判别方法**

判别方法和重建范式正交，因为二者直接从图像估计3D姿态。之前的工作：

- 使用ConvNets回归3D关节坐标
- 将每个图像分类到适当的姿势类

上诉方法的缺陷在于，训练时需要具有相应3D基准真值的图像。

本分的方法：

<img src="D:\Work_APP\Typora\assets\图片2.png" alt="图片2" style="zoom: 33%;" />

**生成训练示例**

训练过程中3D基准真值的可用性受限问题得到解决。

- 使用图像来增强训练数据。
- 拼贴方法，通过从不同图像中组合人体部位来产生已知3D姿态组合

缺陷：户外图像的该有的细节和多样性水平不够

**3D注释**

依赖人工感知和注释，通过2D图像投影到3D场景中，丢失了3D属性。

- [12]注释了图像中**像素表观**深度的顺序关系
- 调整3D姿势，使得程序费力。
- [23]提供了人体姿势的3D注释，但仅以头部和躯干的偏航角的形式。
- [35]其中关于身体部位的相对3D位置的属性包含在他们的posebits数据库中

本文提出了一种比posebits更大的注释图像，仅需要2D关键点位置和顺序深度关系，不是探索大量的姿势属性。

- [21]估计了注释者接受或拒绝的单个图像的3D人体形状拟合建议——
  自动建议的质量很低，导致很多dropout不合理

本文提供一种更平衡的解决方案，3D注释具有较弱的形式，并且注释对人工来说比较容易，可以为几乎任何可用的图像提供大规模的注释。

### 3**顺序（深度）关系**

- 使用关系来学习场景的表观深度[70，12]或反射率[29，63]

本文中的顺序关系与度量深度或绝对反射率值相比，序数关系更容易为人工注释。

**1）深度预测**

目标是建立训练过程→利用深度顺序关系→学习人体关节的深度。（并不显式预测3D姿态）

具体的方法：

**+1表示关节i比关节j近**（距离相机坐标的原点？）

下面是部分损失函数

<img src="D:\Work_APP\Typora\assets\image-20240124110502137.png" alt="image-20240124110502137" style="zoom:50%;" />



解释一波：如果深度Zi和Zj中的一个比另一个更近，则强制他们之间有个大的余量，否则强制他们相等，下面是全部损失

<img src="D:\Work_APP\Typora\assets\image-20240124111052295.png" alt="image-20240124111052295" style="zoom:50%;" />

- 不需要再训练过程中所有关节对的关系都可用，损失可以进基于注释的对的自己计算。
- 关系不是必须是一致的
- 不需要严格全局排序
- 允许ConvNet通过最小化所参数的损失，来从提供的关系中学习共性！（防止注释的歧义）

**2） 3D姿态的坐标预测**

从最初预测的关节深度出发，在图片上进行精确定位相应关节，为上一节中的ConvNet提供预测的2D坐标来丰富它的输出。

**(X,Y)+2N个附加值→弱3D信息**

ConvNet →弱监督版本

$w_n$基准真值2D位置，$\hat{w}_n$表示卷积的预测2D位置

<img src="D:\Work_APP\Typora\assets\image-20240124112126208.png" alt="image-20240124112126208" style="zoom: 50%;" />

其中的$$\lambda $$=100，模型的损失函数

<img src="D:\Work_APP\Typora\assets\image-20240124112214003.png" alt="image-20240124112214003" style="zoom:50%;" />

**3）3D姿态的体积预测**

​		除了3D姿态坐标的直接回归，3D人体姿态的体积表示使用。受试者空间被离散化，用ConvNet预测3D空间，每个体素（体积像素）中包含关节的概率。这些似然值是针对3D空间中的每个关节进行预测的，**网络本质上是估计整个3D体积中关节的概率分布**。训练目标是以每个关节的3D位置为中心的3D高斯。然而，没有正确的3D基准真值，确定高斯分布参数变得困难。因此，将3D真值调整为序数深度监督，并且可以证明序数关系的普适性。

​		保留体积的结构，分解2D平面监督，和深度。用卷积预测每个关节n的得分，并且用softmax将其转换为概率分布。关节n落在u位置的概率为p。

​													$$p ( x , y | n ) = \sum _ { z } p ( u | n ) , $$

​		体积的所有的切片（体积在某一维度下的截面），和池化（sum-pooling）对输入数据的子区域（在这种情况下是体积的切片）执行求和操作。通过将多个切片的对应元素相加，来得到忽略其他变量时该变量的总概率。相当用来个弱透视模型。

​													$$p ( z | n ) = \sum _ { x , y } p ( u | n ) , $$

​		可以再次利用求和池化操作，计算跨所有切片的像素的深度。2D图像和深度被独立监督，但它们通过执行3D一致性的底层体积表示连接。损失函数：

$$L = L _ { r a n k } + \lambda L _ { h e a t } . $$  $L_{rank}$是z的排序损失。软分布的平均值，恢复深度，x-y维度的目标是热图，并且以真实位置为中心的一个高斯分布。

​		监督类型不影响在最先进的卷积网络中使用，并且在3D真值不可用[64,44...]这些论文中与所提出的顺序监督进行补充。

**4)与重构组件集成**

**优势**：利用基于图像信息来解决单视图深度模糊性，并产生遵从人体关节的循序深度的深度估计$z_n$。

**缺陷**：由于没有完整后的3D姿态示例被网络训练。预测深度通常和精确深度不匹配。

**解决方法**：重建组件来增强架构。

![image-20240125095325305](D:\Work_APP\Typora\assets\image-20240125095325305.png)

$w_n=(x_n,y_n)$,输入是像素位置$x_n$和$y_n$与有序深度$z_n$的拼接，经过两个双线性单元的多层感知器，输出三维姿态坐标（$S∈R^{n×3}$）。利用Mocap数据，训练这个组件。

**训练过程**：将每个3D姿态骨架投影到2D平面。使用投影的（有噪声的）2D关节位置和深度模拟输入，便于保留大部分顺序关系。它们的值可能不一定和实际深度匹配，用L2损失作为卷积输出的3D关节坐标和用于生成输入的3D姿态关节坐标。$$L _ { 3 D } = \sum _ { n = 1 } ^ { N } || S _ { n } - \widehat { S } _ { n } | | _ { 2 } ^ { 2 } . $$

如图所示上述模块可以集成到端对端框架。

![image-20240125100809949](D:\Work_APP\Typora\assets\image-20240125100809949.png)

### **4.实验评估**

（1）采用的定量和定性评价的基准。（2）提供了一些必要的实现细节的方法。 （3）选定的数据集上给出了定量和定性结果。

**1）数据集**

​		使用了：Human3.6M（50fps→10fps）、HumanEva-I、MPI-INF-3DHP（户外的）、LSP + MPII Ordinal（有序深度注释）

​		注释器用来表示每个图像的一对关节，哪个更接近相机，也提供了模糊的选项。一共14个（除胸部、脊柱外）关节。循环进行推理，知道所有关节的全局排序结果都出来了，强制进行全局排序，不会遇到有矛盾的注释。一个需要解决$\binom{14}{2}=91$此排序问题。每张图片处理17个问题，注释时间加快了5倍。连续询问注释器一对特定关节的效率要高得多（不改变焦点）。建立50个图片组，其中包含关于同一对关节问题。通过这种方法，可以以每个问题3.5s的速度获得注释。

​		使用MPI-INF-3DHP证明：a)可以提高标准基准的3D人体姿势估计的性能,b）它们有助于我们的ConvNets进行适当的泛化

**2) 实现细节**

2D关键点或深度的卷积，根据沙漏模型设计。

**两种（沙漏）形式：**

a)输出是左边形式，一个沙漏，最后是全连接层

b)有体积目标时，两个沙漏。

训练方法也值得学习，采用混合训练策略，将来着H36M和HEI的3D基准真值相结合，也是用了LMO的图像。

**处理方式：**

在LMO示例中，采用基于人工注释的弱监督计算损失。

相应数据集示例，基于已知基准真值（完全监督）计算损失

**具体参数细节：**

批量为4，学习率为2.5e-4，使用rmsprop优化，旋转（±30°）、缩放（0.75-1.25）、翻转（左右）来增强数据。

**训练的时间：**

取决于数据集的大小，h36m经过300k次迭代，h36m和LMO混合经过2.5M次迭代，而HEI和LMO混合则经过1.5m次迭代。训练批量大小64，学习率2.5e-4，使用rmsprop进行优化，持续20万次迭代。

**3）消融实验**

***有序监督：***

- 检查使用有序深度监督与使用实际3D基准真值进行训练的效果。（Aim：考虑监督形式，唯一一次用到3D基准真值的）
  - 使用Human3.6M
  - 提供3D基准真值来执行**定量**比较。
  - 考虑每对关节的**深度值**→定义深度顺序（r = 0、r = ±1）
- 三种不同的预测方案，深度预测、坐标回归、体积回归。对其中的每一个加入顺序监督进行比较，以及使用3D真值进行训练。从表里面看出，有序监督和完全监督差不是很大。差距比较大的会出现在体积回归的情况，这种情况是一种比较强的架构具有两个沙漏分量。

***改进3D姿态检测器：***

- 使用仍标注的有序深度注释，提高卷积网络的3D性能[32]，遵循混合训练，利用H36m图像和3D真值和LMO和注释，使用2D关键点注释的自然图像数据增强一种标准做法[48,26,36,64,44].**考虑顺序深度监督效果，重建组件放在网络末端。**
- 使用更多的训练示例可以提高性能

## **3D Human Pose Estimation with Spatial and Temporal Transformers**（PoseFormer）

### 摘要

提出一种纯粹transformer的方法，用于视频中进行3D人体姿态估计。采用时空变换器结构，以全面建模每个帧内的人体关节关系。

### 1.介绍

目前的方法有：1.直接估计法，2.2D到3D提升的方法。为了使得2D姿态和3D姿态唯一对应，保持动作的自然连贯性，集成时间信息。利用**时间卷积CNN**来捕获相邻帧的全局依赖关系。（或**递归**的方式）。**局限在于**：时间相关性窗口对于这两种都是有限的。

a）基于CNN的方法依赖膨胀技术，只有有限的时间连续性。

b)  递归网络主要限于简单的顺序相关。

TF的方法局限是：大规模数据集，小数据集时需要数据增强和正则化。

本文的方法：PF直接使用了两个维度不同的TF对时空进行建模。好处是：表示了时空的同时，不为长序列引入大量标记。从现成2D姿态→3D中心帧姿态。

a)TF模块模块来编码每个帧中的2D关节之间的局部关系。空间自注意力层考虑2D关节的位置信息，并返回该帧的潜在特征表示。

b)





## point clouds

点云是三维坐标系中的一组数据点。在点云中包含了多种测量，包括颜色和亮度，以及它沿XYZ轴的位置。



## Accurate 3D Pose Estimation From a Single Depth Image

### 摘要

1.将输入深度图与一组预先捕获的运动样本进行匹配，来生成身体配置估计以及输入点云的语义标记

2.通过直接将身体构型与观察结果拟合来细化初始估计(输入深度图)

3.改进云平滑技术，在独立视图中高效实现点云对齐和姿态搜索。

## 介绍

1.只使用的深度信息来重建关节的身体构型。

- 深度测量**避免**了透视投影在2D图像中引起的**模糊性**;对光照条件不敏感，
- 低成本的有源深度传感器变得越来越可用。

挑战和解决：

- 深度传感器仅生成具有噪声和离群值的点云;必须提取关于哪个部分对应于哪个关节的语义信息
- 存在大的遮挡：至少50%的身体在任何单一视图中不可观察。

2.本文方法：使用一个预先捕获的运动数据库来约束可能的身体配置空间。数据库包含身体表面模型和相应的骨骼身体配置。

- 深度图和身体表面模型匹配，从表面模型获得语义标记，并获得底层身体配置。

- 使用**非刚性点配准**过程进一步优化身体配置。

  有两个目的：

  - **考虑样本表面模型之间的差异**
  - **填充输入中被遮挡的缺失区域**

  避免了一个巨大的运动数据库与直接映射输入数据到身体配置的典型问题

3.本文贡献了一种新的视图无关匹配算法即：**三维全身表面网格和深度图匹配算法**

使用PCA用于深度图和运动数据库

- 在三个主要的轴对齐
- 在缩小的空间中搜索
- 用加速计算，消除由姿态微变引起的模糊性

4.使用主动式深度传感器进行运动捕获，获得38mm的平均精度，优于[9]中的100mm。**可量化的准确性**是关键。

### 相关工作

...

### 方法概述

数据库是通过驱动通用人类网格模型来生成的。

1.该模型具有从以120 Hz操作的八相机光学运动捕捉系统捕捉的运动。使用线性混合技术对网格模型进行动画处理。4个不同视角$P _ { l } ^ { i }$渲染四个合成的深度图像，用于解释与视图无关的形状匹配。

2.单个深度传感器的一个或多个深度图像或等效的点云，方法的目标是基于我们的运动数据库$$ \left\{ M _ { l } , P _ { l } ^ { 1 } , P _ { l } ^ { 2 } , P _ { l } ^ { 3 } , P _ { l } ^ { 4 } , Y _ { l } \right\}$$，来估计给定深度图Xj的身体构型。

3.处理点云的过程如下

- 根据距离信息删除不相关的对象，两个固定的距离阈值表示感兴趣距离。
- 曲面重建算法去噪
- 将点云数据转换到一个规范的坐标系中，在运动数据库中识别出相似的姿态
- 通过输入与对应姿态的渲染深度图之间的**非刚性配准**来估计细化的姿态配置
- 依靠数据库样本和形状完成方法来处理大的遮挡
- 依靠**数据库样本**和形状完成方法来**处理**大的**遮挡**，采用故障检测和恢复机制来处理偶然的故障

<img src="D:\Work_APP\Typora\assets\image-20240220220449368.png" alt="image-20240220220449368" style="zoom:33%;" />

通过深度阈值得到去除背景的深度图，去除噪声（离群点）得到人体点云，比对并在数据库中进行检索，将形状进行细化，最后进行骨骼评估。

## 点云分割和去噪

1.处理输入深度图以去除背景和噪声

- 通过定义**边界框**或通过背景减除来容易地去除背景对象。
- 后续处理需要找到输入点云和数据库样本之间的点对应关系
- 克服噪声输入的这一障碍，修改表面重建算法-局部最优投影算法（LOP）

2.LOP是一种无参数化算子，给定目标点P，将任意点投影到数据P上。重建数据P中的底层几何结构。使投影点集与P之间的加权距离之和最小。防止投影点集中的点彼此过于接近。

3.LOP直接应用存在问题：半径h控制平滑度，决定了投影点可以为目标距离贡献多大邻域。h太大，点云收缩，h太小去噪不好。

4.寻求一个，平滑和几何结构的保护。通过仔细研究将这种方法应用于深度图，通过投影点集的z坐标来避免收缩，其中包含最重要的深度信息，而x和y可以通过重新投影来计算。（为了找个折中的半径）

5.LOP算法的初始化

<img src="D:\Work_APP\Typora\assets\image-20240221203839882.png" alt="image-20240221203839882" style="zoom: 50%;" />

其中θ(.)受h控制的快速递减函数，每次迭代k=1,2,...K，点的更新为

<img src="D:\Work_APP\Typora\assets\image-20240221204044347.png" alt="image-20240221204044347" style="zoom:50%;" />

<img src="D:\Work_APP\Typora\assets\image-20240221204205000.png" alt="image-20240221204205000" style="zoom:50%;" />

$η(r)=1/3r^3$和μ∈[0,0.5)是平滑度和表面几何精度发挥作用的排斥参数。设置h = 0.5和µ = 0.35通常在K = 5次迭代中获得最佳结果。在应用LOP之后，删除了孤立的离群点，这些点没有有效支持的邻域，并且早投影后保持不变。这些点通过阈值化到其最近点的距离来识别。我们的去噪方案的有效如图5所示，其中我们还将其深度图上的双边滤波(BF)进行了比较，(BF)会生成链接不想交部分的不需要点，这可能是由于深度图的低分辨率和遮挡边界上的杂散点太多。

## 基于模型的运动评估

在深度图被分割和清理之后，我们的下一步是在运动数据库中搜索类似的姿势。直接测量完整的网格模型和深度图之间的相似性是困难的，因为深度图是不完整的。并且没有关于从什么视点捕获深度图的先验知识。

解决方案的探索：

- 从代表性的观看方向生成几个合成的深度图，并将输入点云与这些**代表性的视图**对齐
- 降维，找到最相似的深度图

### 点云排列

1.解决这个视点依赖性问题的观察点云的主轴提供鲁棒性的姿态，通过它可以构建和应用的点云上的转换，以纠正它的规范视图的特点。

2.局部坐标系定义：（）

- 计算主轴：
  给定一个点云 �*χ*，首先计算其维度为 �×3*N*×3 的协方差矩阵，其中 �*N* 是点云中点的数量。
  这个协方差矩阵的特征向量（eigenvectors）代表点云跨越的三个主要方向，这些也称为点云的主轴。
- 匹配不同视点：
  这些主轴通常能在不同的视点下提供足够的匹配，这对于多视图任务来说是有用的。
- **处理微小的不对齐**：
  使存在微小的不对齐，作者也描述了使用邻居搜索和非刚性配准方法来处理这些不对齐
- **定义局部坐标系**：
  - 通过计算点云的平均值来定义一个局部坐标系的原点。
  - 对于点云的三个主轴，由于不知道主轴的正方向，选择最大的主轴并将其正方向定义为相机坐标系的“向上”方向，假设相机通常不会倒置
- **创建规范的坐标系**
  	使用点云的平均值作为原点，定义四个规范坐标系。这是通过交替剩余两个轴的正方向来实现的
- **定义虚拟深度相机**
  对于每个规范坐标系，定义一个虚拟的深度相机，这个相机远离原点，并且朝向深度方向。
- **变换和渲染**
  将每个网格模型 ��*M**l* 转换到它自己的规范坐标系中，并在这些坐标系中合成四个深度图，记为 {���}�=14{*P**i**l*}*i*=14。

3.对于输入点云，计算变换T，变换后的点云Xc也在视图依赖的规范坐标系中。

## 低维子空间中的最近邻搜索

搜索合成深度图来找到最相似的姿态。有两种方法可以搜索，1.比较三维空间中的点的距离在3D空间搜索。2.图像空间的PCA空间搜索，并在PCA空间中寻找最接近$X^c$的$P^i_l$。

​	1.消除视点依赖性：变换后的点云中，对捕获点云的视点的依赖性大部分被消除，从而可以对不同视图之间的姿势进行更一致的比较。

​	2.PCA空间搜索:
​	不是在原始3D空间中搜索相似姿态，而是在图像空间PCA中进行。使用PCA空间的原因是它可以用更少维度表示数据中的主要变化，从而简化了对相似姿态的搜索。将深度图转化成PCA空间，替代在3D空间中比较点的距离。涉及到对合成的深度图进行矢量化，并将其堆叠成一个矩阵。然后将PCA应用于该矩阵以找到捕获数据中最显著方差的子空间。得到的PCA子空间简化了数据，降低了数据的维数，并允许更快、更有效的计算。

​	3.系数向量：在PCA空间中，系统向量是学习得到。这些向量表示约简PCA空间中的原始深度图。

​	4.最近邻搜索：为了找到于给定深度图最相似的姿态，该方法在PCA空间中搜索具有最近系数向量的合成深度图。这是通过最小化系数向量之间的差的范数来完成的，有效地找到PCA子空间中的最近邻。

5.姿态估计:在识别出最近邻后，获得与给定深度图相似的姿态对应的点云和表面模型。这也允许对给定深度图的联合配置进行初始估计。

6.转换到输入帧：该方法承认所有的计算已经在一个规范的坐标帧中执行。为了利用原始上下文中的结果，对曲面模型关节构型进行逆变化，将它们映射回输入坐标框。这一步对于保持所分析姿态的正确空间关系和方向至关重要。

7。细化

## 姿态细化

1.<img src="D:\Work_APP\Typora\assets\image-20240223145922548.png" alt="image-20240223145922548" style="zoom: 67%;" />

2.[11]的方法保持点云X和表面模型之间点的对应，用拉式坐标保持Ml的几何结构，用最小二乘法估计变形的网络

<img src="D:\Work_APP\Typora\assets\image-20240223150420026.png" alt="image-20240223150420026" style="zoom:50%;" />

在表面网络模型$M_l$中，用一组网络顶格$q^k_i$确定关节中心，以下是原始表明网格模型中的顶格和变形后网格的关系

![image-20240223150712051](D:\Work_APP\Typora\assets\image-20240223150712051.png)

3.用低通滤波平滑轨迹

### 故障检测和修复

相邻节理的长度发生巨大突变，节理角度超出正常范围。解决：用补全的方法借助$\hat{\mathcal{M}}_{l}^{t-1}$得到$\hat{\mathcal{M}}_{l}^{t}$

## 实验结果

以下是误差的计算公式。

![image-20240223151505698](D:\Work_APP\Typora\assets\image-20240223151505698.png)

在数据集中找到21、27序列为代表进行讨论。分别进行了去噪、姿态识别改进、视点独立性、错误检验、数据集依赖性、在Kinect上进行了定性评估，根据[21]的底层算法，定量估计了[9]中数据集的精度（mAP），最后进行了身体尺寸不变性的计算。

## 实验结论

通过使用先验数据库，不仅减少了可能的关节构型空间，而且为填补不可观测部分提供了一种有效的方法。

提出一种视图无关的姿态检测算法，该算法大大减少了运动数据库的大小。从一个通用的人体模型合成的运动样本。



## 文献阅读方法补充

一、综述类文章

（三类）

## Towards Viewpoint Invariant 3D Human Pose Estimation







## V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map





















改进的参考方法。

在您的网络中引入多头注意力机制，您需要修改或扩展现有的网络架构以包含注意力层。多头注意力（Multi-Head Attention）是Transformer模型中的一种关键机制，它允许模型同时在多个位置关注输入的不同部分，增强模型处理序列数据的能力。

以下是在您提供的3D网络架构中引入多头注意力机制的基本步骤和代码示例。这个例子将展示如何定义一个多头注意力模块，并在现有的编码器-解码器结构中的合适位置加入这个模块。

1. **定义多头注意力模块**：首先，您需要定义一个多头注意力模块。PyTorch的`torch.nn.MultiheadAttention`提供了这一功能。但是，由于您的网络是针对3D数据设计的，而`MultiheadAttention`默认用于处理2D序列数据，您可能需要自定义一个适用于3D数据的版本或通过调整数据维度使其兼容。

2. **集成到网络中**：选择一个合适的位置在您的网络中插入多头注意力模块。在3D网络中，一个常见的选择是在编码器深层次提取特征后或在解码器中加入注意力机制，以增强网络对特征的理解和重建能力。

下面是一个简化的示例，展示了如何定义一个基于PyTorch的简单的多头注意力模块，并假设您已经将数据调整为合适的形式以适配标准的`MultiheadAttention`。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 假设的多头注意力模块
class MultiHeadAttention3D(nn.Module):
    def __init__(self, embed_size, heads):
        super(MultiHeadAttention3D, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, value, key, query):
        # Split the embedding into self.heads different pieces
        N = query.shape[0]
        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]

        # Split the embedding into self.heads pieces
        values = value.reshape(N, value_len, self.heads, self.head_dim)
        keys = key.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)

        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)

        # Attention mechanism here (omitted for brevity)
        # output = ...

        return output

# 在您的网络中使用多头注意力
class V2VModelWithAttention(V2VModel):
    def __init__(self, input_channels, output_channels, embed_size, heads):
        super(V2VModelWithAttention, self).__init__(input_channels, output_channels)
        self.multihead_attention = MultiHeadAttention3D(embed_size, heads)

    def forward(self, x):
        # 假设x是在合适的维度上
        # 应用多头注意力机制
        attention_output = self.multihead_attention(x, x, x)
        # 继续网络的其余部分
        x = self.front_layers(x)
        x = self.encoder_decoder(x)
        x = self.back_layers(x)
        x = self.output_layer(x)
        return x
```

注意，这个例子是一个高度简化的版本，主要用于展示如何结构化多头注意力模块。在实际应用中，您需要根据您的具体需





## VGG网络学习（由ITOP的文献引发）

<img src="D:\Work_APP\Typora\assets\image-20240302090041324.png" alt="image-20240302090041324" style="zoom: 50%;" />

根据卷积核的大小，核卷积层数，VGG共有6中配置，分别为A、A-LRN、B、C、D、E，其中D和E两种是最为常用的VGG16和VGG19。

隐藏层：（在训练集中，中间结点的准确值我们不知道）用的是ReLU进行激活。
**（是浅层的隐层网络更加关注局部的细节特征，高层的隐层网络更加关注全局的抽象特征）**

<img src="D:\Work_APP\Typora\assets\image-20240302093915012.png" alt="image-20240302093915012" style="zoom:50%;" />

<img src="D:\Work_APP\Typora\assets\image-20240302093929671.png" alt="image-20240302093929671" style="zoom:50%;" />



### 损失函数，代价函数，目标函数

| **函数名称**                    |                                                              |
| ------------------------------- | ------------------------------------------------------------ |
| **损失函数（Loss Function ）**  | 是定义在单个样本上的，算的是一个样本的误差                   |
| **代价函数（Cost Function ）**  | 是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均 |
| **目标函数（Object Function）** | 最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项） |



训练一个神经网络包括向前计算代价函数函数和向后进行梯度下降的过程

计算dW和db：

<img src="D:\Work_APP\Typora\assets\image-20240302094648362.png" alt="image-20240302094648362" style="zoom: 67%;" />

VGG-16的计算
<img src="https://img-blog.csdnimg.cn/46be649cabf440c3918f7c48170a7b76.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5qmZ5a2Q5ZCWMjE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img" style="zoom: 67%;" />

1. 输入图像尺寸为224×224×3，经过64个通道为3的卷积核，步长为1，padding=same填充，卷积两次，再经过ReLU激活，输出尺寸为224×224×64
2. 经max pooling（最大池化），滤波器2×2，步长为2，图像尺寸减半，池化后的尺寸为112×112×64
3. 经128个3×3的卷积核的两次卷积，Relu激活，尺寸变为112×112×128
4. 最大池化后，尺寸56×56×128
5. 经256个3x3的卷积核，三次卷积，ReLU激活，尺寸变为56x56x256
6. max pooling池化，尺寸变为28x28x256
7. 经512个3x3的卷积核，三次卷积，ReLU激活，尺寸变为28x28x512
8. max pooling池化，尺寸变为14x14x512
9. 经512个3x3的卷积核，三次卷积，ReLU，尺寸变为14x14x512
10. max pooling池化，尺寸变为7x7x512
11. 然后Flatten()，将数据拉平成向量，变成一维51277=25088
12. 再经过两层1x1x4096，一层1x1x1000的全连接层（共三层），经ReLU激活**（文献中减少到了2048）**
13. 最后通过softmax输出1000个预测结果

## LSTM的学习

### RNN的学习

### RNN的结构原理

单层网络是酱紫的，但是对于序列化的输入x1...xn而言需要引入一个隐状态来处理，**隐状态**的作用是对序列形数据进行特征提取，再转化为输出。

<img src="D:\Work_APP\Typora\assets\image-20240302101834942.png" alt="image-20240302101834942" style="zoom:50%;" />

以h1为例的话，圆圈方块代表向量，箭头代表对该向量进行一次变化

<img src="D:\Work_APP\Typora\assets\image-20240302101810931.png" alt="image-20240302101810931" style="zoom:50%;" />

**要注意的是：** **每一步使用的参数U、W、b都是一样的，也就是说每个步骤的参数都是共享的，这是RNN的重要特点**

利用f（Wx+b）的变换得到yi

<img src="D:\Work_APP\Typora\assets\image-20240302102441859.png" alt="image-20240302102441859" style="zoom:50%;" />



最终使得，输入和输出序列必须要是等长。

### RNN的作用

模仿人脑进行通过上文推断下文
<img src="D:\Work_APP\Typora\assets\image-20240302102913258.png" alt="image-20240302102913258" style="zoom:50%;" />

结构上可以等效为上面这样。将RNN视为同一神经网络经过多次复制，每个神经网络模块会把消息传递给下一个。

### RNN的缺陷（梯度消失和爆炸问题）

以根据句子预测单词为例the clouds are in the sky，预测“sky”可能比较容易，但是如果句子很长，如I grew up in France...I speak fluent French，要预测最后一个French，当前上下文信息可以知道是一种语言，但是具体是哪一种语言需要到很远的地方去找，间隔的增到，会导致RNN丧失学习到连接如此元的信息的能力。

<img src="D:\Work_APP\Typora\assets\image-20240302103135624.png" alt="image-20240302103135624" style="zoom:33%;" />

<img src="D:\Work_APP\Typora\assets\image-20240302103149438.png" alt="image-20240302103149438" style="zoom: 33%;" />

**梯度会随着时间的推移不断下降减少，而当梯度值变得非常小时，就不会继续学习**，而**RNN会忘记它在较长序列中以前看到的内容，因此RNN只具有短时记忆**。

<img src="D:\Work_APP\Typora\assets\image-20240302103737677.png" alt="image-20240302103737677" style="zoom:33%;" />

### LSTM网络

区别于RNN的是，LSTM用了不同的函数来计算隐状态。

RNN的重复神经网络模块的链式

<img src="D:\Work_APP\Typora\assets\image-20240302104319740.png" alt="image-20240302104319740" style="zoom:33%;" />

激活函数 Tanh 作用在于帮助调节流经网络的值

Tanh

<img src="https://img-blog.csdnimg.cn/img_convert/026b192c71f4ea6df34f1c54561903a9.gif" alt="img" style="zoom:50%;" />

Sigmoid 

<img src="https://img-blog.csdnimg.cn/img_convert/c7f45003745e605895ee53d50b5b29ba.gif" alt="img" style="zoom:50%;" />



<img src="D:\Work_APP\Typora\assets\image-20240302104432544.png" alt="image-20240302104432544" style="zoom:50%;" />

<img src="D:\Work_APP\Typora\assets\image-20240302105027344.png" alt="image-20240302105027344" style="zoom:50%;" />

LSTM中的重复模块则包含四个交互的层，三个Sigmoid 和一个tanh层，并以一种非常特殊的方式进行交互。Sigmoid 压值到0~1，有助于更新和忘记信息。

- 因为任何数乘以0都得0，这部分信息就会被剔除
- 乘1得本身，就能保存下来。



## 胶囊网络--Capsule Network

1.背景介绍
CNN的本质是大量向量和矩阵相乘或相加，为了很好的传递图片特征需要经过卷积池化。胶囊网络同CNN主要区别是方向性。



### 3胶囊网络的动态寻路算法

![image-20240305090354246](D:\Work_APP\Typora\assets\image-20240305090354246.png)

1.$ \widehat { u } _ { j| i }  :$是预测向量，由下层胶囊i预测上层胶囊j的输出；r是路由迭代次数；l是当前层编号。
2.初始化所有胶囊对i和j之间的对数先验概率$b_{ij}$为0（与算法中的$c_{ij}$一一对应）

3.设定r次迭代（超参数）

4.计算胶囊i的耦合系数ci，它是对数先验概率bi的softmax。这决定了下层胶囊i将上层的哪些胶囊j发送其输出的权重。

5.计算上层胶囊j的总输入向量sj，它是所由来自下层胶囊i的预测向量$ \widehat { u } _ { j| i }$的加权和。

6.应用squash函数来压缩每个胶囊j的总输入$s_j$,得到输出向量$v_j$

7.更新对数先验概率 $b_{ij}$，增加当前预测向量和$ \widehat { u } _ { j| i }$输出向量 的点$v_j$积。这个更新反映了下层胶囊 i 的预测与上层胶囊 j的实际输出之间的一致性
8.`return v_j`：返回上层胶囊 �*j* 的输出向量 $v_j$

## 卷积的笔记（概括性的）

滤波器filter是卷积核kernel的集合，filter中kernel的个数与输入图像的通道数channel要严格对应（例如，**灰度图是单通道则为1个kernel，RGB图是3通道则为3个kernel**）。一个filter对应图像的一种特征，比如亮度，锐度，纹理等，一个卷积层可以有过多个filter，用来提取对应数量的特征，并送往下一层。

一个滤波器中含有一个或多个卷积核
输入图像的通道数 = 滤波器中kernel的个数
滤波器的数量 = 输出图像的通道数

### 三维卷积

三维卷积输入多了深度C这个维度，输入是高度H*宽度W*深度C的三维矩阵。多通道卷积看起来和三维卷积有一样的深度，但两者之间是有本质的区别的。

<img src="D:\Work_APP\Typora\assets\image-20240306160544093.png" alt="image-20240306160544093" style="zoom: 33%;" />

#### 3D卷积特点

1.**过滤器深度小于输入层深度**（核大小<通道数）

2.3D滤波器可以在是三个方向移动

#### 3D卷积工作原理

<img src="https://img-blog.csdnimg.cn/20181104094535571.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjczOTYy,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />

如上图所示对视频进行3DCNN处理，将时间维度视为第三维度，上图中对连续4帧图像进行卷积操作，3D卷积是通过堆叠多个连续的帧组成一个立方体，然后在立方体中运用3D卷积核，卷积层中每一个特征map都会与上一层中**多个邻近的连续帧**相连

#### 多通道卷积

<img src="https://img-blog.csdnimg.cn/20200630171048940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4OTQ5ODQ3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

公式

$$(n*n*n_c)*(f*f*n_c)=[ \frac { ( n + 2 p - f ) } { s } ]^2*n_c'$$

n：原始图片大小，nc：原始通道数，p：padding维度，f：核大小，s：步长

计算结果：维度是整数，不是整数用向下取整

一组卷积核大小相同

原始图片的卷积核中通道数必须相等，$n_c’$表示卷积核个数

#### 视频分类中的应用

![img](https://img-blog.csdnimg.cn/20181104100334956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjczOTYy,size_16,color_FFFFFF,t_70)

1.**input—>H1：**

7帧大小为60*40的视频，通过预设的硬核获得5种特征：灰度、x方向梯度、y方向梯度、x方向光流、y方向光流，前面三个通道的信息可以直接对每帧分别操作获取，后面的光流则需要利用两帧信息才能获取，因此H1层的特征图数量：（7+7+7+6+6）后面的两个6表示两帧之间作差。

2.**H1—>C2**

用两个$7*7*3$的3D卷积核对5个通道分别卷积，获得两个序列（参考对4帧的操作），每个序列5个通道（$7*7$空间维度，$3$时间维度，每次操作3帧），同时，为了增加特征图个数，在这一层采用了两中不同的3D核，因此C2层的特征图数量是：$(((7-3)+1)*3+((6-3)+1)*2=23*2$这里右乘2表示两种卷积核，特征大小为:$((60-7)+1)*((40-7)+1)=54 * 34$然后为卷积结果加上偏置套一个tanh函数进行输出

3.**C2—>S3**
 2x2池化，下采样。下采样之后的特征maps数量保持不变，因此S3层的特征maps数量为：23 *2。特征maps的大小为：（（54 / 2） *  （34 /2）=27  *17

4.**S3—>C4**

为了提取更多的图像特征，用三个$7*6*3$的3D卷积核分别对各个系列各个channels进行卷积，获得6个系列，每个系列依旧5个channels的大量maps。
我们知道，从输入的7帧图像获得了5个通道的信息，因此结合总图S3的上面一组特征maps的数量为（（7-3）+1） * 3+（（6-3）+1） * 2=23,可以获得各个通道在S3层的数量分布：

前面的乘3表示gray通道maps数量= gradient-x通道maps数量= gradient-y通道maps数量=（7-3）+1）=5；

后面的乘2表示optflow-x通道maps数量=optflow-y通道maps数量=（6-3）+1=4；假设对总图S3的上面一组特征maps采用一种7 6 3的3D卷积核进行卷积就可以获得：

（（5-3）+1）* 3+（（4-3）+1）* 2=9+4=13；

三种不同的3D卷积核就可获得13* 3个特征maps，同理对总图S3的下面一组特征maps采用三种不同的卷积核进行卷积操作也可以获得13*3个特征maps，

因此C4层的特征maps数量：13* 3* 2=13* 6

C4层的特征maps的大小为：（（27-7）+1）* （（17-6）+1）=21*12

然后加偏置套tanh。

**5.c4→s5**

3X3池化，下采样。此时每个maps的大小：7* 4。通道maps数量分布情况如下：

gray通道maps数量= gradient-x通道maps数量= gradient-y通道maps数量=3

optflow-x通道maps数量=optflow-y通道maps数量=2；

**6.S5—>C6**

​    进行了两次3D卷积之后，时间上的维数已经被压缩得无法再次进行3D卷积（两个光流channels只有两个maps）。此时对各个maps用7*42D卷积核进行卷积，加偏置套tanh（烦死了！），获得C6层。C6层维度已经相当小，flatten为一列有128个节点的神经网络层。

**7.C6—>output4**

经典神经网络模型两层之间全链接，output的节点数目随标签而定。

#### pointnet网络

pointnet采用了两次STN，第一次input transform是对空间中点云进行调整，直观上理解是旋转出一个更有利于分类或分割的角度，比如把物体转到正面；第二次feature transform是对提取出的64维特征进行对齐，即在特征层面对点云进行变换。

![img](https://img-blog.csdn.net/20180509232127662)

### 注意力机制

#### **一、SE (Squeeze and Excitation）**

SE注意力机制是通道注意力模式下的一种确定权重的方法，它通过在不同通道间分配权重达到主次有限的目的。
①将特征图Squeeze（压缩），该步骤通过全局平均池化特征图（N,C,H,W）→（N,C,1,1）
②Excitation：双全连，第一个用ReLU并且输入只有原始输入的1/4或1/16减少计算量；第二个用Sigmoid，将权重映射到（0,1）

![image-20240307084120487](D:\Work_APP\Typora\assets\image-20240307084120487.png)

​																						SE注意力机制
③：将reshape过后的权重值与原来的特征图做乘法运算（广播机制）。

```Python
class SE(nn.Module):
    def __init__(self, in_channel, reduction=16):
        super(SE, self).__init__()
        self.pool = nn.AdaptiveAvgPool2d(output_size=1)
        self.fc = nn.Sequential(
            nn.Linear(in_features=in_channel, out_features=in_channel//reduction, bias=False),
            nn.ReLU(),
            nn.Linear(in_features=in_channel//reduction, out_features=in_channel, bias=False),
            nn.Sigmoid()
        )
    def forward(self, x):
        out = self.pool(x)
        out = self.fc(out.view(out.size(0), -1))
        out = out.view(x.size(0), x.size(1), 1, 1)
        return out*x

```

#### **二、ECA（Efficient Channel Attention）**

   为SE虽然全连接的降维可以降低模型的复杂度，但是破坏了**通道与其权重**之间的直接对应**关系**，先降维后升维，这样权重和通道的对应关系是**间接的**。提出一维卷积的方法，**避免了降维对数据的影响**。

1.同SE

2.计算自使用卷积核大小$k=|\frac{log_2(C)}γ+\frac{b}γ|$其中C是通道数，b=1，γ=2，并采用以为卷积计算通道权重，最后用Sigmoid

3.将reshape过后的权重值与原来的特征图做乘法运算

![image-20240307105416553](D:\Work_APP\Typora\assets\image-20240307105416553.png)

加了个一维卷积


#### 三、CBAM（Convolutional Block Attention Module）

<img src="https://pic1.zhimg.com/80/v2-93adc0864f443bda9f1e86ea5e015e78_1440w.webp" alt="img" style="zoom:50%;" />

①：通过将特征图进行Squeeze(压缩)，该步骤分别采用全局平均池化和全局最大池化把特征图从大小为（N,C,H,W)转换为（N,C,1,1)，这样就达到了全局上下文信息的融合。

②：分别将全局最大池化和全局平均池化结果进行MLP（多层感知机）操作，MLP在这里定义与SE的操作一样，为两层全连接层，中间采用ReLU激活，最后将两者相加后利用Sigmoid函数激活。

③：将reshape过后的权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。

空间结构主要分为以下三个方面：

①：将上述通道注意力操作的结果，分别在通道维度上进行最大池化和平均池化，即将经过通道注意力机制的特征图从（N,C,H,W)转换为（N,1,H,W），达到融合不同通道的信息的效果，然后在通道维度上将最大池化与平均池化结果叠加起来，即采用torch.cat()。

②：将叠加后2个通道的结果做卷积运算，输出通道为1，卷积核大小为7，最后将输出结果采用Sigmoid函数激活。

③：将权重值与原有的特征图做乘法运算（该步骤采用了Python的广播机制），得到不同权重下的特征图。

<img src="https://pic3.zhimg.com/80/v2-0b468c9d39c0ceaa860516556ff55b9a_1440w.webp" alt="img" style="zoom:50%;" />

```Python
import torch
import torch.nn as nn
import math
class CBAM(nn.Module):
    def __init__(self,in_channel,reduction=16,kernel_size=7):
        super(CBAM, self).__init__()
        #通道注意力机制
        self.max_pool=nn.AdaptiveMaxPool2d(output_size=1)
        self.avg_pool=nn.AdaptiveAvgPool2d(output_size=1)
        self.mlp=nn.Sequential(
            nn.Linear(in_features=in_channel,out_features=in_channel//reduction,bias=False),
            nn.ReLU(),
            nn.Linear(in_features=in_channel//reduction,out_features=in_channel,bias=False)
        )
        self.sigmoid=nn.Sigmoid()
        #空间注意力机制
        self.conv=nn.Conv2d(in_channels=2,out_channels=1,kernel_size=kernel_size ,stride=1,padding=kernel_size//2,bias=False)

    def forward(self,x):
        #通道注意力机制
        maxout=self.max_pool(x)
        maxout=self.mlp(maxout.view(maxout.size(0),-1))
        avgout=self.avg_pool(x)
        avgout=self.mlp(avgout.view(avgout.size(0),-1))
        channel_out=self.sigmoid(maxout+avgout)
        channel_out=channel_out.view(x.size(0),x.size(1),1,1)
        channel_out=channel_out*x
        #空间注意力机制
        max_out,_=torch.max(channel_out,dim=1,keepdim=True)
        mean_out=torch.mean(channel_out,dim=1,keepdim=True)
        out=torch.cat((max_out,mean_out),dim=1)
        out=self.sigmoid(self.conv(out))
        out=out*channel_out
        return out
```



## 三维重建-对极几何

解决的问题：在相机2中如何找到相机1中x1的位置

<img src="https://img-blog.csdnimg.cn/direct/c119f2adc211419d8167abdfdc89e9f0.png" alt="在这里插入图片描述" style="zoom:67%;" />

Baseline指的是两个相机位置之间的连线，**红色虚线**代表相机2所有可能

<img src="https://img-blog.csdnimg.cn/direct/aaab9c5d13414b92818088ea2ccfd98f.png" alt="在这里插入图片描述" style="zoom: 67%;" />

- $O_1-X-O_2$构成的平面角对极平面
- 候选点构成的直线，即相机平面与极平面相交的线
- $O_1-O_2$连线在相机平面上投影为Epipole

问题的回答
Q1：为什么要以baseline为底构造Epipolar Plane？

A：因为两个相机可能处于不同高度，为了确保**候选点**能落在另一个相机的**一条直线**上，需要线构造出两个相机所在平面。
Q2：为啥要让候选点落在另一个相机平面的一条直线上，这条E L先有什么意义。

A:因为这限制了寻找对应点的搜索空间，提供了一定的约束条件，使立体匹配算法更高效。E L还描述了两相机视图和它们捕捉的3D场景之间的关系，方便后续对齐。
**###########################还没看完。。。数学部分**###########################
https://blog.csdn.net/Bartender_VA11/article/details/136080481

# GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video

论文的目标：专注于通过基准真值数据来改进3D人体姿态提升，以便将来改进更多质量的





# Deep learning-based estimation of whole-body kinematics from multi-view images

为了评估职业作业中致死损伤和肌肉骨骼损伤的风险，有必要分析全身运动学（包括关节位置和关节角度）。人体姿态估计作为以中国确定关节位置误差最小的方法，今年来受到越来越多的关注。然而，关节角度的估计并不常见，关节角度估计质量也没有得到评估，本文提出一种基于多视角图像的直接估计关节角度的端对端方法。我们的方法利用了姿态表示，并将旋转表示映射到连续空间，其中每个旋转都是唯一表示的。我们还提出了一个新的







